[Course index]

{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)",
  "@search.nextPageParameters": {
    "search": "AI+Azure",
    "skip": 50
  },
  "value": [
    {
      "@search.score": 9.98237,
      "Key": "ms-learn7be24452-07d1-4acc-99b3-89b901049505",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 9.98237,
      "Key": "ms-learnbdb45f14-e397-40ab-bf99-d3ff1cc0ea43",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-bot-service",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 9.98237,
      "Key": "ms-learnefcdc4e3-d588-4b25-b109-65b7c593572b",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 8.957269,
      "Key": "ms-learn7d70effe-470c-403c-98f4-dd0295dab8a8",
      "description": "This module provides an overview of Azure AI and demonstrates how Microsoft tools, services, and infrastructure can help make AI real for your organization, whether you want to unlock insights from your latent data with knowledge mining, develop your own AI models with machine learning, or build immersive apps using AI.",
      "duration": "59",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 4523,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Microsoft Azure Artificial Intelligence (AI) strategy and solutions",
      "url": "https://docs.microsoft.com/en-us/learn/modules/azure-artificial-intelligence/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn0479d887-32ce-477b-b184-9230e096562a",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn25b613a7-0afa-46dc-be9a-3b34c30a1d40",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-bot-service",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "developer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn3e93d44c-8438-46e8-a157-15a2dfa6c6ca",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "student",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn64fe0253-71fd-4c27-b288-6e8cf819d02c",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "developer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn7a84294d-d64c-4fa9-afb8-6f99822c848e",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "developer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn843d6d18-cb80-417e-932e-206e15c80b57",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "student",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learn88c9641b-7fc9-4791-9798-fc047ba013a5",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-bot-service",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "student",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learncb1e0a98-cce5-45a9-810e-c6e147a777f2",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.610014,
      "Key": "ms-learncf9fbb13-4097-4c66-9cc9-582abfcc7298",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-bot-service",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.4163914,
      "Key": "ms-learn13f442b0-f1d1-4402-8c3e-13f0c8b6ce8d",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.159274,
      "Key": "ms-learnf41c2cab-9943-42db-a0c9-8bed47619c53",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.0758657,
      "Key": "ms-learn5244e529-28c6-4b6c-bba0-a3edcb956327",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.0758657,
      "Key": "ms-learn5eb8b628-5bd4-444a-a8f2-268baff04eb0",
      "description": "Create a Clustering Model with Azure Machine Learning designer",
      "duration": "49",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 2435,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a Clustering Model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-clustering-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.0758657,
      "Key": "ms-learn7e28c65f-5a23-47e7-b575-66139b3a43ca",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.0758657,
      "Key": "ms-learn8860465a-b002-41bc-9025-f54d5ea1376d",
      "description": "Create a classification model with Azure Machine Learning designer",
      "duration": "60",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 1848,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a classification model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-classification-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 7.0758657,
      "Key": "ms-learnfac8a485-321e-4777-bf59-b61d6d02d25d",
      "description": "Create a Regression Model with Azure Machine Learning designer",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 2352,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a Regression Model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-regression-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 6.788085,
      "Key": "ms-learn0fcb600b-4bb5-480c-97ed-63cf6d842fe8",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 6.5849123,
      "Key": "ms-learnda546e02-79d1-4ea6-ac52-f694037e97a6",
      "description": "This module provides an overview of Azure AI and demonstrates how Microsoft tools, services, and infrastructure can help make AI real for your organization, whether you want to unlock insights from your latent data with knowledge mining, develop your own AI models with machine learning, or build immersive apps using AI.",
      "duration": "59",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 4523,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Microsoft Azure Artificial Intelligence (AI) strategy and solutions",
      "url": "https://docs.microsoft.com/en-us/learn/modules/azure-artificial-intelligence/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.6605253,
      "Key": "ms-learn048934af-02e4-43b8-bd22-48b6a114efdb",
      "description": "Use Python, Flask, and Azure Cognitive Services to build a web app that incorporates AI",
      "duration": "75",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 248,
      "role": "developer",
      "source": "MS Learn",
      "title": "Build an AI web app by using Python and Flask",
      "url": "https://docs.microsoft.com/en-us/learn/modules/python-flask-build-ai-web-app/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.6605253,
      "Key": "ms-learna0cf3e55-0c19-443f-a84c-1610bfb5de07",
      "description": "Use Python, Flask, and Azure Cognitive Services to build a web app that incorporates AI",
      "duration": "75",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 248,
      "role": "student",
      "source": "MS Learn",
      "title": "Build an AI web app by using Python and Flask",
      "url": "https://docs.microsoft.com/en-us/learn/modules/python-flask-build-ai-web-app/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.3798347,
      "Key": "ms-learncf8b55b7-23d8-4763-ae3d-36966bc00e32",
      "description": "Use automated machine learning in Azure Machine Learning",
      "duration": "45",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 3459,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use automated machine learning in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-automated-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.1785088,
      "Key": "ms-learnf21756f8-7233-4d03-8d41-2361ca6e1ba2",
      "description": "Learn how to use the Computer Vision API in Azure to identify facial details in pictures",
      "duration": "53",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 1512,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Identify faces and expressions by using the Computer Vision API in Azure Cognitive Services",
      "url": "https://docs.microsoft.com/en-us/learn/modules/identify-faces-with-computer-vision/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.070628,
      "Key": "ms-learn1c943a04-6ef7-4461-a7d8-87b508289acd",
      "description": "Learn enterprise AI management with our free open online course, including scaling AI.",
      "duration": "56",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.81,
      "rating_count": 42,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Implement AI in your organization",
      "url": "https://docs.microsoft.com/en-us/learn/modules/implement-ai-organization/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.070628,
      "Key": "ms-learn9fc0767a-a020-44da-942e-9ebc34f81ba7",
      "description": "Learn enterprise AI management with our free open online course, including scaling AI.",
      "duration": "56",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.81,
      "rating_count": 42,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Implement AI in your organization",
      "url": "https://docs.microsoft.com/en-us/learn/modules/implement-ai-organization/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 5.008617,
      "Key": "ms-learnb0045ced-858c-44a4-b076-1f44158d562b",
      "description": "Learn how to use the Computer Vision API in Azure to identify facial details in pictures",
      "duration": "53",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-sdks",
      "rating_average": 4.65,
      "rating_count": 1512,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Identify faces and expressions by using the Computer Vision API in Azure Cognitive Services",
      "url": "https://docs.microsoft.com/en-us/learn/modules/identify-faces-with-computer-vision/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.966016,
      "Key": "ms-learn891d8e28-1a77-48ba-afe4-829615e177e7",
      "description": "Azure Cognitive Services enable developers to easily add cognitive features into their applications. Learn how to configure and manage these services for your AI application needs.",
      "duration": "44",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.56,
      "rating_count": 25,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create and manage Cognitive Services",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-manage-cognitive-services/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.9214706,
      "Key": "ms-learn82345acf-9bd7-4a6f-946c-24fd4604be35",
      "description": "University of Oxford",
      "duration": "76",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-iot-hub",
      "rating_average": 4.59,
      "rating_count": 94,
      "role": "ai-edge-engineer",
      "source": "MS Learn",
      "title": "Connecting IoT devices to Cognitive Services using Azure Functions",
      "url": "https://docs.microsoft.com/en-us/learn/modules/connecting-iot-devices-cognitive-services-azure-functions/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.9086256,
      "Key": "ms-learn4c7a9482-9c37-4502-9899-c599b6126a18",
      "description": "Learn the primary concepts and technologies of AI, and how Microsoft is turning the latest AI into tools, products, and services that organizations can leverage",
      "duration": "37",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.7,
      "rating_count": 1868,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Introduction to AI technology",
      "url": "https://docs.microsoft.com/en-us/learn/modules/introduction-to-ai-technology/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.9086256,
      "Key": "ms-learnf4d30c45-11b8-4dc9-92e0-dd13520d73ae",
      "description": "Learn the primary concepts and technologies of AI, and how Microsoft is turning the latest AI into tools, products, and services that organizations can leverage",
      "duration": "37",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.7,
      "rating_count": 1868,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Introduction to AI technology",
      "url": "https://docs.microsoft.com/en-us/learn/modules/introduction-to-ai-technology/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.712715,
      "Key": "ms-learn0c97eb2f-794d-432f-a042-476118f2b508",
      "description": "Azure Cognitive Services enable developers to easily add cognitive features into their applications. Learn how to configure and manage these services for your AI application needs.",
      "duration": "44",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-container-instances",
      "rating_average": 4.56,
      "rating_count": 25,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create and manage Cognitive Services",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-manage-cognitive-services/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.712715,
      "Key": "ms-learn143dc2ce-a8fb-48da-b001-b28dc2fe256b",
      "description": "Azure Cognitive Services enable developers to easily add cognitive features into their applications. Learn how to configure and manage these services for your AI application needs.",
      "duration": "44",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.56,
      "rating_count": 25,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create and manage Cognitive Services",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-manage-cognitive-services/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.712715,
      "Key": "ms-learn99978375-0287-4bb2-85e9-d96ba16f6b54",
      "description": "Azure Cognitive Services enable developers to easily add cognitive features into their applications. Learn how to configure and manage these services for your AI application needs.",
      "duration": "44",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-container-registry",
      "rating_average": 4.56,
      "rating_count": 25,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create and manage Cognitive Services",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-manage-cognitive-services/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.593542,
      "Key": "ms-learnb2d9f18e-9e22-4036-b262-9d4f4ee1809d",
      "description": "Detect and track polar bears through photos using AI, and then use Power BI to show where cameras spot polar bears.",
      "duration": "82",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 318,
      "role": "student",
      "source": "MS Learn",
      "title": "Track wild polar bears with AI",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-ml-model-with-azure-stream-analytics/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.593542,
      "Key": "ms-learnc4e16a38-3b2c-4cda-9d4c-3c5b6a9cf364",
      "description": "Detect and track polar bears through photos using AI, and then use Power BI to show where cameras spot polar bears.",
      "duration": "82",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 318,
      "role": "developer",
      "source": "MS Learn",
      "title": "Track wild polar bears with AI",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-ml-model-with-azure-stream-analytics/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.593542,
      "Key": "ms-learnda32f627-6b1c-4104-b7ec-b2ea289b6225",
      "description": "Detect and track polar bears through photos using AI, and then use Power BI to show where cameras spot polar bears.",
      "duration": "82",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 318,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Track wild polar bears with AI",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-ml-model-with-azure-stream-analytics/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learn11567d1f-8f06-4573-9ad4-2b8eb2dbc479",
      "description": "Learn the necessary cultural changes for enterprise sales organizations to make AI transformation successful, and how they fit into a holistic AI strategy.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.7,
      "rating_count": 581,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Discover how to foster an AI-ready culture in sales",
      "url": "https://docs.microsoft.com/en-us/learn/modules/foster-ai-ready-culture-sales/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learn676c8734-28b6-4cbf-85b8-073afcde7c37",
      "description": "Understand the key elements that make a culture AI-ready and a framework to drive the change needed in your organization for a successful AI strategy.",
      "duration": "49",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.74,
      "rating_count": 1136,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Understand the importance of building an AI-ready culture",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-an-ai-ready-culture/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learn832d7da2-2022-43da-99ee-4223996f821f",
      "description": "Learn the necessary cultural changes for enterprise finance organizations to make AI transformation successful, and how they fit into a holistic AI strategy.",
      "duration": "39",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.8,
      "rating_count": 756,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Discover how to foster an AI-ready culture in finance",
      "url": "https://docs.microsoft.com/en-us/learn/modules/foster-ai-ready-culture-finance/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learn9f7001c4-7fa7-48d5-9e98-898ad84cfa6d",
      "description": "Learn the necessary cultural changes for enterprise finance organizations to make AI transformation successful, and how they fit into a holistic AI strategy.",
      "duration": "39",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.8,
      "rating_count": 756,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Discover how to foster an AI-ready culture in finance",
      "url": "https://docs.microsoft.com/en-us/learn/modules/foster-ai-ready-culture-finance/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learnb69fb72b-bb75-445c-ae2a-c7a964f1f959",
      "description": "Understand the key elements that make a culture AI-ready and a framework to drive the change needed in your organization for a successful AI strategy.",
      "duration": "49",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.74,
      "rating_count": 1136,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Understand the importance of building an AI-ready culture",
      "url": "https://docs.microsoft.com/en-us/learn/modules/build-an-ai-ready-culture/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learnbd1a9237-f768-49f6-91ac-76072b54772f",
      "description": "Learn the necessary cultural changes for enterprise marketing organizations to make AI transformation successful, and how they fit into a holistic AI strategy.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.77,
      "rating_count": 717,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Discover how to foster an AI-ready culture in marketing",
      "url": "https://docs.microsoft.com/en-us/learn/modules/foster-ai-ready-culture-marketing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learne431417e-f8a4-42b2-9f09-e14c38bf25e3",
      "description": "Learn the necessary cultural changes for enterprise sales organizations to make AI transformation successful, and how they fit into a holistic AI strategy.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.7,
      "rating_count": 581,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Discover how to foster an AI-ready culture in sales",
      "url": "https://docs.microsoft.com/en-us/learn/modules/foster-ai-ready-culture-sales/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.4250917,
      "Key": "ms-learnf0d68727-7d99-41cf-bc25-2a4321256ea0",
      "description": "Learn the necessary cultural changes for enterprise marketing organizations to make AI transformation successful, and how they fit into a holistic AI strategy.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.77,
      "rating_count": 717,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Discover how to foster an AI-ready culture in marketing",
      "url": "https://docs.microsoft.com/en-us/learn/modules/foster-ai-ready-culture-marketing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.332525,
      "Key": "ms-learn2ec52d05-6693-4852-b022-2e5709e8db45",
      "description": "Explore six principles to guide AI development and use - fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability",
      "duration": "56",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.71,
      "rating_count": 956,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Identify guiding principles for responsible AI",
      "url": "https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.332525,
      "Key": "ms-learn5e9c9c3a-457f-49ab-8160-940f1b20036a",
      "description": "Explore six principles to guide AI development and use - fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability",
      "duration": "56",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.71,
      "rating_count": 956,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Identify guiding principles for responsible AI",
      "url": "https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    },
    {
      "@search.score": 4.262995,
      "Key": "ms-learn194266cc-721b-4b85-aaec-c53cdc7a9379",
      "description": "Learn Microsoft development guidelines for fair and secure responsible conversational AI.",
      "duration": "36",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.85,
      "rating_count": 13,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Discover Microsoft safety guidelines for responsible conversational AI development",
      "url": "https://docs.microsoft.com/en-us/learn/modules/secure-responsible-conversational-ai/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ]
    }
  ],
  "@odata.nextLink": "https://atc-aisearch.search.windows.net/indexes/azuretable-index/docs/search?api-version=2024-03-01-preview"
}

[Library index]

{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 5.970807,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 593265,
      "metadata_storage_name": "Köchling-Wehner2020_Article_DiscriminatedByAnAlgorithmASys.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L0slQzMlQjZjaGxpbmctV2VobmVyMjAyMF9BcnRpY2xlX0Rpc2NyaW1pbmF0ZWRCeUFuQWxnb3JpdGhtQVN5cy5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Alina Köchling ",
      "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "metadata_creation_date": "2020-11-19T15:45:16Z",
      "keyphrases": [
        "200 artificial intelligence (AI) specialists",
        "two essential HR functions",
        "Marius Claus Wehner1",
        "Heinrich-Heine-University Düsseldorf",
        "major driving forces",
        "human resource management",
        "Alina Köchling1",
        "routinized workplace decisions",
        "current human resource",
        "crucial future research",
        "HRM � Literature review",
        "Abstract Algorithmic decision-making",
        "Keywords Fairness � Discrimination",
        "human biases",
        "systematic review",
        "HR recruitment",
        "HR development",
        "HR) practices",
        "ORIGINAL RESEARCH",
        "new source",
        "unfair treatment",
        "implicit discrimination",
        "implicit) discrimination",
        "current state",
        "research gaps",
        "36 journal articles",
        "possible pitfalls",
        "important theoretical",
        "practical implications",
        "fruitful avenues",
        "fairness � Ethics",
        "rapid growth",
        "Business Administration",
        "Universitätsstrasse",
        "Business Research",
        "automated decision-making",
        "remote control",
        "Möhlmann",
        "important individual",
        "societal implications",
        "organizational optimization",
        "hidden talented",
        "large number",
        "German companies",
        "competitive advantages",
        "commercial providers",
        "algorithmic platforms",
        "performance measurements",
        "large companies",
        "economic reasons",
        "personal beliefs",
        "doi.org",
        "orcid.org",
        "context",
        "Author",
        "advice",
        "firms",
        "costs",
        "efficiency",
        "objectivity",
        "groups",
        "people",
        "unfairness",
        "knowledge",
        "threats",
        "goal",
        "directions",
        "applications",
        "researchers",
        "practitioners",
        "1 Introduction",
        "information",
        "importance",
        "digitalization",
        "organizations",
        "koechling",
        "hhu",
        "1 Faculty",
        "Economics",
        "40225 Dusseldorf",
        "Germany",
        "crossmark",
        "crossref",
        "standardization",
        "Zalmanson",
        "Algorithms",
        "humans",
        "Chalfin",
        "Lee",
        "Lindebaum",
        "changes",
        "favor",
        "employees",
        "Silverman",
        "Waller",
        "Carey",
        "Smith",
        "Savage",
        "Bales",
        "survey",
        "Deloitte",
        "Several",
        "Google",
        "IBM",
        "SAP",
        "Microsoft",
        "systems",
        "hiring",
        "Walker",
        "turn",
        "Vodafone",
        "Unilever",
        "Ikea",
        "Daugherty",
        "Wilson",
        "Precire",
        "savings",
        "time",
        "risks",
        "productivity",
        "certainty",
        "Suen",
        "McDonald",
        "McColl",
        "Michelotti",
        "Woods",
        "prejudices",
        "15",
        "American e-commerce specialist",
        "algorithmic decision- making",
        "new research avenues",
        "HR development processes",
        "unrepresentative input data",
        "algorithmic decision-making system",
        "complete algorithmic decision-making",
        "training) data",
        "algorithmic outcomes",
        "same attention",
        "same requirements",
        "first glance",
        "human decision-making",
        "unequal treatment",
        "different groups",
        "qualitative differences",
        "individual performance",
        "biased outcomes",
        "biased decisions",
        "current debate",
        "extreme disadvantage",
        "796 Business Research",
        "Previous studies",
        "common procedures",
        "labor shortages",
        "one hand",
        "other side",
        "computer science",
        "neous state",
        "distinct challenges",
        "future research",
        "practical point",
        "known companies",
        "negative consequences",
        "potential downsides",
        "potential dangers",
        "possible threat",
        "prominent example",
        "female applicants",
        "hiring decision",
        "employees’ acceptance",
        "existing knowledge",
        "current literature",
        "hiring algorithms",
        "potential biases",
        "HRM context",
        "consistency",
        "fairness",
        "Langer",
        "Florentine",
        "Raghavan",
        "application",
        "criteria",
        "Lepri",
        "discrimination",
        "Simbeck",
        "gender",
        "ethnicity",
        "Arrow",
        "Kim",
        "Barocas",
        "Selbst",
        "Suresh",
        "Guttag",
        "Chander",
        "issue",
        "transparency",
        "Dwork",
        "Diakopoulos",
        "Amazon",
        "Dastin",
        "Miller",
        "lack",
        "accountability",
        "factors",
        "Citron",
        "Pasquale",
        "question",
        "Kaibel",
        "discrepancy",
        "enthusiasm",
        "panacea",
        "inefficiencies",
        "field",
        "infancy",
        "digitization",
        "automation",
        "view",
        "aim",
        "study",
        "awareness",
        "The Oxford Living Dictionary",
        "two important HR areas",
        "several different research areas",
        "various algorithmic decision-making tools",
        "several AI decision tools",
        "other problem-solving operations",
        "two HR functions",
        "explicit human interference",
        "natural language processing",
        "several research gaps",
        "future research avenues",
        "systematic literature review",
        "HR development methods",
        "future research directions",
        "several ways",
        "workplace decision",
        "human intelligence",
        "algorithmic selection",
        "career development",
        "future performance",
        "person-organization fit",
        "serious consequences",
        "Ötting",
        "existing body",
        "ethical issues",
        "illustrative examples",
        "timely topic",
        "enormous importance",
        "reputational risk",
        "ment process",
        "key terms",
        "descriptive analysis",
        "subsequent discussion",
        "theoretical implications",
        "Conceptual background",
        "computational mechanism",
        "umbrella term",
        "wide array",
        "machine learning",
        "image recognition",
        "existing literature",
        "AI applications",
        "current employees",
        "HRM field",
        "lacking focus",
        "statistical models",
        "new data",
        "present paper",
        "distributive fairness",
        "understanding",
        "end",
        "potential",
        "Huselid",
        "Decisions",
        "algorithms",
        "individuals",
        "company",
        "society",
        "ethics",
        "procedural",
        "Maier",
        "Tambe",
        "Cappelli",
        "bias",
        "fact",
        "Companies",
        "legal",
        "applicants",
        "reason",
        "guidance",
        "detailed",
        "definitions",
        "methodology",
        "illustration",
        "processes",
        "sets",
        "rules",
        "calculations",
        "computer",
        "routinized",
        "basis",
        "prescriptions",
        "acquisition",
        "environment",
        "Russell",
        "Norvig",
        "ML",
        "speech",
        "NLP",
        "Kaplan",
        "Haenlein",
        "Paschen",
        "2.1",
        "many AI decision-making tools",
        "three major types",
        "artificial neural networks",
        "random variable Y",
        "three core elements",
        "three different levels",
        "neural network models",
        "deep learning models",
        "fixed input/output data",
        "reinforcement type learning",
        "Unsupervised ML algorithms",
        "reinforcement learning",
        "ML models",
        "network structures",
        "probabilistic models",
        "unsupervised settings",
        "learning tasks",
        "ML approach",
        "regression-type problems",
        "ground truth",
        "Human experts",
        "human decisions",
        "future instances",
        "same problem",
        "priori labeling",
        "structural behaviors",
        "theme analysis",
        "798 Business Research",
        "separate group",
        "error interactions",
        "dynamic environment",
        "Single nodes",
        "human brain",
        "human thinking",
        "other settings",
        "prediction tasks",
        "expected values",
        "systematic error",
        "human judgment",
        "algorithmic evaluations",
        "computer systems",
        "black box",
        "glass boxes",
        "making sense",
        "human involvement",
        "Cognitive biases",
        "entire model",
        "nonlinear transformation",
        "individual components",
        "input data",
        "Shin",
        "predictions",
        "outputs",
        "labels",
        "patterns",
        "Canhoto",
        "contrast",
        "Murphy",
        "variables",
        "methods",
        "trial",
        "bling",
        "methodologies",
        "Examples",
        "complex",
        "relationship",
        "phases",
        "layers",
        "neurons",
        "Bengio",
        "multiple",
        "stages",
        "features",
        "representations",
        "advantage",
        "Deng",
        "Yu",
        "Reason",
        "estimation",
        "bY",
        "difference",
        "Kauermann",
        "Kuechenhoff",
        "Goodfellow",
        "uncertainty",
        "Kahneman",
        "others",
        "Friedman",
        "Nissenbaum",
        "HRM",
        "Cheng",
        "Hackett",
        "theory",
        "consideration",
        "distinction",
        "interpretability",
        "explainability",
        "Roscher",
        "combination",
        "former",
        "simulatability",
        "parameters",
        "2.2",
        "predictive algorithmic decision-making tool",
        "accurate algorithmic output",
        "explicit human judgments",
        "input data set",
        "historical employment data",
        "algorithmic transparency",
        "historical data",
        "ML model",
        "different sets",
        "HR department",
        "employee satisfaction",
        "turnover intentions",
        "specific criteria",
        "underlying reasons",
        "main reasons",
        "learning process",
        "exposed examples",
        "one way",
        "subsequent analysis",
        "worst case",
        "discriminatory outputs",
        "manual programming",
        "recruitment process",
        "white men",
        "protected group",
        "systematic disadvantage",
        "biased data",
        "historical biases",
        "different biases",
        "preexisting biases",
        "Contextual information",
        "emergent bias",
        "representation bias",
        "specific information",
        "personality characteristics",
        "ML algorithm",
        "valid example",
        "implicit bias",
        "decomposability",
        "level",
        "training",
        "Interpretability",
        "element",
        "domain",
        "interpretations",
        "derive",
        "conclusions",
        "results",
        "prediction",
        "interest",
        "technical",
        "quality",
        "Danks",
        "London",
        "keyword",
        "garbage",
        "stereotypes",
        "Barfield",
        "Pagallo",
        "creation",
        "kind",
        "racist",
        "Veale",
        "Binns",
        "tendencies",
        "past",
        "Datta",
        "Hispanics",
        "applicant",
        "member",
        "job",
        "interview",
        "designer",
        "categories",
        "disadvantages",
        "absence",
        "selection",
        "measurements",
        "information systems Leventhal",
        "new societal knowledge",
        "different fairness definitions",
        "new knowledge",
        "several steps",
        "feature engineering",
        "free lunch",
        "discriminatory outcomes",
        "technical constraints",
        "technical consideration",
        "several reasons",
        "important conditions",
        "human constructs",
        "human interpretation",
        "cultural values",
        "two reasons",
        "different values",
        "design process",
        "cooperative efforts",
        "competitive behavior",
        "equal treatment",
        "meso- level",
        "statistical parity",
        "true outcomes",
        "fair treatment",
        "equal opportunity",
        "individual subjects",
        "representative bias",
        "Technical bias",
        "algorithmic system",
        "system design",
        "measurement data",
        "model testing",
        "best model",
        "800 Business Research",
        "training data",
        "Individual fairness",
        "group membership",
        "computer technology",
        "specific context",
        "cultural context",
        "group fairness",
        "real users",
        "protected) groups",
        "preprocessing",
        "theorems",
        "Wolpert",
        "Macready",
        "choice",
        "benchmark",
        "performance",
        "rotations",
        "example",
        "females",
        "comparison",
        "overrepresented",
        "limited",
        "hardware",
        "software",
        "peripherals",
        "Bozdag",
        "formalization",
        "computers",
        "problem",
        "judgments",
        "intuitions",
        "law",
        "litigation",
        "construction",
        "result",
        "population",
        "shift",
        "mismatch",
        "Table",
        "state",
        "art",
        "competition",
        "individualistic",
        "needs",
        "overview",
        "measures",
        "hand",
        "micro-level",
        "Hardt",
        "notions",
        "sense",
        "positives/negatives",
        "sources",
        "disadvantage",
        "2.3",
        "fairness Name Author Definition Individual fairness Dwork",
        "similar’’ classifications Group fairness",
        "descriptive personal evaluation",
        "considerable adverse consequences",
        "individual differences",
        "actual fairness",
        "fairness perceptions",
        "objective fairness",
        "Subjective fairness",
        "Discriminatory categories",
        "working experience",
        "conscious process",
        "current position",
        "personnel selection",
        "prediction systems",
        "selection context",
        "differential validity",
        "subgroup validity",
        "differential prediction",
        "biased results",
        "Table 1 Definitions",
        "P bY",
        "Pð bY",
        "opportunity Hardt",
        "False-negative rates",
        "1f g",
        "random variable",
        "802 Business Research",
        "selection process",
        "applicant pool",
        "training process",
        "meaningful effects",
        "negative experiences",
        "recruitment context",
        "job offer",
        "job turnover",
        "explicit discrimination",
        "selection measure",
        "essential role",
        "equal probability",
        "conscientious behavior",
        "managerial decisions",
        "Implicit discrimination",
        "implicit attitudes",
        "HR literature",
        "years",
        "Persson",
        "Bertrand",
        "aversion",
        "support",
        "characteristics",
        "Frijters",
        "Cascio",
        "Aguinis",
        "probabilities",
        "estimates",
        "slopes",
        "intercepts",
        "subgroups",
        "Meade",
        "Fetzer",
        "Roth",
        "Bobko",
        "Bartlett",
        "usage",
        "algorithmic",
        "decision-making",
        "regard",
        "justice",
        "reality",
        "Cropanzano",
        "employers",
        "subjects",
        "positive",
        "1jG",
        "recidivism",
        "estimator",
        "likelihood",
        "altruisms",
        "Cohen-Charash",
        "Spector",
        "candidates",
        "Bauer",
        "morale",
        "intentions",
        "acceptance",
        "rejection",
        "dissatisfaction",
        "reduction",
        "elimination",
        "conflicts",
        "Gilliland",
        "McCarthy",
        "Hausknecht",
        "image",
        "systematic literature review approach",
        "systematic literature reviews",
        "Several online platforms",
        "extensive keyword searching",
        "defined individual concepts",
        "three core dimensions",
        "fruitful research avenues",
        "high procedural justice",
        "three fairness dimensions",
        "three dimensions",
        "replicable approach.1",
        "individual fairness",
        "research question",
        "rating companies",
        "Van Hoye",
        "most urgency",
        "equal opportunities",
        "ethical norms",
        "important role",
        "relevant information",
        "algorithmic decision-making",
        "information processing",
        "human raters",
        "employee evaluation",
        "personal contact",
        "reliable picture",
        "discrimination potential",
        "particular field",
        "overall picture",
        "two authors",
        "Distributive justice",
        "informational justice",
        "fairness measures",
        "Search terms",
        "interactional justice",
        "development process",
        "organizational context",
        "evidence-based decisions",
        "decision process",
        "iterative process",
        "interactional fairness",
        "same treatment",
        "possibility",
        "outcome",
        "Rules",
        "equality",
        "equity",
        "accordance",
        "contributions",
        "extent",
        "Leventhal",
        "Consistency",
        "accuracy",
        "representation",
        "parties",
        "correction",
        "appropriateness",
        "dignity",
        "courtesy",
        "respect",
        "share",
        "general",
        "procedures",
        "errors",
        "3 Methods",
        "insights",
        "other",
        "suggestions",
        "Petticrew",
        "Roberts",
        "Crossan",
        "Apaydin",
        "Siddaway",
        "Gough",
        "databases",
        "discussion",
        "keywords",
        "3.1",
        "EBSCO Business Source Premier database",
        "social science citation index",
        "language peer-reviewed journals",
        "first source",
        "singular/plural forms",
        "different spellings",
        "narrow terms",
        "classification terms",
        "complete list",
        "broad coverage",
        "impact factor",
        "reasonable proxy",
        "important publications",
        "reference section",
        "search strings",
        "relevant articles",
        "scholarly literature",
        "electronic databases",
        "terminology",
        "synonyms",
        "contents",
        "priority",
        "balance",
        "sensitivity",
        "specificity",
        "unintentional",
        "omission",
        "SSCI",
        "management",
        "part",
        "Web",
        "Knowledge",
        "breadth",
        "papers"
      ]
    },
    {
      "@search.score": 4.02183,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relatives",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2986829,
      "metadata_storage_name": "s40493-015-0019-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxOS16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Partheeban Chandrasekaran",
      "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "metadata_creation_date": "2015-09-04T09:59:41Z",
      "keyphrases": [
        "new classi- fication scheme",
        "peer file-sharing applications",
        "social networking websites",
        "comprehensive test tool",
        "Attribution 4.0 International License",
        "original author(s",
        "three trust algorithms",
        "various trust properties",
        "computational trust mechanisms",
        "online community-based systems",
        "Creative Commons license",
        "computational trust models",
        "social trust models",
        "three reputation systems",
        "different trust models",
        "RESEARCH Open Access",
        "trust assessment workflow",
        "existing reputation systems",
        "existing models",
        "Many models",
        "different stages",
        "Trust Management",
        "trust relationship",
        "Multi-agent systems",
        "Computer Engineering",
        "Introduction Motivation",
        "simple averaging",
        "based scores",
        "Advogato website",
        "application-dependent metrics",
        "transformation stage",
        "unrestricted use",
        "appropriate credit",
        "first contribution",
        "second contribution",
        "evaluation schemes",
        "Trust testbed",
        "Esfandiari Journal",
        "latest model",
        "single model",
        "generic testbed",
        "increasing need",
        "past transactions",
        "graph transformations",
        "Carleton University",
        "larger number",
        "Partheeban Chandrasekaran",
        "slandering attacks",
        "two agents",
        "Babak Esfandiari",
        "DOI",
        "experiments",
        "analysis",
        "Correspondence",
        "Department",
        "1125 Colonel",
        "Drive",
        "Ottawa",
        "Ontario",
        "Canada",
        "Abstract",
        "tesbed",
        "flexibility",
        "design",
        "prototype",
        "EigenTrust",
        "PeerTrust",
        "Appleseed",
        "vulnerabilites",
        "compliance",
        "example",
        "discrepancies",
        "trade-offs",
        "resistance",
        "self-promotion",
        "Keywords",
        "growth",
        "users",
        "history",
        "interactions",
        "calculations",
        "ratings",
        "eBay",
        "researcher",
        "tools",
        "evaluations",
        "set",
        "vulnerabilities",
        "types",
        "Overview",
        "solution",
        "contributions",
        "paper",
        "family",
        "process",
        "succession",
        "vertices",
        "meaning",
        "edges",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "crossmark",
        "org",
        "mailto",
        "Page",
        "presence",
        "existence",
        "variety",
        "certificate authority-based PKI trust models",
        "Trust management systems aid agents",
        "reputation-based trust management systems",
        "three different trust algorithms",
        "public key infras",
        "Social trust models",
        "basic trust properties",
        "social trust algorithms",
        "existing trust algorithms",
        "Beta Reputation System",
        "three systems",
        "reputation systems",
        "chaining algorithms",
        "mutual trust",
        "trust assertions",
        "trust score",
        "existing testbeds",
        "subtle interplay",
        "higher sensitivity",
        "lower sensitivity",
        "literature review",
        "implementation details",
        "actual mechanisms",
        "earlier direct",
        "indirect interactions",
        "e-commerce website",
        "third party",
        "Various inputs",
        "file-sharing network",
        "blogging website",
        "particular level",
        "specific context",
        "simple attacks",
        "self-promoting attacks",
        "Problem description",
        "research problem",
        "particular agent",
        "one attack",
        "friend chain",
        "past experiences",
        "testbed prototype",
        "aggregated trustworthiness",
        "evaluation results",
        "range",
        "differences",
        "way",
        "slandering",
        "Organization",
        "section",
        "Background",
        "state",
        "discussion",
        "Conclusions",
        "limitations",
        "tructures",
        "certificates",
        "The",
        "means",
        "order",
        "instance",
        "sellers",
        "trustworthy",
        "product",
        "drawbacks",
        "Chandrasekaran",
        "root",
        "recommendations",
        "turn",
        "recommenders",
        "Nature",
        "TRAVOS",
        "BRS",
        "satisfaction",
        "transaction",
        "P2P",
        "Aberer",
        "Despotovic",
        "plaints",
        "Advogato",
        "goal",
        "spam",
        "community",
        "TidalTrust",
        "beta probability density function",
        "late indirect trust scores",
        "other agents’ past experiences",
        "Spreading Activation model",
        "local trust scores",
        "global trust algorithms",
        "global trust score",
        "local trust algorithms",
        "three algorithms",
        "general trust",
        "Eigen- Trust",
        "Trust decisions",
        "similarity score",
        "file authenticity",
        "recommenda- tions",
        "different inputs",
        "thresholding techniques",
        "edge weights",
        "maximum “flow",
        "direct experience",
        "party recommendations",
        "different families",
        "next sections",
        "detailed descriptions",
        "respective sections",
        "satisfaction ratings",
        "trust graph",
        "direct interactions",
        "truster agent",
        "Credence",
        "votes",
        "files",
        "trustee",
        "gossiping",
        "information",
        "perspective",
        "survey",
        "Marsh",
        "Abdul-Rahman",
        "case",
        "percentage",
        "top",
        "value",
        "energy",
        "tor",
        "short",
        "third",
        "one",
        "scope",
        "testbed",
        "details",
        "output",
        "readers",
        "high local trust values",
        "low local trust values",
        "local direct trust score",
        "normalized local trust score",
        "global trust score computations",
        "current trust scores",
        "trans- action ratings",
        "local trust vector",
        "global trust scores",
        "fellow malicious agents",
        "gence threshold Tc",
        "global value",
        "trust relationships",
        "trust propagation",
        "trust graphs",
        "flow-based algorithm",
        "trust seed",
        "path length",
        "termination condition",
        "weighted graph",
        "Tij trij",
        "cij elements",
        "factor decay",
        "decay factor",
        "honest agents",
        "agent u",
        "agent j",
        "satisfaction rating",
        "transaction rating",
        "raters",
        "Eq.",
        "transactions",
        "system",
        "method",
        "input",
        "sij",
        "sum",
        "truster",
        "friends",
        "cijcjk",
        "matrix",
        "tik",
        "opinion",
        "number",
        "hops",
        "i.",
        "rank",
        "nodes",
        "trustworthiness",
        "sink",
        "amount",
        "places",
        "loops",
        "Ai",
        "step",
        "∑",
        "∈",
        "open-source message- driven simulation engine",
        "two different targets",
        "two testbed implementations",
        "document recommendation systems",
        "beta probability distribution",
        "local trust algorithm",
        "reputation estimation errors",
        "two testbed models",
        "general-purpose trust models",
        "previous reputation value",
        "art painting sales",
        "general trust systems",
        "The Agent Reputation",
        "beta distribution",
        "two roles",
        "initial value",
        "real systems",
        "trust assessment",
        "brief survey",
        "important role",
        "case studies",
        "direct feedbacks",
        "current reputation",
        "single fixpoint",
        "important stage",
        "one trustworthiness",
        "dom variables",
        "various attacks",
        "particular era",
        "market values",
        "specific era",
        "other eras",
        "reputation scores",
        "trust algorithms",
        "next section",
        "rating function",
        "desirable properties",
        "negative experiences",
        "mutual ratings",
        "target pairing",
        "other agents",
        "Guha Guha",
        "seed",
        "methods",
        "look",
        "andMacau",
        "TREET",
        "investigation",
        "graph",
        "documents",
        "many",
        "specialization",
        "subclass",
        "Hazard",
        "Singh",
        "authors",
        "rater",
        "favor",
        "payoff",
        "belief",
        "likelihood",
        "future",
        "definitions",
        "Monotonicity",
        "computed",
        "b.",
        "Unambiguity",
        "convergence",
        "time",
        "Accuracy",
        "total",
        "update",
        "positive",
        "comparison",
        "resilience",
        "performance",
        "domain",
        "client",
        "paintings",
        "appraisals",
        "expert",
        "dedicated distributed infras- tructure",
        "online product review websites",
        "trust score rep- resentation",
        "graphical user interface",
        "first person point",
        "P2P file-sharing networks",
        "honest sales ratio",
        "trust calculation schemes",
        "decentralized trust algorithms",
        "highest bank balance",
        "Value Imbalance attack",
        "general marketplace scenario",
        "various reputation systems",
        "Reputation Lag attack",
        "honest profit ratio",
        "multiple buyer accounts",
        "Multiple seller accounts",
        "The ART testbed",
        "distributed version",
        "utility value",
        "Proliferation attack",
        "cheater sales",
        "other systems",
        "marketplace domain",
        "Reputation Experimentation",
        "trust mechanism",
        "trust management",
        "The Trust",
        "future interactions",
        "possible messages",
        "time interval",
        "The engine",
        "new clients",
        "correct ability",
        "problem domain",
        "Evaluation Testbed",
        "varying prices",
        "inexpensive items",
        "market competition",
        "selling price",
        "future transactions",
        "following metrics",
        "evaluation metrics",
        "simulation engine",
        "simulation days",
        "trustworthy agents",
        "right agents",
        "participating agents",
        "different agents",
        "centralized version",
        "sale price",
        "random number",
        "collusion attacks",
        "winning agent",
        "1,000 different products",
        "departing agent",
        "protocol",
        "Themessages",
        "track",
        "results",
        "database",
        "GUI",
        "runtime",
        "platform",
        "service",
        "cooperation",
        "additional",
        "model",
        "buyers",
        "influence",
        "cost",
        "purchase",
        "probability",
        "initialization",
        "offers",
        "feedback",
        "experience",
        "White-Washing",
        "Self-Promoting",
        "limitation",
        "restriction",
        "others",
        "knowledge",
        "100",
        "many trust management systems",
        "obtain feedback history graph",
        "discrete event simulator",
        "generic evaluation metrics",
        "particular trust algorithm",
        "A Reputation Graph",
        "trust assessment process",
        "Existing trust models",
        "new systems",
        "decision-making process",
        "evaluation framework",
        "labelled graph",
        "agent A",
        "Summary Trust",
        "social trust",
        "trust properties",
        "nth-hand trust",
        "application domain",
        "rational decisions",
        "abstraction layer",
        "n feedbacks",
        "ith satisfaction",
        "different ranges",
        "labelled arcs",
        "other things",
        "next step",
        "transitive closure",
        "target agent",
        "agent B",
        "indirect feedback",
        "feedback histories",
        "following stages",
        "past behavior",
        "R N",
        "common set",
        "final stage",
        "abstract model",
        "feedback value",
        "output requirements",
        "R.",
        "tool",
        "manymodels",
        "attacks",
        "andmodel",
        "developers",
        "foundation",
        "expectation",
        "individual",
        "observations",
        "estimation",
        "combination",
        "rest",
        "graphs",
        "group",
        "actions",
        "list",
        "FHG",
        "downloader",
        "uploader",
        "values",
        "timestamps",
        "directed",
        "second",
        "E.",
        "∅",
        "single reputa- tion score",
        "absolute global reputation score",
        "new classification scheme",
        "continuous reputation values",
        "global reputation scores",
        "existing classification criteria",
        "stage transi- tions",
        "top k agents",
        "local reputation scores",
        "obtain trust graph",
        "Global algorithms",
        "existing literature",
        "one scores",
        "Reputation algorithms",
        "particular task",
        "two groups",
        "clar- ity",
        "Two methods",
        "trust models",
        "above sections",
        "Apple- seed",
        "other hand",
        "same codomain",
        "other words",
        "reputation graph",
        "various algorithms",
        "different algorithms",
        "edge labels",
        "local algorithms",
        "other method",
        "intermediate graph",
        "reflexive property",
        "incoming arcs",
        "among agents",
        "ranking algorithms",
        "One method",
        "trusting agent",
        "EigenTrust outputs",
        "one stage",
        "looping",
        "degree",
        "Figs",
        "weights",
        "Fig.",
        "Examples",
        "decision",
        "TG",
        "ourmodel",
        "stages",
        "workflow",
        "Classifying",
        "RG",
        "nisms",
        "threshold",
        "transitions",
        "addition",
        "Table",
        "semantics",
        "relative",
        "Trust Algorithm Transitions Input",
        "satisfaction local absoluteratings Ranking",
        "continuous feedback values",
        "trust models Stage",
        "graph transformation function",
        "Stage transitions",
        "global absoluteratings",
        "local absolutescores",
        "Trust algorithms",
        "functional view",
        "evaluation mechanisms",
        "global relativeratings",
        "N/A relatives",
        "Reputation Scores",
        "experimenter",
        "inputs",
        "outputs",
        "classification",
        "AppleSeed",
        "3 complaints",
        "3 certificates"
      ]
    },
    {
      "@search.score": 3.0842705,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-s",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 548509,
      "metadata_storage_name": "s13640-020-00545-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMjAtMDA1NDUtei5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "metadata_creation_date": "2021-01-12T23:22:39Z",
      "keyphrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "doi.org",
        "orcid.org",
        "commerce platforms",
        "Correspondence",
        "shaoshuaib",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "BERT",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "creativecommons",
        "licenses",
        "crossmark",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "BERT model",
        "Related work",
        "present stage",
        "good results",
        "extraction capacity",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social information Preprocessing",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "Background processing",
        "subsequent processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "characteristic dimension",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "Data label",
        "tf matrix",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "dividend",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "BERT-Base",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "Text classification BERT fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "Text classification fine-tuning",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "official recommended values",
        "feature matching scheme",
        "text information token",
        "full connection layer",
        "additional 9500 text data",
        "natural language processing",
        "machine learning scheme",
        "word segmentation process",
        "social information data",
        "5.2 Classification scheme",
        "text length",
        "text description",
        "input BERT",
        "first token",
        "TF-IDF model",
        "classification problem",
        "deep learning",
        "learning rate",
        "TF-IDF-based model",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "commodities",
        "social e-commerce market",
        "standard description text",
        "social e-commerce environment",
        "knowledge distillation technology",
        "standard product names",
        "large-scale data marking",
        "social e-commerce classification",
        "model recognition rate",
        "product information",
        "test data",
        "text-based classification",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "high correlation",
        "work",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "semi"
      ]
    },
    {
      "@search.score": 1.3259062,
      "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\n\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\n\nn\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C)−H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN (AD − CB)2\n\n(A+ C)(B+ D)(A+ B)(C + D)\n\n(5)χ2(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf\n+ is the number of documents in the positive class that contain feature f, DFf\n\n− \nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and mf, mc\n\nf are \nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc −mf\n\n)2\n\n(8)mc =\n\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1− qf\n)\n\n(\n\n1− pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf\n+ and DFf\n\n− are the raw counts of documents that contain f in the positive and \nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs. 12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5 GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table 2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nTu\nrk\n\nis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\no",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1280780,
      "metadata_storage_name": "s13673-018-0135-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTgtMDEzNS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tuba Parlar ",
      "metadata_title": "QER: a new feature selection method for sentiment analysis",
      "metadata_creation_date": "2018-04-18T08:33:56Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "naïve Bayes multinomial classifier",
        "Selma Ayşe Özel2",
        "natural language processing task",
        "Text classification Open Access",
        "document frequency difference methods",
        "new feature selection method",
        "Creative Commons license",
        "other feature selection methods",
        "query expansion ranking",
        "query expansion term",
        "optimal orthogonal centroid",
        "four classi- fiers",
        "support vector machines",
        "third class label",
        "decision making process",
        "two-class classification problem",
        "original author(s",
        "Internet World Stats",
        "review docu- ments",
        "Hum. Cent. Comput",
        "statistical methods",
        "learning methods",
        "senti- ments",
        "decision trees",
        "classification accuracy",
        "sentiment classification",
        "author information",
        "Tuba Parlar1",
        "Fei Song3",
        "important piece",
        "social media",
        "top 20 countries",
        "highest numbers",
        "Internet users",
        "exploding growth",
        "main reasons",
        "low ranks",
        "multiple kinds",
        "product reviews",
        "Machine learning",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Parlar",
        "Inf. Sci.",
        "Kemal University",
        "Full list",
        "ferent approaches",
        "data preprocessing",
        "other people",
        "The Author",
        "classification performance",
        "opinionated information",
        "information retrieval",
        "Chi square",
        "information gain",
        "sentiment analysis",
        "ent languages",
        "noisy features",
        "doi.org",
        "orcid.org",
        "irrelevant features",
        "major source",
        "online documents",
        "many researchers",
        "different domains",
        "Turkish reviews",
        "English languages",
        "QER",
        "Introduction",
        "most",
        "opinions",
        "Turkey",
        "area",
        "SA",
        "general",
        "internetworldstats",
        "Abstract",
        "sentiments",
        "order",
        "non-informative",
        "study",
        "field",
        "ling",
        "movie",
        "performances",
        "classifiers",
        "results",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "tparlar",
        "mku",
        "1 Department",
        "Mathematics",
        "Mustafa",
        "Antakya",
        "Hatay",
        "end",
        "crossmark",
        "crossref",
        "Page",
        "19Parlar",
        "studies",
        "query expansion term weighting methods",
        "other common feature selection methods",
        "ten different review documents datasets",
        "maximum entropy mod- elling",
        "naïve Bayes multinomial",
        "traditional topic-based text classification",
        "good feature selection",
        "information retrieval systems",
        "Natural Language Processing",
        "machine learning methods",
        "text classification task",
        "document frequency difference",
        "machine learning approaches",
        "four text classifiers",
        "kitchen appliances reviews",
        "different document domains",
        "Related work SA",
        "feature space",
        "feature sizes",
        "training datasets",
        "different reviews",
        "classification accu",
        "particular review",
        "related attributes",
        "different languages",
        "LCD screen",
        "sentiment-expressing words",
        "experimental settings",
        "important topic",
        "Artificial Intelligence",
        "opinion mining",
        "useful applications",
        "negative ratings",
        "Hum. Cent",
        "popular implementation",
        "movie reviews",
        "book reviews",
        "search performance",
        "performance measures",
        "testing results",
        "Numerous studies",
        "SA problems",
        "overall sentiments",
        "non-informative features",
        "great pictures",
        "NB",
        "SVM",
        "DT",
        "MEM",
        "differences",
        "example",
        "camera",
        "people",
        "breath",
        "observation",
        "account",
        "paper",
        "dimensionality",
        "aim",
        "texts",
        "Turkish",
        "English",
        "effectiveness",
        "DFD",
        "OCFS",
        "goal",
        "racy",
        "respect",
        "rest",
        "Experiments",
        "Conclusion",
        "evalua",
        "emotions",
        "entities",
        "products",
        "services",
        "organizations",
        "als",
        "issues",
        "events",
        "kind",
        "many",
        "popularity",
        "user",
        "companies",
        "businesses",
        "Comput",
        "models",
        "cally",
        "Pang",
        "researchers",
        "effects",
        "Yang",
        "Yu",
        "C4.5",
        "Tabu heuristic search-enhanced Markov blanket model",
        "karşılaştırmalısın",
        "information gain feature selection method",
        "document frequency feature selection method",
        "minimum redundancy maximum relevancy",
        "Weighted Log Likelihood Ratio",
        "Weka [33] data mining tool",
        "DFD feature selection method",
        "reduced feature sizes",
        "Machine learning algorithms",
        "product review dataset",
        "Chinese online reviews",
        "movie review dataset",
        "The Turkish language",
        "feature sets",
        "Boynukalın",
        "n words",
        "lish reviews",
        "product datasets",
        "execution time",
        "large number",
        "comparative experiments",
        "significant improvement",
        "Altaic branch",
        "Ural-Altaic family",
        "agglutinative language",
        "longer sentence",
        "ture space",
        "emotional analysis",
        "initial version",
        "DT classifiers",
        "statistical analysis",
        "IG methods",
        "word relation",
        "sentiment features",
        "related features",
        "unigram features",
        "single word",
        "one classifier",
        "SVM classifiers",
        "two types",
        "high accuracy",
        "95% average accuracy",
        "English language",
        "Complementary NB",
        "MEM classifier",
        "English datasets",
        "SA studies",
        "Nicholls",
        "difference",
        "Agarwal",
        "mRMR",
        "NBM",
        "Abbasi",
        "EWGA",
        "Xia",
        "design",
        "Bai",
        "vocabulary",
        "Mladenovic",
        "mapping",
        "Zheng",
        "languages",
        "Republic",
        "Finnish",
        "garian",
        "instance",
        "something",
        "teristics",
        "WLLR",
        "combinations",
        "n-grams",
        "sequences",
        "Akba",
        "CHI2",
        "85.",
        "92",
        "optimal orthogonal cen- troid",
        "new query expansion ranking",
        "traditional text classification tasks",
        "several feature selection methods",
        "common feature selection methods",
        "high information gain scores",
        "follow- ing reasons",
        "decision tree classifier",
        "Turkish review datasets",
        "English review datasets",
        "good classification performance",
        "large vocabulary sizes",
        "many classification problems",
        "J48 classification methods",
        "total feature sizes",
        "Chi square score",
        "Such methods",
        "higher score",
        "low score",
        "feature f",
        "four classifiers",
        "tain measures",
        "same time",
        "effec- tiveness",
        "distinct values",
        "same way",
        "contingency table",
        "total number",
        "related class",
        "class c",
        "other class",
        "many documents",
        "valuable features",
        "previous study",
        "different classes",
        "P(aj",
        "m number",
        "probability distribution",
        "entropy values",
        "attribute value",
        "experiments",
        "SMO",
        "LR",
        "specific",
        "efficiency",
        "range",
        "500 increments",
        "improvement",
        "content",
        "power",
        "tainty",
        "Hum",
        "Cent",
        "definitions",
        "reviews",
        "positive",
        "egories",
        "keyword",
        "formulas",
        "dependence",
        "3000",
        "The Chi square statistics",
        "Query expansion ranking method",
        "probabilistic weighting model",
        "similar document frequencies",
        "high dimensional vectors",
        "Document frequency difference",
        "query expansion methods",
        "query expansion technique",
        "Optimal orthogonal centroid",
        "orthogonal centroid algorithm",
        "many sentiment-expressing features",
        "one specific class",
        "document vectors",
        "CHI2 method",
        "bigger difference",
        "document collection",
        "One problem",
        "One limitation",
        "new terms",
        "original query",
        "expanded query",
        "big impact",
        "H(C",
        "+ D",
        "big issue",
        "sentiment word",
        "optimized form",
        "Docu- ments",
        "age vector",
        "DFD method",
        "high scores",
        "centroid m",
        "OCFS method",
        "negative class",
        "rare features",
        "individual review",
        "xi N",
        "positive class",
        "highest scores",
        "Nc N",
        "relevant documents",
        "equal number",
        "classes",
        "dataset",
        "text",
        "|A",
        "aj",
        "CB",
        "classification",
        "Song",
        "DFf",
        "weights",
        "importance",
        "subset",
        "sum",
        "distances",
        "subspace",
        "mc",
        "mf",
        "values",
        "centroids",
        "IR",
        "Scoref",
        "Harman",
        "∑",
        "χ",
        "tional topic-based text classification",
        "QER feature selection method",
        "information retrieval strategy",
        "statistical language processing",
        "informa- tion gain",
        "feature selection methods",
        "efficient feature selector",
        "information retrieval system",
        "score computation method",
        "k new terms",
        "classification process",
        "score formula",
        "ranking score",
        "k terms",
        "high scored",
        "Sparck Jones",
        "raw counts",
        "negative classes",
        "small constants",
        "similar constants",
        "data smoothing",
        "fea- ture",
        "multiple classes",
        "two classes",
        "greedy approach",
        "related classes",
        "high values",
        "distance/similarity measures",
        "document frequencies",
        "Q. Equation",
        "term f",
        "negative documents",
        "related documents",
        "valuable terms",
        "top-k scores",
        "ability scores",
        "compute scores",
        "lowest scores",
        "zero probabilities",
        "query Q",
        "probability calculations",
        "sentiment-expressing features",
        "positive documents",
        "precision",
        "man",
        "set",
        "pf",
        "qf",
        "Robertson",
        "Eq.",
        "ratio",
        "Eqs",
        "log2",
        "DF",
        "N−",
        "numbers",
        "numerators",
        "denominators",
        "CHI",
        "collection",
        "vectors",
        "review",
        "introduction",
        "1−",
        "2.5 GHz Intel Core i7 processor",
        "The English movie review dataset",
        "The Turkish product review dataset",
        "The Turkish movie reviews",
        "English product review dataset",
        "English product reviews",
        "multi-fold cross validations",
        "fivefold cross validation",
        "Turkish product reviews",
        "same dataset sizes",
        "punctua- tion marks",
        "five F measures",
        "large feature space",
        "average F measures",
        "final classification results",
        "English reviews",
        "final P",
        "classification system",
        "five folds",
        "punctuation marks",
        "question marks",
        "addi- tion",
        "five portions",
        "reduc- tion",
        "978 negative reviews",
        "700 negative reviews",
        "1000 negative reviews",
        "feature spaces",
        "four subsets",
        "elec- tronics",
        "kitchen appliances",
        "existing work",
        "similar datasets",
        "composite score",
        "harmonic mean",
        "Experimental settings",
        "MacBook Pro",
        "16 GB 1600 MHz",
        "NLTK [38] library",
        "case normalization",
        "docu- ments",
        "two probabilities",
        "commerce website",
        "Performance evaluation",
        "classified items",
        "LR classifier",
        "document",
        "quencies",
        "beyazperde",
        "1057 positive",
        "hepsiburada",
        "books",
        "DVDs",
        "700 positive",
        "Lee",
        "1000 positive",
        "Blitzer",
        "electronics",
        "recall",
        "Python",
        "words",
        "exclamation",
        "polarity",
        "suffixes",
        "J48",
        "MEM.",
        "couple",
        "thousands",
        "training",
        "remaining",
        "testing",
        "Table",
        "memory",
        "features",
        "NBM SVM J48 LR Movie",
        "NBM SVM J48 LR Features",
        "five folds cross validation",
        "six fea- ture sizes",
        "smaller fea- ture sizes",
        "five feature selection methods",
        "best micro-average F measure",
        "bigger feature sizes",
        "post hoc tests",
        "desirable size n",
        "senti- ment analysis",
        "electronic review dataset",
        "topical text classification",
        "kitchen review datasets",
        "best classification results",
        "electronics review dataset",
        "Table 1 Baseline results",
        "NBM classifier",
        "best performer",
        "top-ranked features",
        "new method",
        "best results",
        "text classifiers",
        "possible improvements",
        "total features",
        "related formulas",
        "ing classifier",
        "same performance",
        "different documents",
        "kitchen reviews",
        "univariate ANOVA",
        "significant differences",
        "three groups",
        "95% confidence level",
        "two groups",
        "new results",
        "baseline systems",
        "low frequencies",
        "same intuition",
        "baselines",
        "Book",
        "scores",
        "settings",
        "120 experiments",
        "pairs",
        "bold-face",
        "Note",
        "expressing",
        "traditional",
        "Cai",
        "2×",
        "C FS C H I2",
        "text classifi- cation",
        "traditional topic-based text",
        "Q ER D",
        "review datasets",
        "QER method",
        "One reason",
        "decision-tree-based solution",
        "smaller set",
        "ss ifi",
        "ze F",
        "evant features",
        "result",
        "classifier",
        "rs",
        "rk",
        "FD",
        "IG"
      ]
    }
  ]
}