### Query 1 ### 
{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)",
  "@search.nextPageParameters": {
    "search": "search=AI&$select=title,instructor,entities",
    "skip": 50
  },
  "value": [
    {
      "@search.score": 17.164331,
      "Key": "ms-learn13f442b0-f1d1-4402-8c3e-13f0c8b6ce8d",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 16.195065,
      "Key": "ms-learn5244e529-28c6-4b6c-bba0-a3edcb956327",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 16.195065,
      "Key": "ms-learn7e28c65f-5a23-47e7-b575-66139b3a43ca",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 14.791975,
      "Key": "ms-learn683814dd-934d-4e72-a5fd-81a3c8037999",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 14.791975,
      "Key": "ms-learne00a9b7f-cd37-41b1-87d3-d87018dcf606",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 14.695646,
      "Key": "ms-learn293c8e0b-a6b6-4da7-8e15-cc8672be21cf",
      "description": "Explore Azure Cognitive Search to discover how to create an index, import data, and query the index for better search results.",
      "duration": "53",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.61,
      "rating_count": 217,
      "role": "developer",
      "source": "MS Learn",
      "title": "Introduction to Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learn6057c369-b74e-459f-851c-8e771063754b",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learn78329655-7993-4435-a246-b73e555909f3",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learnc960fef1-1977-4b34-8f09-f00cbe326e34",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learnfac634ec-2700-4257-85c1-066454b639d4",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.2997675,
      "Key": "ms-learn0fcb600b-4bb5-480c-97ed-63cf6d842fe8",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.2997675,
      "Key": "ms-learnf41c2cab-9943-42db-a0c9-8bed47619c53",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 11.800348,
      "Key": "ms-learn67d5d774-bb50-4f9f-8507-b1d7500772e6",
      "description": "Explore Azure Cognitive Search to discover how to create an index, import data, and query the index for better search results.",
      "duration": "53",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.61,
      "rating_count": 217,
      "role": "developer",
      "source": "MS Learn",
      "title": "Introduction to Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learn1918bcce-cb5a-4db1-871e-052f4af46b79",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learn2f19c7cb-45b5-45f1-a6ec-00409eadab7b",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learn4905153f-1a39-40b4-bcc2-b07638f69296",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learna838646f-97c5-4312-b39d-f419a1312b58",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn13a499a7-5603-4803-b239-a48c9a217e67",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "administrator",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn3771573c-1c94-4adf-9b70-021b7f096f9d",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "administrator",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn4d782479-7739-4f0f-a5bb-77d01d11f48d",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "devops-engineer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn6b15a22b-e603-48a3-8d61-c01ffce2dc52",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn736cf038-17f7-4270-a979-ac94e3483917",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "developer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learnab300ac1-0df5-46ac-b8f8-9d50cfe297fb",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learncfe0bf9b-74a0-46de-934a-9ff386ff1fa5",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "devops-engineer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learne4d66639-f3bf-414c-a062-ea7c7e2f2974",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "developer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 7.25819,
      "Key": "ms-learn359e902e-9270-42de-9687-536b9fe0164c",
      "description": "Learn about AI Builder licensing and how to buy and manage your AI Builder licenses.",
      "duration": "40",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.59,
      "rating_count": 17,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder licensing",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-licensing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 7.25819,
      "Key": "ms-learnba340cd7-5db3-42f8-bdd7-036a6974f5f1",
      "description": "Learn about AI Builder licensing and how to buy and manage your AI Builder licenses.",
      "duration": "40",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.59,
      "rating_count": 17,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder licensing",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-licensing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn0197c8c6-dfc4-450b-9fa7-3f610977cc79",
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.61,
      "rating_count": 197,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Text recognition",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn15e073b4-6de0-4d11-b12a-32d65151d584",
      "description": "Learn about AI Builder Sentiment analysis and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.65,
      "rating_count": 170,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Sentiment analysis",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-sentiment-analysis/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn2adf55dd-65ba-4833-976a-c6452dd7a775",
      "description": "Explore AI Builder Language detection and learn how to use it with other Power Platform products.",
      "duration": "27",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.63,
      "rating_count": 139,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Language detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-language-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn73883ec6-3887-45a6-a027-bc9f2892392b",
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.61,
      "rating_count": 197,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Text recognition",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learna2535dbf-ec97-4265-a26b-5b5c0aa689c3",
      "description": "Learn the basics of AI Builder Object detection and how it can benefit your organization.",
      "duration": "25",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.73,
      "rating_count": 147,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Object detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-object-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learnba849c68-1b7c-475d-8ea9-a9e255411450",
      "description": "Learn the basics of AI Builder Object detection and how it can benefit your organization.",
      "duration": "25",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.73,
      "rating_count": 147,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Object detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-object-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learnd56b0c83-5e61-466a-972b-b95682fcfeb2",
      "description": "Explore AI Builder Language detection and learn how to use it with other Power Platform products.",
      "duration": "27",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.63,
      "rating_count": 139,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Language detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-language-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learnfc2cc759-b7e1-4d52-82f4-c26e23404919",
      "description": "Learn about AI Builder Sentiment analysis and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.65,
      "rating_count": 170,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Sentiment analysis",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-sentiment-analysis/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.678481,
      "Key": "ms-learn683b3d6c-a177-4384-a4cb-6f461202de0e",
      "description": "Learn the basics of AI Builder usage in Microsoft Power Automate and how it can benefit your organization.",
      "duration": "60",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.71,
      "rating_count": 702,
      "role": "maker",
      "source": "MS Learn",
      "title": "Use AI Builder in Power Automate",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-power-automate/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.678481,
      "Key": "ms-learna2cadb41-c6a9-4c0a-a9df-6054df03bd5f",
      "description": "Explore the AI Builder Key phrase extraction model and learn how to use it with other Power Platform products.",
      "duration": "52",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.67,
      "rating_count": 178,
      "role": "maker",
      "source": "MS Learn",
      "title": "Introduction to AI Builder Key phrase extraction",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-key-phrase-extraction/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.678481,
      "Key": "ms-learneedc2427-1be9-4593-8c65-a135f852ddff",
      "description": "Explore the AI Builder Key phrase extraction model and learn how to use it with other Power Platform products.",
      "duration": "52",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.67,
      "rating_count": 178,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Introduction to AI Builder Key phrase extraction",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-key-phrase-extraction/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.634366,
      "Key": "ms-learn7be24452-07d1-4acc-99b3-89b901049505",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.634366,
      "Key": "ms-learnbdb45f14-e397-40ab-bf99-d3ff1cc0ea43",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-bot-service",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.634366,
      "Key": "ms-learnefcdc4e3-d588-4b25-b109-65b7c593572b",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learn13636881-10bb-406c-a7a0-4e77a1e37d5a",
      "description": "Learn the basics of invoice processing in AI Builder and how it can benefit your organization.",
      "duration": "10",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.81,
      "rating_count": 21,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with invoice processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-invoice-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learn2f945536-a4a5-4c71-983d-717dba1a40c6",
      "description": "Learn the basics of receipt processing in AI Builder and how it can benefit your organization.",
      "duration": "12",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.79,
      "rating_count": 208,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with receipt processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-receipt-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learn538855b9-afd9-4877-8730-2f93256b0ad2",
      "description": "Learn the basics of receipt processing in AI Builder and how it can benefit your organization.",
      "duration": "12",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.79,
      "rating_count": 208,
      "role": "administrator",
      "source": "MS Learn",
      "title": "Get started with receipt processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-receipt-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learnb1ba00e9-49f3-4fb0-8cba-d67a7d207f84",
      "description": "Learn how to use Ai Builder Text Recognition to automate the tracking of shipments.",
      "duration": "108",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.8,
      "rating_count": 10,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Apply AI Builder Text Recognition in the transportation industry",
      "url": "https://docs.microsoft.com/en-us/learn/modules/apply-ai-builder-text-recognition-transportation-industry/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learnb4303e0a-a9ba-4900-a370-4ff2f1e43ba0",
      "description": "Learn how to use Ai Builder Text Recognition to automate the tracking of shipments.",
      "duration": "108",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.8,
      "rating_count": 10,
      "role": "maker",
      "source": "MS Learn",
      "title": "Apply AI Builder Text Recognition in the transportation industry",
      "url": "https://docs.microsoft.com/en-us/learn/modules/apply-ai-builder-text-recognition-transportation-industry/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learnfd9c0ab6-fac1-4786-9d9e-319d219af5ee",
      "description": "Learn the basics of invoice processing in AI Builder and how it can benefit your organization.",
      "duration": "10",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.81,
      "rating_count": 21,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with invoice processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-invoice-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.4981384,
      "Key": "ms-learn11a560e9-d107-4229-8b6b-f99cf3c4920b",
      "description": "This self-paced module helps you build an AI model from the beginning and shows how you can use it in your business without writing a single line of code.",
      "duration": "30",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.75,
      "rating_count": 1744,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.4981384,
      "Key": "ms-learn2b006004-9810-4dd1-9232-694e89f5510d",
      "description": "This self-paced module helps you build an AI model from the beginning and shows how you can use it in your business without writing a single line of code.",
      "duration": "30",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.75,
      "rating_count": 1744,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.4771347,
      "Key": "ms-learn1545c987-7064-4af9-b056-b1e0c2669385",
      "description": "Learn about AI Builder Business card reader and how to use it in Microsoft Power Apps and Power Automate.",
      "duration": "90",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.62,
      "rating_count": 157,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Business card reader",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-business-card-reader/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    }
  ],
  "@odata.nextLink": "https://atc-aisearch.search.windows.net/indexes/azuretable-index/docs/search?api-version=2024-03-01-preview"
}

### Query 2 ###
{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)",
  "@search.nextPageParameters": {
    "search": "search=AI&$select=metadata_title,metadata_author,publicationName,publisher,publicationDate",
    "skip": 50
  },
  "value": [
    {
      "@search.score": 17.164331,
      "Key": "ms-learn13f442b0-f1d1-4402-8c3e-13f0c8b6ce8d",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 16.195065,
      "Key": "ms-learn5244e529-28c6-4b6c-bba0-a3edcb956327",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 16.195065,
      "Key": "ms-learn7e28c65f-5a23-47e7-b575-66139b3a43ca",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 14.791975,
      "Key": "ms-learn683814dd-934d-4e72-a5fd-81a3c8037999",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 14.791975,
      "Key": "ms-learne00a9b7f-cd37-41b1-87d3-d87018dcf606",
      "description": "Create an Azure Cognitive Search solution",
      "duration": "63",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.91,
      "rating_count": 45,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create an Azure Cognitive Search solution",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-azure-cognitive-search-solution/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 14.695646,
      "Key": "ms-learn293c8e0b-a6b6-4da7-8e15-cc8672be21cf",
      "description": "Explore Azure Cognitive Search to discover how to create an index, import data, and query the index for better search results.",
      "duration": "53",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.61,
      "rating_count": 217,
      "role": "developer",
      "source": "MS Learn",
      "title": "Introduction to Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learn6057c369-b74e-459f-851c-8e771063754b",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learn78329655-7993-4435-a246-b73e555909f3",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learnc960fef1-1977-4b34-8f09-f00cbe326e34",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.822708,
      "Key": "ms-learnfac634ec-2700-4257-85c1-066454b639d4",
      "description": "Create a knowledge store with Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-search",
      "rating_average": 4.79,
      "rating_count": 42,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a knowledge store with Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-knowledge-store-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.2997675,
      "Key": "ms-learn0fcb600b-4bb5-480c-97ed-63cf6d842fe8",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 13.2997675,
      "Key": "ms-learnf41c2cab-9943-42db-a0c9-8bed47619c53",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 11.800348,
      "Key": "ms-learn67d5d774-bb50-4f9f-8507-b1d7500772e6",
      "description": "Explore Azure Cognitive Search to discover how to create an index, import data, and query the index for better search results.",
      "duration": "53",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.61,
      "rating_count": 217,
      "role": "developer",
      "source": "MS Learn",
      "title": "Introduction to Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learn1918bcce-cb5a-4db1-871e-052f4af46b79",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learn2f19c7cb-45b5-45f1-a6ec-00409eadab7b",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-functions",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learn4905153f-1a39-40b4-bcc2-b07638f69296",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "developer",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.92741,
      "Key": "ms-learna838646f-97c5-4312-b39d-f419a1312b58",
      "description": "Create a custom skill for Azure Cognitive Search",
      "duration": "46",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "vs-code",
      "rating_average": 4.81,
      "rating_count": 31,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Create a custom skill for Azure Cognitive Search",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-enrichment-pipeline-azure-cognitive-search/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn13a499a7-5603-4803-b239-a48c9a217e67",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "administrator",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn3771573c-1c94-4adf-9b70-021b7f096f9d",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "administrator",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn4d782479-7739-4f0f-a5bb-77d01d11f48d",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "devops-engineer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn6b15a22b-e603-48a3-8d61-c01ffce2dc52",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learn736cf038-17f7-4270-a979-ac94e3483917",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "developer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learnab300ac1-0df5-46ac-b8f8-9d50cfe297fb",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "github",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "solution-architect",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learncfe0bf9b-74a0-46de-934a-9ff386ff1fa5",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "devops-engineer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 10.323752,
      "Key": "ms-learne4d66639-f3bf-414c-a062-ea7c7e2f2974",
      "description": "Learn to search and organize repository history by using filters, blame, and cross-linking on GitHub.",
      "duration": "38",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure",
      "rating_average": 4.79,
      "rating_count": 241,
      "role": "developer",
      "source": "MS Learn",
      "title": "Search and organize repository history by using GitHub",
      "url": "https://docs.microsoft.com/en-us/learn/modules/search-organize-repository-history-github/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 7.25819,
      "Key": "ms-learn359e902e-9270-42de-9687-536b9fe0164c",
      "description": "Learn about AI Builder licensing and how to buy and manage your AI Builder licenses.",
      "duration": "40",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.59,
      "rating_count": 17,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder licensing",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-licensing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 7.25819,
      "Key": "ms-learnba340cd7-5db3-42f8-bdd7-036a6974f5f1",
      "description": "Learn about AI Builder licensing and how to buy and manage your AI Builder licenses.",
      "duration": "40",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.59,
      "rating_count": 17,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder licensing",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-licensing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn0197c8c6-dfc4-450b-9fa7-3f610977cc79",
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.61,
      "rating_count": 197,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Text recognition",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn15e073b4-6de0-4d11-b12a-32d65151d584",
      "description": "Learn about AI Builder Sentiment analysis and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.65,
      "rating_count": 170,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Sentiment analysis",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-sentiment-analysis/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn2adf55dd-65ba-4833-976a-c6452dd7a775",
      "description": "Explore AI Builder Language detection and learn how to use it with other Power Platform products.",
      "duration": "27",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.63,
      "rating_count": 139,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Language detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-language-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learn73883ec6-3887-45a6-a027-bc9f2892392b",
      "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.61,
      "rating_count": 197,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Text recognition",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learna2535dbf-ec97-4265-a26b-5b5c0aa689c3",
      "description": "Learn the basics of AI Builder Object detection and how it can benefit your organization.",
      "duration": "25",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.73,
      "rating_count": 147,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Object detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-object-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learnba849c68-1b7c-475d-8ea9-a9e255411450",
      "description": "Learn the basics of AI Builder Object detection and how it can benefit your organization.",
      "duration": "25",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.73,
      "rating_count": 147,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder Object detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-object-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learnd56b0c83-5e61-466a-972b-b95682fcfeb2",
      "description": "Explore AI Builder Language detection and learn how to use it with other Power Platform products.",
      "duration": "27",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.63,
      "rating_count": 139,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Language detection",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-language-detection/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.8102965,
      "Key": "ms-learnfc2cc759-b7e1-4d52-82f4-c26e23404919",
      "description": "Learn about AI Builder Sentiment analysis and how to use it with other Power Platform products.",
      "duration": "55",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.65,
      "rating_count": 170,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Sentiment analysis",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-sentiment-analysis/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.678481,
      "Key": "ms-learn683b3d6c-a177-4384-a4cb-6f461202de0e",
      "description": "Learn the basics of AI Builder usage in Microsoft Power Automate and how it can benefit your organization.",
      "duration": "60",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.71,
      "rating_count": 702,
      "role": "maker",
      "source": "MS Learn",
      "title": "Use AI Builder in Power Automate",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-power-automate/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.678481,
      "Key": "ms-learna2cadb41-c6a9-4c0a-a9df-6054df03bd5f",
      "description": "Explore the AI Builder Key phrase extraction model and learn how to use it with other Power Platform products.",
      "duration": "52",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.67,
      "rating_count": 178,
      "role": "maker",
      "source": "MS Learn",
      "title": "Introduction to AI Builder Key phrase extraction",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-key-phrase-extraction/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.678481,
      "Key": "ms-learneedc2427-1be9-4593-8c65-a135f852ddff",
      "description": "Explore the AI Builder Key phrase extraction model and learn how to use it with other Power Platform products.",
      "duration": "52",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.67,
      "rating_count": 178,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Introduction to AI Builder Key phrase extraction",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-key-phrase-extraction/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.634366,
      "Key": "ms-learn7be24452-07d1-4acc-99b3-89b901049505",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-machine-learning",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.634366,
      "Key": "ms-learnbdb45f14-e397-40ab-bf99-d3ff1cc0ea43",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-bot-service",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.634366,
      "Key": "ms-learnefcdc4e3-d588-4b25-b109-65b7c593572b",
      "description": "Get started with AI on Azure",
      "duration": "34",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "azure-cognitive-services",
      "rating_average": 4.78,
      "rating_count": 10997,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Get started with AI on Azure",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-ai-fundamentals/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learn13636881-10bb-406c-a7a0-4e77a1e37d5a",
      "description": "Learn the basics of invoice processing in AI Builder and how it can benefit your organization.",
      "duration": "10",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.81,
      "rating_count": 21,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with invoice processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-invoice-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learn2f945536-a4a5-4c71-983d-717dba1a40c6",
      "description": "Learn the basics of receipt processing in AI Builder and how it can benefit your organization.",
      "duration": "12",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.79,
      "rating_count": 208,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with receipt processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-receipt-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learn538855b9-afd9-4877-8730-2f93256b0ad2",
      "description": "Learn the basics of receipt processing in AI Builder and how it can benefit your organization.",
      "duration": "12",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.79,
      "rating_count": 208,
      "role": "administrator",
      "source": "MS Learn",
      "title": "Get started with receipt processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-receipt-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learnb1ba00e9-49f3-4fb0-8cba-d67a7d207f84",
      "description": "Learn how to use Ai Builder Text Recognition to automate the tracking of shipments.",
      "duration": "108",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.8,
      "rating_count": 10,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Apply AI Builder Text Recognition in the transportation industry",
      "url": "https://docs.microsoft.com/en-us/learn/modules/apply-ai-builder-text-recognition-transportation-industry/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learnb4303e0a-a9ba-4900-a370-4ff2f1e43ba0",
      "description": "Learn how to use Ai Builder Text Recognition to automate the tracking of shipments.",
      "duration": "108",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.8,
      "rating_count": 10,
      "role": "maker",
      "source": "MS Learn",
      "title": "Apply AI Builder Text Recognition in the transportation industry",
      "url": "https://docs.microsoft.com/en-us/learn/modules/apply-ai-builder-text-recognition-transportation-industry/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.6089506,
      "Key": "ms-learnfd9c0ab6-fac1-4786-9d9e-319d219af5ee",
      "description": "Learn the basics of invoice processing in AI Builder and how it can benefit your organization.",
      "duration": "10",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.81,
      "rating_count": 21,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with invoice processing in AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/ai-builder-invoice-processing/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.4981384,
      "Key": "ms-learn11a560e9-d107-4229-8b6b-f99cf3c4920b",
      "description": "This self-paced module helps you build an AI model from the beginning and shows how you can use it in your business without writing a single line of code.",
      "duration": "30",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.75,
      "rating_count": 1744,
      "role": "maker",
      "source": "MS Learn",
      "title": "Get started with AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.4981384,
      "Key": "ms-learn2b006004-9810-4dd1-9232-694e89f5510d",
      "description": "This self-paced module helps you build an AI model from the beginning and shows how you can use it in your business without writing a single line of code.",
      "duration": "30",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.75,
      "rating_count": 1744,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    },
    {
      "@search.score": 6.4771347,
      "Key": "ms-learn1545c987-7064-4af9-b056-b1e0c2669385",
      "description": "Learn about AI Builder Business card reader and how to use it in Microsoft Power Apps and Power Automate.",
      "duration": "90",
      "instructor": null,
      "level": "System.Byte[]",
      "product": "ai-builder",
      "rating_average": 4.62,
      "rating_count": 157,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Get started with AI Builder Business card reader",
      "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-business-card-reader/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "ms",
        "learn"
      ],
      "instructorName": [],
      "instructorBio": []
    }
  ],
  "@odata.nextLink": "https://atc-aisearch.search.windows.net/indexes/azuretable-index/docs/search?api-version=2024-03-01-preview"
}

### Query 3 ###
{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 8.221968,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 593265,
      "metadata_storage_name": "Köchling-Wehner2020_Article_DiscriminatedByAnAlgorithmASys.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L0slQzMlQjZjaGxpbmctV2VobmVyMjAyMF9BcnRpY2xlX0Rpc2NyaW1pbmF0ZWRCeUFuQWxnb3JpdGhtQVN5cy5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Alina Köchling ",
      "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "metadata_creation_date": "2020-11-19T15:45:16Z",
      "keyphrases": [
        "200 artificial intelligence (AI) specialists",
        "two essential HR functions",
        "Marius Claus Wehner1",
        "Heinrich-Heine-University Düsseldorf",
        "major driving forces",
        "human resource management",
        "Alina Köchling1",
        "routinized workplace decisions",
        "current human resource",
        "crucial future research",
        "HRM � Literature review",
        "Abstract Algorithmic decision-making",
        "Keywords Fairness � Discrimination",
        "human biases",
        "systematic review",
        "HR recruitment",
        "HR development",
        "HR) practices",
        "ORIGINAL RESEARCH",
        "new source",
        "unfair treatment",
        "implicit discrimination",
        "implicit) discrimination",
        "current state",
        "research gaps",
        "36 journal articles",
        "possible pitfalls",
        "important theoretical",
        "practical implications",
        "fruitful avenues",
        "fairness � Ethics",
        "rapid growth",
        "Business Administration",
        "Universitätsstrasse",
        "Business Research",
        "automated decision-making",
        "remote control",
        "Möhlmann",
        "important individual",
        "societal implications",
        "organizational optimization",
        "hidden talented",
        "large number",
        "German companies",
        "competitive advantages",
        "commercial providers",
        "algorithmic platforms",
        "performance measurements",
        "large companies",
        "economic reasons",
        "personal beliefs",
        "doi.org",
        "orcid.org",
        "context",
        "Author",
        "advice",
        "firms",
        "costs",
        "efficiency",
        "objectivity",
        "groups",
        "people",
        "unfairness",
        "knowledge",
        "threats",
        "goal",
        "directions",
        "applications",
        "researchers",
        "practitioners",
        "1 Introduction",
        "information",
        "importance",
        "digitalization",
        "organizations",
        "koechling",
        "hhu",
        "1 Faculty",
        "Economics",
        "40225 Dusseldorf",
        "Germany",
        "crossmark",
        "crossref",
        "standardization",
        "Zalmanson",
        "Algorithms",
        "humans",
        "Chalfin",
        "Lee",
        "Lindebaum",
        "changes",
        "favor",
        "employees",
        "Silverman",
        "Waller",
        "Carey",
        "Smith",
        "Savage",
        "Bales",
        "survey",
        "Deloitte",
        "Several",
        "Google",
        "IBM",
        "SAP",
        "Microsoft",
        "systems",
        "hiring",
        "Walker",
        "turn",
        "Vodafone",
        "Unilever",
        "Ikea",
        "Daugherty",
        "Wilson",
        "Precire",
        "savings",
        "time",
        "risks",
        "productivity",
        "certainty",
        "Suen",
        "McDonald",
        "McColl",
        "Michelotti",
        "Woods",
        "prejudices",
        "15",
        "American e-commerce specialist",
        "algorithmic decision- making",
        "new research avenues",
        "HR development processes",
        "unrepresentative input data",
        "algorithmic decision-making system",
        "complete algorithmic decision-making",
        "training) data",
        "algorithmic outcomes",
        "same attention",
        "same requirements",
        "first glance",
        "human decision-making",
        "unequal treatment",
        "different groups",
        "qualitative differences",
        "individual performance",
        "biased outcomes",
        "biased decisions",
        "current debate",
        "extreme disadvantage",
        "796 Business Research",
        "Previous studies",
        "common procedures",
        "labor shortages",
        "one hand",
        "other side",
        "computer science",
        "neous state",
        "distinct challenges",
        "future research",
        "practical point",
        "known companies",
        "negative consequences",
        "potential downsides",
        "potential dangers",
        "possible threat",
        "prominent example",
        "female applicants",
        "hiring decision",
        "employees’ acceptance",
        "existing knowledge",
        "current literature",
        "hiring algorithms",
        "potential biases",
        "HRM context",
        "consistency",
        "fairness",
        "Langer",
        "Florentine",
        "Raghavan",
        "application",
        "criteria",
        "Lepri",
        "discrimination",
        "Simbeck",
        "gender",
        "ethnicity",
        "Arrow",
        "Kim",
        "Barocas",
        "Selbst",
        "Suresh",
        "Guttag",
        "Chander",
        "issue",
        "transparency",
        "Dwork",
        "Diakopoulos",
        "Amazon",
        "Dastin",
        "Miller",
        "lack",
        "accountability",
        "factors",
        "Citron",
        "Pasquale",
        "question",
        "Kaibel",
        "discrepancy",
        "enthusiasm",
        "panacea",
        "inefficiencies",
        "field",
        "infancy",
        "digitization",
        "automation",
        "view",
        "aim",
        "study",
        "awareness",
        "The Oxford Living Dictionary",
        "two important HR areas",
        "several different research areas",
        "various algorithmic decision-making tools",
        "several AI decision tools",
        "other problem-solving operations",
        "two HR functions",
        "explicit human interference",
        "natural language processing",
        "several research gaps",
        "future research avenues",
        "systematic literature review",
        "HR development methods",
        "future research directions",
        "several ways",
        "workplace decision",
        "human intelligence",
        "algorithmic selection",
        "career development",
        "future performance",
        "person-organization fit",
        "serious consequences",
        "Ötting",
        "existing body",
        "ethical issues",
        "illustrative examples",
        "timely topic",
        "enormous importance",
        "reputational risk",
        "ment process",
        "key terms",
        "descriptive analysis",
        "subsequent discussion",
        "theoretical implications",
        "Conceptual background",
        "computational mechanism",
        "umbrella term",
        "wide array",
        "machine learning",
        "image recognition",
        "existing literature",
        "AI applications",
        "current employees",
        "HRM field",
        "lacking focus",
        "statistical models",
        "new data",
        "present paper",
        "distributive fairness",
        "understanding",
        "end",
        "potential",
        "Huselid",
        "Decisions",
        "algorithms",
        "individuals",
        "company",
        "society",
        "ethics",
        "procedural",
        "Maier",
        "Tambe",
        "Cappelli",
        "bias",
        "fact",
        "Companies",
        "legal",
        "applicants",
        "reason",
        "guidance",
        "detailed",
        "definitions",
        "methodology",
        "illustration",
        "processes",
        "sets",
        "rules",
        "calculations",
        "computer",
        "routinized",
        "basis",
        "prescriptions",
        "acquisition",
        "environment",
        "Russell",
        "Norvig",
        "ML",
        "speech",
        "NLP",
        "Kaplan",
        "Haenlein",
        "Paschen",
        "2.1",
        "many AI decision-making tools",
        "three major types",
        "artificial neural networks",
        "random variable Y",
        "three core elements",
        "three different levels",
        "neural network models",
        "deep learning models",
        "fixed input/output data",
        "reinforcement type learning",
        "Unsupervised ML algorithms",
        "reinforcement learning",
        "ML models",
        "network structures",
        "probabilistic models",
        "unsupervised settings",
        "learning tasks",
        "ML approach",
        "regression-type problems",
        "ground truth",
        "Human experts",
        "human decisions",
        "future instances",
        "same problem",
        "priori labeling",
        "structural behaviors",
        "theme analysis",
        "798 Business Research",
        "separate group",
        "error interactions",
        "dynamic environment",
        "Single nodes",
        "human brain",
        "human thinking",
        "other settings",
        "prediction tasks",
        "expected values",
        "systematic error",
        "human judgment",
        "algorithmic evaluations",
        "computer systems",
        "black box",
        "glass boxes",
        "making sense",
        "human involvement",
        "Cognitive biases",
        "entire model",
        "nonlinear transformation",
        "individual components",
        "input data",
        "Shin",
        "predictions",
        "outputs",
        "labels",
        "patterns",
        "Canhoto",
        "contrast",
        "Murphy",
        "variables",
        "methods",
        "trial",
        "bling",
        "methodologies",
        "Examples",
        "complex",
        "relationship",
        "phases",
        "layers",
        "neurons",
        "Bengio",
        "multiple",
        "stages",
        "features",
        "representations",
        "advantage",
        "Deng",
        "Yu",
        "Reason",
        "estimation",
        "bY",
        "difference",
        "Kauermann",
        "Kuechenhoff",
        "Goodfellow",
        "uncertainty",
        "Kahneman",
        "others",
        "Friedman",
        "Nissenbaum",
        "HRM",
        "Cheng",
        "Hackett",
        "theory",
        "consideration",
        "distinction",
        "interpretability",
        "explainability",
        "Roscher",
        "combination",
        "former",
        "simulatability",
        "parameters",
        "2.2",
        "predictive algorithmic decision-making tool",
        "accurate algorithmic output",
        "explicit human judgments",
        "input data set",
        "historical employment data",
        "algorithmic transparency",
        "historical data",
        "ML model",
        "different sets",
        "HR department",
        "employee satisfaction",
        "turnover intentions",
        "specific criteria",
        "underlying reasons",
        "main reasons",
        "learning process",
        "exposed examples",
        "one way",
        "subsequent analysis",
        "worst case",
        "discriminatory outputs",
        "manual programming",
        "recruitment process",
        "white men",
        "protected group",
        "systematic disadvantage",
        "biased data",
        "historical biases",
        "different biases",
        "preexisting biases",
        "Contextual information",
        "emergent bias",
        "representation bias",
        "specific information",
        "personality characteristics",
        "ML algorithm",
        "valid example",
        "implicit bias",
        "decomposability",
        "level",
        "training",
        "Interpretability",
        "element",
        "domain",
        "interpretations",
        "derive",
        "conclusions",
        "results",
        "prediction",
        "interest",
        "technical",
        "quality",
        "Danks",
        "London",
        "keyword",
        "garbage",
        "stereotypes",
        "Barfield",
        "Pagallo",
        "creation",
        "kind",
        "racist",
        "Veale",
        "Binns",
        "tendencies",
        "past",
        "Datta",
        "Hispanics",
        "applicant",
        "member",
        "job",
        "interview",
        "designer",
        "categories",
        "disadvantages",
        "absence",
        "selection",
        "measurements",
        "information systems Leventhal",
        "new societal knowledge",
        "different fairness definitions",
        "new knowledge",
        "several steps",
        "feature engineering",
        "free lunch",
        "discriminatory outcomes",
        "technical constraints",
        "technical consideration",
        "several reasons",
        "important conditions",
        "human constructs",
        "human interpretation",
        "cultural values",
        "two reasons",
        "different values",
        "design process",
        "cooperative efforts",
        "competitive behavior",
        "equal treatment",
        "meso- level",
        "statistical parity",
        "true outcomes",
        "fair treatment",
        "equal opportunity",
        "individual subjects",
        "representative bias",
        "Technical bias",
        "algorithmic system",
        "system design",
        "measurement data",
        "model testing",
        "best model",
        "800 Business Research",
        "training data",
        "Individual fairness",
        "group membership",
        "computer technology",
        "specific context",
        "cultural context",
        "group fairness",
        "real users",
        "protected) groups",
        "preprocessing",
        "theorems",
        "Wolpert",
        "Macready",
        "choice",
        "benchmark",
        "performance",
        "rotations",
        "example",
        "females",
        "comparison",
        "overrepresented",
        "limited",
        "hardware",
        "software",
        "peripherals",
        "Bozdag",
        "formalization",
        "computers",
        "problem",
        "judgments",
        "intuitions",
        "law",
        "litigation",
        "construction",
        "result",
        "population",
        "shift",
        "mismatch",
        "Table",
        "state",
        "art",
        "competition",
        "individualistic",
        "needs",
        "overview",
        "measures",
        "hand",
        "micro-level",
        "Hardt",
        "notions",
        "sense",
        "positives/negatives",
        "sources",
        "disadvantage",
        "2.3",
        "fairness Name Author Definition Individual fairness Dwork",
        "similar’’ classifications Group fairness",
        "descriptive personal evaluation",
        "considerable adverse consequences",
        "individual differences",
        "actual fairness",
        "fairness perceptions",
        "objective fairness",
        "Subjective fairness",
        "Discriminatory categories",
        "working experience",
        "conscious process",
        "current position",
        "personnel selection",
        "prediction systems",
        "selection context",
        "differential validity",
        "subgroup validity",
        "differential prediction",
        "biased results",
        "Table 1 Definitions",
        "P bY",
        "Pð bY",
        "opportunity Hardt",
        "False-negative rates",
        "1f g",
        "random variable",
        "802 Business Research",
        "selection process",
        "applicant pool",
        "training process",
        "meaningful effects",
        "negative experiences",
        "recruitment context",
        "job offer",
        "job turnover",
        "explicit discrimination",
        "selection measure",
        "essential role",
        "equal probability",
        "conscientious behavior",
        "managerial decisions",
        "Implicit discrimination",
        "implicit attitudes",
        "HR literature",
        "years",
        "Persson",
        "Bertrand",
        "aversion",
        "support",
        "characteristics",
        "Frijters",
        "Cascio",
        "Aguinis",
        "probabilities",
        "estimates",
        "slopes",
        "intercepts",
        "subgroups",
        "Meade",
        "Fetzer",
        "Roth",
        "Bobko",
        "Bartlett",
        "usage",
        "algorithmic",
        "decision-making",
        "regard",
        "justice",
        "reality",
        "Cropanzano",
        "employers",
        "subjects",
        "positive",
        "1jG",
        "recidivism",
        "estimator",
        "likelihood",
        "altruisms",
        "Cohen-Charash",
        "Spector",
        "candidates",
        "Bauer",
        "morale",
        "intentions",
        "acceptance",
        "rejection",
        "dissatisfaction",
        "reduction",
        "elimination",
        "conflicts",
        "Gilliland",
        "McCarthy",
        "Hausknecht",
        "image",
        "systematic literature review approach",
        "systematic literature reviews",
        "Several online platforms",
        "extensive keyword searching",
        "defined individual concepts",
        "three core dimensions",
        "fruitful research avenues",
        "high procedural justice",
        "three fairness dimensions",
        "three dimensions",
        "replicable approach.1",
        "individual fairness",
        "research question",
        "rating companies",
        "Van Hoye",
        "most urgency",
        "equal opportunities",
        "ethical norms",
        "important role",
        "relevant information",
        "algorithmic decision-making",
        "information processing",
        "human raters",
        "employee evaluation",
        "personal contact",
        "reliable picture",
        "discrimination potential",
        "particular field",
        "overall picture",
        "two authors",
        "Distributive justice",
        "informational justice",
        "fairness measures",
        "Search terms",
        "interactional justice",
        "development process",
        "organizational context",
        "evidence-based decisions",
        "decision process",
        "iterative process",
        "interactional fairness",
        "same treatment",
        "possibility",
        "outcome",
        "Rules",
        "equality",
        "equity",
        "accordance",
        "contributions",
        "extent",
        "Leventhal",
        "Consistency",
        "accuracy",
        "representation",
        "parties",
        "correction",
        "appropriateness",
        "dignity",
        "courtesy",
        "respect",
        "share",
        "general",
        "procedures",
        "errors",
        "3 Methods",
        "insights",
        "other",
        "suggestions",
        "Petticrew",
        "Roberts",
        "Crossan",
        "Apaydin",
        "Siddaway",
        "Gough",
        "databases",
        "discussion",
        "keywords",
        "3.1",
        "EBSCO Business Source Premier database",
        "social science citation index",
        "language peer-reviewed journals",
        "first source",
        "singular/plural forms",
        "different spellings",
        "narrow terms",
        "classification terms",
        "complete list",
        "broad coverage",
        "impact factor",
        "reasonable proxy",
        "important publications",
        "reference section",
        "search strings",
        "relevant articles",
        "scholarly literature",
        "electronic databases",
        "terminology",
        "synonyms",
        "contents",
        "priority",
        "balance",
        "sensitivity",
        "specificity",
        "unintentional",
        "omission",
        "SSCI",
        "management",
        "part",
        "Web",
        "Knowledge",
        "breadth",
        "papers"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.14229,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relatives",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2986829,
      "metadata_storage_name": "s40493-015-0019-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxOS16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Partheeban Chandrasekaran",
      "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "metadata_creation_date": "2015-09-04T09:59:41Z",
      "keyphrases": [
        "new classi- fication scheme",
        "peer file-sharing applications",
        "social networking websites",
        "comprehensive test tool",
        "Attribution 4.0 International License",
        "original author(s",
        "three trust algorithms",
        "various trust properties",
        "computational trust mechanisms",
        "online community-based systems",
        "Creative Commons license",
        "computational trust models",
        "social trust models",
        "three reputation systems",
        "different trust models",
        "RESEARCH Open Access",
        "trust assessment workflow",
        "existing reputation systems",
        "existing models",
        "Many models",
        "different stages",
        "Trust Management",
        "trust relationship",
        "Multi-agent systems",
        "Computer Engineering",
        "Introduction Motivation",
        "simple averaging",
        "based scores",
        "Advogato website",
        "application-dependent metrics",
        "transformation stage",
        "unrestricted use",
        "appropriate credit",
        "first contribution",
        "second contribution",
        "evaluation schemes",
        "Trust testbed",
        "Esfandiari Journal",
        "latest model",
        "single model",
        "generic testbed",
        "increasing need",
        "past transactions",
        "graph transformations",
        "Carleton University",
        "larger number",
        "Partheeban Chandrasekaran",
        "slandering attacks",
        "two agents",
        "Babak Esfandiari",
        "DOI",
        "experiments",
        "analysis",
        "Correspondence",
        "Department",
        "1125 Colonel",
        "Drive",
        "Ottawa",
        "Ontario",
        "Canada",
        "Abstract",
        "tesbed",
        "flexibility",
        "design",
        "prototype",
        "EigenTrust",
        "PeerTrust",
        "Appleseed",
        "vulnerabilites",
        "compliance",
        "example",
        "discrepancies",
        "trade-offs",
        "resistance",
        "self-promotion",
        "Keywords",
        "growth",
        "users",
        "history",
        "interactions",
        "calculations",
        "ratings",
        "eBay",
        "researcher",
        "tools",
        "evaluations",
        "set",
        "vulnerabilities",
        "types",
        "Overview",
        "solution",
        "contributions",
        "paper",
        "family",
        "process",
        "succession",
        "vertices",
        "meaning",
        "edges",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "crossmark",
        "org",
        "mailto",
        "Page",
        "presence",
        "existence",
        "variety",
        "certificate authority-based PKI trust models",
        "Trust management systems aid agents",
        "reputation-based trust management systems",
        "three different trust algorithms",
        "public key infras",
        "Social trust models",
        "basic trust properties",
        "social trust algorithms",
        "existing trust algorithms",
        "Beta Reputation System",
        "three systems",
        "reputation systems",
        "chaining algorithms",
        "mutual trust",
        "trust assertions",
        "trust score",
        "existing testbeds",
        "subtle interplay",
        "higher sensitivity",
        "lower sensitivity",
        "literature review",
        "implementation details",
        "actual mechanisms",
        "earlier direct",
        "indirect interactions",
        "e-commerce website",
        "third party",
        "Various inputs",
        "file-sharing network",
        "blogging website",
        "particular level",
        "specific context",
        "simple attacks",
        "self-promoting attacks",
        "Problem description",
        "research problem",
        "particular agent",
        "one attack",
        "friend chain",
        "past experiences",
        "testbed prototype",
        "aggregated trustworthiness",
        "evaluation results",
        "range",
        "differences",
        "way",
        "slandering",
        "Organization",
        "section",
        "Background",
        "state",
        "discussion",
        "Conclusions",
        "limitations",
        "tructures",
        "certificates",
        "The",
        "means",
        "order",
        "instance",
        "sellers",
        "trustworthy",
        "product",
        "drawbacks",
        "Chandrasekaran",
        "root",
        "recommendations",
        "turn",
        "recommenders",
        "Nature",
        "TRAVOS",
        "BRS",
        "satisfaction",
        "transaction",
        "P2P",
        "Aberer",
        "Despotovic",
        "plaints",
        "Advogato",
        "goal",
        "spam",
        "community",
        "TidalTrust",
        "beta probability density function",
        "late indirect trust scores",
        "other agents’ past experiences",
        "Spreading Activation model",
        "local trust scores",
        "global trust algorithms",
        "global trust score",
        "local trust algorithms",
        "three algorithms",
        "general trust",
        "Eigen- Trust",
        "Trust decisions",
        "similarity score",
        "file authenticity",
        "recommenda- tions",
        "different inputs",
        "thresholding techniques",
        "edge weights",
        "maximum “flow",
        "direct experience",
        "party recommendations",
        "different families",
        "next sections",
        "detailed descriptions",
        "respective sections",
        "satisfaction ratings",
        "trust graph",
        "direct interactions",
        "truster agent",
        "Credence",
        "votes",
        "files",
        "trustee",
        "gossiping",
        "information",
        "perspective",
        "survey",
        "Marsh",
        "Abdul-Rahman",
        "case",
        "percentage",
        "top",
        "value",
        "energy",
        "tor",
        "short",
        "third",
        "one",
        "scope",
        "testbed",
        "details",
        "output",
        "readers",
        "high local trust values",
        "low local trust values",
        "local direct trust score",
        "normalized local trust score",
        "global trust score computations",
        "current trust scores",
        "trans- action ratings",
        "local trust vector",
        "global trust scores",
        "fellow malicious agents",
        "gence threshold Tc",
        "global value",
        "trust relationships",
        "trust propagation",
        "trust graphs",
        "flow-based algorithm",
        "trust seed",
        "path length",
        "termination condition",
        "weighted graph",
        "Tij trij",
        "cij elements",
        "factor decay",
        "decay factor",
        "honest agents",
        "agent u",
        "agent j",
        "satisfaction rating",
        "transaction rating",
        "raters",
        "Eq.",
        "transactions",
        "system",
        "method",
        "input",
        "sij",
        "sum",
        "truster",
        "friends",
        "cijcjk",
        "matrix",
        "tik",
        "opinion",
        "number",
        "hops",
        "i.",
        "rank",
        "nodes",
        "trustworthiness",
        "sink",
        "amount",
        "places",
        "loops",
        "Ai",
        "step",
        "∑",
        "∈",
        "open-source message- driven simulation engine",
        "two different targets",
        "two testbed implementations",
        "document recommendation systems",
        "beta probability distribution",
        "local trust algorithm",
        "reputation estimation errors",
        "two testbed models",
        "general-purpose trust models",
        "previous reputation value",
        "art painting sales",
        "general trust systems",
        "The Agent Reputation",
        "beta distribution",
        "two roles",
        "initial value",
        "real systems",
        "trust assessment",
        "brief survey",
        "important role",
        "case studies",
        "direct feedbacks",
        "current reputation",
        "single fixpoint",
        "important stage",
        "one trustworthiness",
        "dom variables",
        "various attacks",
        "particular era",
        "market values",
        "specific era",
        "other eras",
        "reputation scores",
        "trust algorithms",
        "next section",
        "rating function",
        "desirable properties",
        "negative experiences",
        "mutual ratings",
        "target pairing",
        "other agents",
        "Guha Guha",
        "seed",
        "methods",
        "look",
        "andMacau",
        "TREET",
        "investigation",
        "graph",
        "documents",
        "many",
        "specialization",
        "subclass",
        "Hazard",
        "Singh",
        "authors",
        "rater",
        "favor",
        "payoff",
        "belief",
        "likelihood",
        "future",
        "definitions",
        "Monotonicity",
        "computed",
        "b.",
        "Unambiguity",
        "convergence",
        "time",
        "Accuracy",
        "total",
        "update",
        "positive",
        "comparison",
        "resilience",
        "performance",
        "domain",
        "client",
        "paintings",
        "appraisals",
        "expert",
        "dedicated distributed infras- tructure",
        "online product review websites",
        "trust score rep- resentation",
        "graphical user interface",
        "first person point",
        "P2P file-sharing networks",
        "honest sales ratio",
        "trust calculation schemes",
        "decentralized trust algorithms",
        "highest bank balance",
        "Value Imbalance attack",
        "general marketplace scenario",
        "various reputation systems",
        "Reputation Lag attack",
        "honest profit ratio",
        "multiple buyer accounts",
        "Multiple seller accounts",
        "The ART testbed",
        "distributed version",
        "utility value",
        "Proliferation attack",
        "cheater sales",
        "other systems",
        "marketplace domain",
        "Reputation Experimentation",
        "trust mechanism",
        "trust management",
        "The Trust",
        "future interactions",
        "possible messages",
        "time interval",
        "The engine",
        "new clients",
        "correct ability",
        "problem domain",
        "Evaluation Testbed",
        "varying prices",
        "inexpensive items",
        "market competition",
        "selling price",
        "future transactions",
        "following metrics",
        "evaluation metrics",
        "simulation engine",
        "simulation days",
        "trustworthy agents",
        "right agents",
        "participating agents",
        "different agents",
        "centralized version",
        "sale price",
        "random number",
        "collusion attacks",
        "winning agent",
        "1,000 different products",
        "departing agent",
        "protocol",
        "Themessages",
        "track",
        "results",
        "database",
        "GUI",
        "runtime",
        "platform",
        "service",
        "cooperation",
        "additional",
        "model",
        "buyers",
        "influence",
        "cost",
        "purchase",
        "probability",
        "initialization",
        "offers",
        "feedback",
        "experience",
        "White-Washing",
        "Self-Promoting",
        "limitation",
        "restriction",
        "others",
        "knowledge",
        "100",
        "many trust management systems",
        "obtain feedback history graph",
        "discrete event simulator",
        "generic evaluation metrics",
        "particular trust algorithm",
        "A Reputation Graph",
        "trust assessment process",
        "Existing trust models",
        "new systems",
        "decision-making process",
        "evaluation framework",
        "labelled graph",
        "agent A",
        "Summary Trust",
        "social trust",
        "trust properties",
        "nth-hand trust",
        "application domain",
        "rational decisions",
        "abstraction layer",
        "n feedbacks",
        "ith satisfaction",
        "different ranges",
        "labelled arcs",
        "other things",
        "next step",
        "transitive closure",
        "target agent",
        "agent B",
        "indirect feedback",
        "feedback histories",
        "following stages",
        "past behavior",
        "R N",
        "common set",
        "final stage",
        "abstract model",
        "feedback value",
        "output requirements",
        "R.",
        "tool",
        "manymodels",
        "attacks",
        "andmodel",
        "developers",
        "foundation",
        "expectation",
        "individual",
        "observations",
        "estimation",
        "combination",
        "rest",
        "graphs",
        "group",
        "actions",
        "list",
        "FHG",
        "downloader",
        "uploader",
        "values",
        "timestamps",
        "directed",
        "second",
        "E.",
        "∅",
        "single reputa- tion score",
        "absolute global reputation score",
        "new classification scheme",
        "continuous reputation values",
        "global reputation scores",
        "existing classification criteria",
        "stage transi- tions",
        "top k agents",
        "local reputation scores",
        "obtain trust graph",
        "Global algorithms",
        "existing literature",
        "one scores",
        "Reputation algorithms",
        "particular task",
        "two groups",
        "clar- ity",
        "Two methods",
        "trust models",
        "above sections",
        "Apple- seed",
        "other hand",
        "same codomain",
        "other words",
        "reputation graph",
        "various algorithms",
        "different algorithms",
        "edge labels",
        "local algorithms",
        "other method",
        "intermediate graph",
        "reflexive property",
        "incoming arcs",
        "among agents",
        "ranking algorithms",
        "One method",
        "trusting agent",
        "EigenTrust outputs",
        "one stage",
        "looping",
        "degree",
        "Figs",
        "weights",
        "Fig.",
        "Examples",
        "decision",
        "TG",
        "ourmodel",
        "stages",
        "workflow",
        "Classifying",
        "RG",
        "nisms",
        "threshold",
        "transitions",
        "addition",
        "Table",
        "semantics",
        "relative",
        "Trust Algorithm Transitions Input",
        "satisfaction local absoluteratings Ranking",
        "continuous feedback values",
        "trust models Stage",
        "graph transformation function",
        "Stage transitions",
        "global absoluteratings",
        "local absolutescores",
        "Trust algorithms",
        "functional view",
        "evaluation mechanisms",
        "global relativeratings",
        "N/A relatives",
        "Reputation Scores",
        "experimenter",
        "inputs",
        "outputs",
        "classification",
        "AppleSeed",
        "3 complaints",
        "3 certificates"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.49273,
      "content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 707841,
      "metadata_storage_name": "s40467-015-0033-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ2Ny0wMTUtMDAzMy05LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-16T18:35:45Z",
      "keyphrases": [
        "Creative Commons Attribution License",
        "interesting, potentially useful patterns",
        "Ant colony optimization algorithm",
        "modern intelligent optimization methods",
        "current data analysis tools",
        "important practical significance",
        "Open Access article",
        "many successful examples",
        "machine learning methods",
        "many different specifiedmethods",
        "Uncertain inference rule",
        "RESEARCH Open Access",
        "Bayesian belief network",
        "large data sets",
        "financial data analysis",
        "data compartment analysis",
        "many different fields",
        "data mining techniques",
        "data mining algorithms",
        "mated data generation",
        "data mining methods",
        "data mining technology",
        "collection tools",
        "many methods",
        "inference rules",
        "many other",
        "Uncertainty Analysis",
        "neutral network",
        "Bayesian classification",
        "Extraction methods",
        "uncertain system",
        "tomany fields",
        "mining objects",
        "tical methods",
        "effective methods",
        "Data classification",
        "existing data",
        "data description",
        "data dependency",
        "data regression",
        "data aggregate",
        "data prediction",
        "training data",
        "Rules extraction",
        "classification rules",
        "Yun Sun",
        "Yuanguo Zhu",
        "recent years",
        "classification problems",
        "computer networks",
        "scientific research",
        "ness decision",
        "computational process",
        "overall goal",
        "standable structure",
        "biomedical research",
        "main jobs",
        "decision trees",
        "mathematical formula",
        "original work",
        "Nanjing University",
        "business management",
        "valuable information",
        "two applications",
        "science research",
        "unrestricted use",
        "Ling Chen",
        "Journal",
        "DOI",
        "Correspondence",
        "chinalsy",
        "School",
        "Abstract",
        "increasing",
        "attention",
        "paper",
        "effectiveness",
        "superiority",
        "Keywords",
        "Introduction",
        "databases",
        "finance",
        "E-commerce",
        "logistics",
        "result",
        "amount",
        "people",
        "basis",
        "difficulty",
        "depth",
        "deficiency",
        "concepts",
        "laws",
        "modes",
        "knowledge",
        "reality",
        "instance",
        "telecommunication",
        "retail",
        "study",
        "couple",
        "models",
        "functions",
        "characteristics",
        "categories",
        "output",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "Page",
        "variety",
        "approaches",
        "start",
        "addition",
        "mutation ant colony optimization algorithm",
        "original ant colony optimization algorithm",
        "early decision trees algorithm",
        "simulated annealing algorithm",
        "global search algorithm",
        "powerful mathematical tool",
        "uncertain differential equation",
        "mature technology methods",
        "Decision trees method",
        "fuzzy rules learning",
        "many mathematical tools",
        "massive data set",
        "data mining technique",
        "neutral network algorithm",
        "probability distri- bution",
        "product measure axiom",
        "decision tree method",
        "incremental tree structure",
        "intelligent optimization algorithms",
        "different network models",
        "random initial solutions",
        "Machine learning methods",
        "popular classification methods",
        "complex nonlinear relations",
        "classification machine learning",
        "many evolution algorithms",
        "Genetic algorithm",
        "optimization strategies",
        "bionic optimization",
        "numerical methods",
        "conceptual learning",
        "inductive learning",
        "ID3 method",
        "C4.5 method",
        "different types",
        "many cases",
        "spatial data",
        "probability theory",
        "normality axiom",
        "duality axiom",
        "subadditivity axiom",
        "pattern classification",
        "statistical principle",
        "tering problems",
        "core content",
        "simulation model",
        "complex system",
        "pattern recognition",
        "biological evolution",
        "genetic mechanism",
        "important role",
        "Mixed algorithms",
        "other algorithms",
        "slow convergence",
        "local optimums",
        "Hybrid genetic",
        "hybrid particle",
        "real world",
        "human being",
        "indetermi- nacy",
        "domain experts",
        "belief degree",
        "tainty theory",
        "Many researchers",
        "theoretical work",
        "uniqueness theorem",
        "practical problems",
        "number",
        "variants",
        "SLIQ",
        "clustering",
        "regression",
        "behavior",
        "ant-miner",
        "Herrera",
        "weakness",
        "reason",
        "Zhu",
        "ACOA",
        "MACO",
        "performance",
        "understanding",
        "indeterminacy",
        "samples",
        "situation",
        "choice",
        "event",
        "unlikely",
        "frequency",
        "view",
        "Liu",
        "Chen",
        "Applications",
        "lot",
        "existence",
        "stability",
        "Peng",
        "Yao",
        "two typical classification problems",
        "ant colony optimization algorithm",
        "uncertain optimal control model",
        "heuristic optimization approach",
        "ant transfer prob",
        "option pricing models",
        "expected value operator",
        "two extraction methods",
        "M feasible solutions",
        "following optimization problem",
        "uncertain inference rule",
        "pheromone evaporation rate",
        "same pheromone level",
        "uncertain inference controller",
        "optimal solution s",
        "one ant",
        "artificial ant",
        "tain controller",
        "uncertain systems",
        "uncertain sets",
        "membership functions",
        "five parts",
        "universal approximator",
        "reasonable tool",
        "inverted pendulum",
        "uncertainty theory",
        "mutation operation",
        "data mining",
        "uncertainty sets",
        "decision variable",
        "domain D",
        "candidate solution",
        "best solution",
        "pheromone communication",
        "pheromone levels",
        "initial pheromone",
        "pheromone trails",
        "many paths",
        "bad paths",
        "good paths",
        "next section",
        "basic concepts",
        "last section",
        "objective function",
        "constraint function",
        "good ability",
        "data set",
        "food sources",
        "real ants",
        "cial ants",
        "one path",
        "walking path",
        "stocks",
        "inputs",
        "outputs",
        "rule-base",
        "Gao",
        "robustness",
        "results",
        "Preliminary",
        "Dorigo",
        "nest",
        "time",
        "parameters",
        "iterations",
        "procedures",
        "Step",
        "probability",
        "sk",
        "5",
        "⎪⎪⎪",
        "first measure inversion formula",
        "following three axiom",
        "product uncertain measureM",
        "Mamdani inference rules",
        "Takagi-Sugeno inference rules",
        "triangular uncertain set",
        "regular membership function μ",
        "inversion formulas",
        "inference systems",
        "fuzzy inference",
        "nonempty set",
        "optimal solution",
        "current iteration",
        "uncertainty space",
        "real numbers",
        "expected value",
        "two integrals",
        "key points",
        "fuzzy systems",
        "CRI approach",
        "human knowledge",
        "set function",
        "Amembership functionμ",
        "fuzzy sets",
        "Mk{�k",
        "M{ξ � r",
        "M{B ⊂",
        "B.",
        "⊂ B",
        "algebra",
        "L.",
        "triplet",
        "Lk",
        "Definition",
        "collection",
        "Borel",
        "Example",
        "power",
        "ξn",
        "Bi",
        "sup",
        "Bc",
        "equations",
        "Remark",
        "mode",
        "Theorem",
        "x0",
        "fact",
        "dx",
        "2b",
        "antecedents",
        "consequents",
        "process",
        "consequences",
        "σ",
        "∈",
        "∑",
        "∞",
        "⎪",
        "γ",
        "1",
        "∫",
        "independent uncertain sets",
        "conditional uncertain set",
        "m crisp inputs",
        "crisp data",
        "crisp values",
        "Uncertain system",
        "uncertain antecedents",
        "uncertain consequents",
        "two concepts",
        "constant a",
        "theory",
        "following",
        "X1",
        "Xm",
        "im",
        "ck",
        "coefficients",
        "Eq.",
        "ηi",
        "constants",
        "k∑",
        "min",
        "μil",
        "experts",
        "Y1",
        "Y2",
        "Yn",
        "Yj",
        "βj",
        "ξ",
        "ν",
        "η∗",
        "⎪⎪⎪⎪",
        "1 μ",
        "special triangular uncertain set",
        "m input data",
        "dent uncertain sets",
        "Q uncertain sets",
        "R uncertain sets",
        "consequence uncertain sets",
        "uncertain inference rules",
        "k inference rules",
        "one inference rule",
        "uncertain consequence",
        "m inputs",
        "three cases",
        "two points",
        "symmetry property",
        "Problem formulation",
        "extraction model",
        "decision vector",
        "rule base",
        "n rules",
        "m antecedents",
        "variable xi",
        "k.",
        "βn",
        "μim",
        "bj",
        "ij",
        "loss",
        "generality",
        "above",
        "1 output",
        "ing",
        "αi",
        "γi",
        "2βi",
        "Proof",
        "Equation",
        "computation",
        "dy",
        "section",
        "example",
        "corresponding objective function values",
        "following optimization model",
        "candidate decision variable",
        "Q + 1 choices",
        "best feasible solution",
        "mean absolute error",
        "best rule base",
        "new candidate sequence",
        "m values",
        "absolute errors",
        "feasible solutionX0",
        "Uncertain systems",
        "mutated solution",
        "current solution",
        "tain sets",
        "five descriptions",
        "system outputs",
        "origin outputs",
        "C classes",
        "ith subinterval",
        "extraction method",
        "The procedures",
        "fixed parameter",
        "evaporation rate",
        "classification problem",
        "next node",
        "sequence xi1xi2",
        "mutated element",
        "step k",
        "left nodes",
        "Once ants",
        "Q∑",
        "4 antecedents",
        "input",
        "MAE",
        "1 P",
        "P∑",
        "integers",
        "nonsense",
        "subintervals",
        "xij",
        "Extractionmethod",
        "withmutations",
        "1 value",
        "R.",
        "steps",
        "walk",
        "Denote",
        "τi",
        "xik",
        "way",
        "termination",
        "condition",
        "ρg",
        "⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪",
        "inner cycle termination criterion",
        "M candidate solutions",
        "good robust property",
        "uncertain rule base",
        "feasible solu- tion",
        "local search operation",
        "Metropolis acceptance criterion",
        "Step 2 Ant movement",
        "new decision vector",
        "Simulated annealing algorithm",
        "feasible solution X0",
        "portfolio optimization",
        "optimization problem",
        "decision variables",
        "initial solution",
        "neighbor solution",
        "initial value",
        "anneal- ing",
        "controlled cooling",
        "sequence xip",
        "allM ants",
        "previous section",
        "Repeat Step",
        "1 steps",
        "values",
        "Xl.",
        "classification",
        "Kirkpatrick",
        "name",
        "inspiration",
        "metallurgy",
        "technique",
        "heating",
        "material",
        "size",
        "crystals",
        "defects",
        "temperature",
        "point",
        "neighborhood",
        "order",
        "xiq",
        "procedure",
        "original feasible solution",
        "search range",
        "current temperature",
        "F(X̂",
        "Set",
        "step",
        "rithm",
        "tk",
        "τ"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.3974075,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-s",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 548509,
      "metadata_storage_name": "s13640-020-00545-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMjAtMDA1NDUtei5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "metadata_creation_date": "2021-01-12T23:22:39Z",
      "keyphrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "doi.org",
        "orcid.org",
        "commerce platforms",
        "Correspondence",
        "shaoshuaib",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "BERT",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "creativecommons",
        "licenses",
        "crossmark",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "BERT model",
        "Related work",
        "present stage",
        "good results",
        "extraction capacity",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social information Preprocessing",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "Background processing",
        "subsequent processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "characteristic dimension",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "Data label",
        "tf matrix",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "dividend",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "BERT-Base",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "Text classification BERT fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "Text classification fine-tuning",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "official recommended values",
        "feature matching scheme",
        "text information token",
        "full connection layer",
        "additional 9500 text data",
        "natural language processing",
        "machine learning scheme",
        "word segmentation process",
        "social information data",
        "5.2 Classification scheme",
        "text length",
        "text description",
        "input BERT",
        "first token",
        "TF-IDF model",
        "classification problem",
        "deep learning",
        "learning rate",
        "TF-IDF-based model",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "commodities",
        "social e-commerce market",
        "standard description text",
        "social e-commerce environment",
        "knowledge distillation technology",
        "standard product names",
        "large-scale data marking",
        "social e-commerce classification",
        "model recognition rate",
        "product information",
        "test data",
        "text-based classification",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "high correlation",
        "work",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "semi"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.8826506,
      "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\n\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\n\nn\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C)−H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN (AD − CB)2\n\n(A+ C)(B+ D)(A+ B)(C + D)\n\n(5)χ2(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf\n+ is the number of documents in the positive class that contain feature f, DFf\n\n− \nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and mf, mc\n\nf are \nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc −mf\n\n)2\n\n(8)mc =\n\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1− qf\n)\n\n(\n\n1− pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf\n+ and DFf\n\n− are the raw counts of documents that contain f in the positive and \nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs. 12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5 GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table 2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nTu\nrk\n\nis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\no",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1280780,
      "metadata_storage_name": "s13673-018-0135-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTgtMDEzNS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tuba Parlar ",
      "metadata_title": "QER: a new feature selection method for sentiment analysis",
      "metadata_creation_date": "2018-04-18T08:33:56Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "naïve Bayes multinomial classifier",
        "Selma Ayşe Özel2",
        "natural language processing task",
        "Text classification Open Access",
        "document frequency difference methods",
        "new feature selection method",
        "Creative Commons license",
        "other feature selection methods",
        "query expansion ranking",
        "query expansion term",
        "optimal orthogonal centroid",
        "four classi- fiers",
        "support vector machines",
        "third class label",
        "decision making process",
        "two-class classification problem",
        "original author(s",
        "Internet World Stats",
        "review docu- ments",
        "Hum. Cent. Comput",
        "statistical methods",
        "learning methods",
        "senti- ments",
        "decision trees",
        "classification accuracy",
        "sentiment classification",
        "author information",
        "Tuba Parlar1",
        "Fei Song3",
        "important piece",
        "social media",
        "top 20 countries",
        "highest numbers",
        "Internet users",
        "exploding growth",
        "main reasons",
        "low ranks",
        "multiple kinds",
        "product reviews",
        "Machine learning",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Parlar",
        "Inf. Sci.",
        "Kemal University",
        "Full list",
        "ferent approaches",
        "data preprocessing",
        "other people",
        "The Author",
        "classification performance",
        "opinionated information",
        "information retrieval",
        "Chi square",
        "information gain",
        "sentiment analysis",
        "ent languages",
        "noisy features",
        "doi.org",
        "orcid.org",
        "irrelevant features",
        "major source",
        "online documents",
        "many researchers",
        "different domains",
        "Turkish reviews",
        "English languages",
        "QER",
        "Introduction",
        "most",
        "opinions",
        "Turkey",
        "area",
        "SA",
        "general",
        "internetworldstats",
        "Abstract",
        "sentiments",
        "order",
        "non-informative",
        "study",
        "field",
        "ling",
        "movie",
        "performances",
        "classifiers",
        "results",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "tparlar",
        "mku",
        "1 Department",
        "Mathematics",
        "Mustafa",
        "Antakya",
        "Hatay",
        "end",
        "crossmark",
        "crossref",
        "Page",
        "19Parlar",
        "studies",
        "query expansion term weighting methods",
        "other common feature selection methods",
        "ten different review documents datasets",
        "maximum entropy mod- elling",
        "naïve Bayes multinomial",
        "traditional topic-based text classification",
        "good feature selection",
        "information retrieval systems",
        "Natural Language Processing",
        "machine learning methods",
        "text classification task",
        "document frequency difference",
        "machine learning approaches",
        "four text classifiers",
        "kitchen appliances reviews",
        "different document domains",
        "Related work SA",
        "feature space",
        "feature sizes",
        "training datasets",
        "different reviews",
        "classification accu",
        "particular review",
        "related attributes",
        "different languages",
        "LCD screen",
        "sentiment-expressing words",
        "experimental settings",
        "important topic",
        "Artificial Intelligence",
        "opinion mining",
        "useful applications",
        "negative ratings",
        "Hum. Cent",
        "popular implementation",
        "movie reviews",
        "book reviews",
        "search performance",
        "performance measures",
        "testing results",
        "Numerous studies",
        "SA problems",
        "overall sentiments",
        "non-informative features",
        "great pictures",
        "NB",
        "SVM",
        "DT",
        "MEM",
        "differences",
        "example",
        "camera",
        "people",
        "breath",
        "observation",
        "account",
        "paper",
        "dimensionality",
        "aim",
        "texts",
        "Turkish",
        "English",
        "effectiveness",
        "DFD",
        "OCFS",
        "goal",
        "racy",
        "respect",
        "rest",
        "Experiments",
        "Conclusion",
        "evalua",
        "emotions",
        "entities",
        "products",
        "services",
        "organizations",
        "als",
        "issues",
        "events",
        "kind",
        "many",
        "popularity",
        "user",
        "companies",
        "businesses",
        "Comput",
        "models",
        "cally",
        "Pang",
        "researchers",
        "effects",
        "Yang",
        "Yu",
        "C4.5",
        "Tabu heuristic search-enhanced Markov blanket model",
        "karşılaştırmalısın",
        "information gain feature selection method",
        "document frequency feature selection method",
        "minimum redundancy maximum relevancy",
        "Weighted Log Likelihood Ratio",
        "Weka [33] data mining tool",
        "DFD feature selection method",
        "reduced feature sizes",
        "Machine learning algorithms",
        "product review dataset",
        "Chinese online reviews",
        "movie review dataset",
        "The Turkish language",
        "feature sets",
        "Boynukalın",
        "n words",
        "lish reviews",
        "product datasets",
        "execution time",
        "large number",
        "comparative experiments",
        "significant improvement",
        "Altaic branch",
        "Ural-Altaic family",
        "agglutinative language",
        "longer sentence",
        "ture space",
        "emotional analysis",
        "initial version",
        "DT classifiers",
        "statistical analysis",
        "IG methods",
        "word relation",
        "sentiment features",
        "related features",
        "unigram features",
        "single word",
        "one classifier",
        "SVM classifiers",
        "two types",
        "high accuracy",
        "95% average accuracy",
        "English language",
        "Complementary NB",
        "MEM classifier",
        "English datasets",
        "SA studies",
        "Nicholls",
        "difference",
        "Agarwal",
        "mRMR",
        "NBM",
        "Abbasi",
        "EWGA",
        "Xia",
        "design",
        "Bai",
        "vocabulary",
        "Mladenovic",
        "mapping",
        "Zheng",
        "languages",
        "Republic",
        "Finnish",
        "garian",
        "instance",
        "something",
        "teristics",
        "WLLR",
        "combinations",
        "n-grams",
        "sequences",
        "Akba",
        "CHI2",
        "85.",
        "92",
        "optimal orthogonal cen- troid",
        "new query expansion ranking",
        "traditional text classification tasks",
        "several feature selection methods",
        "common feature selection methods",
        "high information gain scores",
        "follow- ing reasons",
        "decision tree classifier",
        "Turkish review datasets",
        "English review datasets",
        "good classification performance",
        "large vocabulary sizes",
        "many classification problems",
        "J48 classification methods",
        "total feature sizes",
        "Chi square score",
        "Such methods",
        "higher score",
        "low score",
        "feature f",
        "four classifiers",
        "tain measures",
        "same time",
        "effec- tiveness",
        "distinct values",
        "same way",
        "contingency table",
        "total number",
        "related class",
        "class c",
        "other class",
        "many documents",
        "valuable features",
        "previous study",
        "different classes",
        "P(aj",
        "m number",
        "probability distribution",
        "entropy values",
        "attribute value",
        "experiments",
        "SMO",
        "LR",
        "specific",
        "efficiency",
        "range",
        "500 increments",
        "improvement",
        "content",
        "power",
        "tainty",
        "Hum",
        "Cent",
        "definitions",
        "reviews",
        "positive",
        "egories",
        "keyword",
        "formulas",
        "dependence",
        "3000",
        "The Chi square statistics",
        "Query expansion ranking method",
        "probabilistic weighting model",
        "similar document frequencies",
        "high dimensional vectors",
        "Document frequency difference",
        "query expansion methods",
        "query expansion technique",
        "Optimal orthogonal centroid",
        "orthogonal centroid algorithm",
        "many sentiment-expressing features",
        "one specific class",
        "document vectors",
        "CHI2 method",
        "bigger difference",
        "document collection",
        "One problem",
        "One limitation",
        "new terms",
        "original query",
        "expanded query",
        "big impact",
        "H(C",
        "+ D",
        "big issue",
        "sentiment word",
        "optimized form",
        "Docu- ments",
        "age vector",
        "DFD method",
        "high scores",
        "centroid m",
        "OCFS method",
        "negative class",
        "rare features",
        "individual review",
        "xi N",
        "positive class",
        "highest scores",
        "Nc N",
        "relevant documents",
        "equal number",
        "classes",
        "dataset",
        "text",
        "|A",
        "aj",
        "CB",
        "classification",
        "Song",
        "DFf",
        "weights",
        "importance",
        "subset",
        "sum",
        "distances",
        "subspace",
        "mc",
        "mf",
        "values",
        "centroids",
        "IR",
        "Scoref",
        "Harman",
        "∑",
        "χ",
        "tional topic-based text classification",
        "QER feature selection method",
        "information retrieval strategy",
        "statistical language processing",
        "informa- tion gain",
        "feature selection methods",
        "efficient feature selector",
        "information retrieval system",
        "score computation method",
        "k new terms",
        "classification process",
        "score formula",
        "ranking score",
        "k terms",
        "high scored",
        "Sparck Jones",
        "raw counts",
        "negative classes",
        "small constants",
        "similar constants",
        "data smoothing",
        "fea- ture",
        "multiple classes",
        "two classes",
        "greedy approach",
        "related classes",
        "high values",
        "distance/similarity measures",
        "document frequencies",
        "Q. Equation",
        "term f",
        "negative documents",
        "related documents",
        "valuable terms",
        "top-k scores",
        "ability scores",
        "compute scores",
        "lowest scores",
        "zero probabilities",
        "query Q",
        "probability calculations",
        "sentiment-expressing features",
        "positive documents",
        "precision",
        "man",
        "set",
        "pf",
        "qf",
        "Robertson",
        "Eq.",
        "ratio",
        "Eqs",
        "log2",
        "DF",
        "N−",
        "numbers",
        "numerators",
        "denominators",
        "CHI",
        "collection",
        "vectors",
        "review",
        "introduction",
        "1−",
        "2.5 GHz Intel Core i7 processor",
        "The English movie review dataset",
        "The Turkish product review dataset",
        "The Turkish movie reviews",
        "English product review dataset",
        "English product reviews",
        "multi-fold cross validations",
        "fivefold cross validation",
        "Turkish product reviews",
        "same dataset sizes",
        "punctua- tion marks",
        "five F measures",
        "large feature space",
        "average F measures",
        "final classification results",
        "English reviews",
        "final P",
        "classification system",
        "five folds",
        "punctuation marks",
        "question marks",
        "addi- tion",
        "five portions",
        "reduc- tion",
        "978 negative reviews",
        "700 negative reviews",
        "1000 negative reviews",
        "feature spaces",
        "four subsets",
        "elec- tronics",
        "kitchen appliances",
        "existing work",
        "similar datasets",
        "composite score",
        "harmonic mean",
        "Experimental settings",
        "MacBook Pro",
        "16 GB 1600 MHz",
        "NLTK [38] library",
        "case normalization",
        "docu- ments",
        "two probabilities",
        "commerce website",
        "Performance evaluation",
        "classified items",
        "LR classifier",
        "document",
        "quencies",
        "beyazperde",
        "1057 positive",
        "hepsiburada",
        "books",
        "DVDs",
        "700 positive",
        "Lee",
        "1000 positive",
        "Blitzer",
        "electronics",
        "recall",
        "Python",
        "words",
        "exclamation",
        "polarity",
        "suffixes",
        "J48",
        "MEM.",
        "couple",
        "thousands",
        "training",
        "remaining",
        "testing",
        "Table",
        "memory",
        "features",
        "NBM SVM J48 LR Movie",
        "NBM SVM J48 LR Features",
        "five folds cross validation",
        "six fea- ture sizes",
        "smaller fea- ture sizes",
        "five feature selection methods",
        "best micro-average F measure",
        "bigger feature sizes",
        "post hoc tests",
        "desirable size n",
        "senti- ment analysis",
        "electronic review dataset",
        "topical text classification",
        "kitchen review datasets",
        "best classification results",
        "electronics review dataset",
        "Table 1 Baseline results",
        "NBM classifier",
        "best performer",
        "top-ranked features",
        "new method",
        "best results",
        "text classifiers",
        "possible improvements",
        "total features",
        "related formulas",
        "ing classifier",
        "same performance",
        "different documents",
        "kitchen reviews",
        "univariate ANOVA",
        "significant differences",
        "three groups",
        "95% confidence level",
        "two groups",
        "new results",
        "baseline systems",
        "low frequencies",
        "same intuition",
        "baselines",
        "Book",
        "scores",
        "settings",
        "120 experiments",
        "pairs",
        "bold-face",
        "Note",
        "expressing",
        "traditional",
        "Cai",
        "2×",
        "C FS C H I2",
        "text classifi- cation",
        "traditional topic-based text",
        "Q ER D",
        "review datasets",
        "QER method",
        "One reason",
        "decision-tree-based solution",
        "smaller set",
        "ss ifi",
        "ze F",
        "evant features",
        "result",
        "classifier",
        "rs",
        "rk",
        "FD",
        "IG"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.8515308,
      "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like precision i",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 514849,
      "metadata_storage_name": "s13173-013-0117-7.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzE3My0wMTMtMDExNy03LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2013-10-11T02:33:25Z",
      "keyphrases": [
        "São José dos Campos",
        "J Braz Comput Soc",
        "Ciências Matemáticas",
        "The Brazilian Computer Society",
        "São Paulo",
        "São Carlos",
        "Ciência e",
        "Paulo Henrique Pisani",
        "Ana Carolina Lorena",
        "typ- ing rhythm",
        "P. H. Pisani",
        "A. C. Lorena",
        "behavioral biometric technology",
        "effective security mechanism",
        "Behavioral intrusion detection",
        "formal review protocol",
        "intrusion detection systems",
        "user profil- ing",
        "secure authentication methods",
        "systematic review method",
        "Computação",
        "behavioral analysis",
        "communication systems",
        "security problems",
        "sophisticated methods",
        "The process",
        "Systematic reviews",
        "keystroke dynamics",
        "Abstract Computing",
        "data exposure",
        "ric technologies",
        "possible technologies",
        "future researches",
        "rigorous procedure",
        "ficial intelligence",
        "performance measures",
        "mark datasets",
        "digital identities",
        "greater worries",
        "information exposure",
        "several activities",
        "online banking",
        "new momentum",
        "personal information",
        "system initialization",
        "several aspects",
        "regular behavior",
        "great variety",
        "rigorous method",
        "initial authentication",
        "possible way",
        "Instituto de",
        "Universidade Federal",
        "wider dissemination",
        "identity theft",
        "imate user",
        "potential intrusions",
        "SURVEY PAPER",
        "user authentication",
        "DOI",
        "life",
        "issue",
        "work",
        "users",
        "order",
        "area",
        "definition",
        "results",
        "state",
        "art",
        "sifiers",
        "features",
        "Keywords",
        "Biometrics",
        "ICMC",
        "USP",
        "mail",
        "phpisani",
        "Tecnologia",
        "ICT",
        "UNIFESP",
        "aclorena",
        "1 Introduction",
        "increased",
        "net",
        "commerce",
        "term",
        "crime",
        "someone",
        "scenario",
        "case",
        "example",
        "intruders",
        "session",
        "ways",
        "concept",
        "fact",
        "models",
        "profile",
        "identification",
        "deviations",
        "18",
        "24",
        "10",
        "S N User profile Store profile",
        "regular typing rhythm",
        "possible devia- tions",
        "keystroke dynam- ics",
        "system audit logs",
        "command line use",
        "user behavior pattern",
        "systematic bibliographic reviews",
        "Behavioural intrusion detection",
        "two major steps",
        "intrusion detection system",
        "possible incidents",
        "behaviour pattern",
        "anomaly detection",
        "user profiling",
        "same user",
        "systematic procedure",
        "behavioural IDS",
        "other researchers",
        "other areas",
        "software engineering",
        "artificial intelligence",
        "remaining sections",
        "basic concepts",
        "security policies",
        "acceptable use",
        "security practices",
        "mod- els",
        "basic flow",
        "key issue",
        "similar values",
        "keyboard input",
        "user behaviour",
        "User Training",
        "review method",
        "normal behaviour",
        "valuable information",
        "information security",
        "Training phase",
        "explicit protocols",
        "new concept",
        "Application usage",
        "same time",
        "Preliminary results",
        "An IDS",
        "name",
        "formal",
        "conduction",
        "way",
        "validation",
        "incidence",
        "bias",
        "problem",
        "medicine",
        "number",
        "benefits",
        "computing",
        "paper",
        "tributes",
        "aim",
        "Sect.",
        "process",
        "conclusions",
        "2 Background",
        "events",
        "computer",
        "signals",
        "violations",
        "threats",
        "analysis",
        "type",
        "alarms",
        "Recognition",
        "Fig.",
        "Figure",
        "matching",
        "aspects",
        "questions",
        "group",
        "monitoring",
        "systems audit logs",
        "lower error rates",
        "support vector machines",
        "Two distinctive processes",
        "machine learning algorithms",
        "Systematic literature review",
        "two ways",
        "several algorithms",
        "other aspects",
        "continuous authentication",
        "mail behav",
        "less effort",
        "first process",
        "second process",
        "neural networks",
        "less susceptibility",
        "Software Engineering",
        "major phases",
        "first phase",
        "second phase",
        "secondary study",
        "third phase",
        "final report",
        "check- ing",
        "specific knowledge",
        "important part",
        "primary studies",
        "eval- uation",
        "essential part",
        "other researches",
        "3 Systematic review",
        "Protocol evaluation",
        "systematic reviews",
        "dynamic text",
        "Static text",
        "bibliographic reviews",
        "review protocol",
        "feature classification",
        "classifica- tion",
        "pro- tocol",
        "three phases",
        "cific topic",
        "following steps",
        "research questions",
        "feature extraction",
        "formal way",
        "review need",
        "typing features",
        "specific user",
        "search strategies",
        "rhythms",
        "model",
        "technology",
        "iour",
        "expressions",
        "password",
        "recognition",
        "terms",
        "instance",
        "addition",
        "completion",
        "Medicine",
        "Computing",
        "application",
        "planning",
        "presentation",
        "information",
        "references",
        "items",
        "Identification",
        "goal",
        "existence",
        "investigation",
        "requirements",
        "Commissioning",
        "cases",
        "lack",
        "time",
        "Specification",
        "Development",
        "selection",
        "576 J Braz Comput Soc",
        "greatest possible number",
        "newer review article",
        "systematic review process",
        "traditional reviews",
        "inclusion/exclusion criteria",
        "Data synthesis",
        "quantitative aspects",
        "academic journals",
        "web sites",
        "two researchers",
        "next sections",
        "intrusion detection",
        "Classification algorithms",
        "Performance measures",
        "Benchmarking datasets",
        "comparative experiments",
        "respective sub-questions",
        "typing data",
        "Reference search",
        "search expressions",
        "planning phase",
        "Quality evaluation",
        "research question",
        "diverse aims",
        "Information extraction",
        "dissemination mechanisms",
        "explicit definition",
        "More details",
        "Extracted features",
        "reference databases",
        "conduction phase",
        "previous work",
        "Report evaluation",
        "summary results",
        "rigour",
        "Selection",
        "use",
        "importance",
        "study",
        "support",
        "forms",
        "step",
        "qualitative",
        "meta-analysis",
        "mulation",
        "publishing",
        "conferences",
        "experts",
        "publication",
        "principles",
        "phases",
        "topic",
        "Advantages",
        "Internet",
        "English",
        "authors",
        "part",
        "need",
        "3.2",
        "1.1",
        "logical connective O R.",
        "logic connective AN D",
        "personal- ization systems",
        "authentica- tion systems",
        "behaviour intrusion detection",
        "behavior intrusion detection",
        "ACM Digital Library",
        "biometric intrusion detection",
        "behavioural intrusion detection",
        "behavioral intrusion detection",
        "basic search expression",
        "following exclusion criteria",
        "keystroke biometric(s",
        "recommender systems",
        "1.3 Selection criteria",
        "keypress biometric",
        "behavioral IDS",
        "behavioural biometrics",
        "keystroke pattern",
        "keystroke authentication",
        "behavioral biometrics",
        "many users",
        "typing pattern",
        "uous authentication",
        "same topic",
        "personalized information",
        "ized content",
        "content delivery",
        "recommendation system",
        "ommendations system",
        "information retrieval",
        "N OT",
        "several data-bases",
        "computing area",
        "data- base",
        "Appendix A",
        "next steps",
        "one page",
        "keystroke biometrics",
        "typing dynamics",
        "web search",
        "keystroke analysis",
        "unrelated topics",
        "IEEE Xplore",
        "Science Direct",
        "last part",
        "systematic review",
        "various terms",
        "exclusion terms",
        "performance comparisons",
        "1.2 References search",
        "OR ‘‘personalization",
        "rithms",
        "measures",
        "datasets",
        "classifier",
        "tests",
        "list",
        "papers",
        "completeness",
        "database",
        "conjunction",
        "reason",
        "differences",
        "syntax",
        "org",
        "ieeexplore",
        "sciencedirect",
        "Scopus",
        "inclusion",
        "Publications",
        "requirement",
        "posters",
        "presentations",
        "cts",
        "editorials",
        "texts",
        "magazines",
        "newspaper",
        "578 J Braz Comput Soc",
        "enough informa- tion",
        "use- ful features",
        "high- light references",
        "complete version",
        "unnecessary work",
        "different versions",
        "restricted access",
        "The value",
        "possible scores",
        "research criteria",
        "feature vector",
        "previous researches",
        "researc- hes",
        "exclusion criteria",
        "static text",
        "first step",
        "data- bases",
        "several papers",
        "quality criteria",
        "Basic information",
        "Best performance",
        "duplicate publications",
        "same study",
        "higher values",
        "algorithm parameters",
        "experimental tests",
        "design step",
        "analysis step",
        "conclusion step",
        "test dataset",
        "first publications",
        "conduction step",
        "quality score",
        "nine items",
        "several items",
        "performance tests",
        "criterion",
        "services",
        "accessible",
        "sum",
        "goals",
        "beginning",
        "advantages",
        "approaches",
        "limitations",
        "problems",
        "acm",
        "isiknowledge",
        "scopus",
        "Part",
        "title",
        "year",
        "Type",
        "device",
        "PC",
        "measure",
        "Number",
        "Algorithms",
        "verification",
        "Observations",
        "line",
        "Table",
        "18/February",
        "tool",
        "mendeley",
        "series",
        "duplicates",
        "organization",
        "category",
        "4",
        "2.1",
        "references Database Number",
        "step Step Number",
        "PDF files",
        "fast reading",
        "secondary studies",
        "200 refer- ences",
        "quality assessment",
        "several texts",
        "direct way",
        "pub- lications",
        "descending order",
        "highest scores",
        "higher scores",
        "greater efforts",
        "average score",
        "high number",
        "exclusion criterion",
        "search databases",
        "plete text",
        "full text",
        "duplicate references",
        "remaining references",
        "eligible references",
        "information extraction",
        "Table 2 Number",
        "Table 1",
        "associations",
        "entries",
        "Web",
        "Gaines",
        "centralization",
        "intersection",
        "removal",
        "university",
        "Conse",
        "reviews",
        "surveys",
        "tasks",
        "spreadsheet",
        "Total",
        "elimination",
        "detail",
        "tables",
        "algorithms",
        "performance",
        "abstract",
        "observation",
        "one",
        "ally",
        "purpose",
        "equals",
        "focus",
        "rel",
        "graph",
        "different",
        "4.2.2",
        "2.3",
        "580 J Braz Comput Soc",
        "approximate normal distribution",
        "tional research efforts",
        "highest quality score",
        "publi- cations",
        "scientific community",
        "research sub-questions",
        "performance evaluation",
        "benchmarking datasets",
        "access card",
        "military purposes",
        "simple login",
        "unauthorized access",
        "card ownership",
        "recent years",
        "greatest growth",
        "same institution",
        "biometric technologies",
        "behavioral features",
        "biometric technology",
        "specific devices",
        "biometric data",
        "growth trend",
        "voice recognition",
        "primary method",
        "One way",
        "additional cost",
        "authentication factors",
        "several users",
        "unauthorized user",
        "security context",
        "Publication Year",
        "widespread use",
        "user identity",
        "zero",
        "scores",
        "maximum",
        "aspect",
        "field",
        "atten",
        "Both",
        "5 Results",
        "section",
        "disadvan",
        "credentials",
        "token",
        "biometrics",
        "gerprint",
        "iris",
        "combination",
        "secrecy",
        "weaknesses",
        "authorization",
        "words",
        "birthday",
        "scenarios",
        "expansion",
        "gally",
        "rate",
        "USA",
        "losses",
        "world",
        "insiders",
        "victim",
        "attack",
        "science",
        "methods",
        "determination",
        "iological",
        "hardware",
        "contrast",
        "other",
        "capture",
        "level",
        "transparency",
        "false alarm rates",
        "nature recognition biometrics",
        "specific operations",
        "stroke dynamics",
        "Recognition precision",
        "same environment",
        "nition performance",
        "user identification",
        "legiti- mate",
        "legitimate user",
        "trol systems",
        "authentication",
        "factor",
        "acceptance",
        "presence",
        "keyboards",
        "teristics",
        "pen",
        "intruder",
        "standards",
        "access",
        "European"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.457851,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1344318,
      "metadata_storage_name": "s40537-019-0210-7.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDIxMC03LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Taiwo Kolajo ",
      "metadata_title": "Big data stream analysis: a systematic literature review",
      "metadata_creation_date": "2019-06-04T14:40:29Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "effective resource allocation strategy",
        "existing data mining tools",
        "big data stream tools",
        "data stream computing mode",
        "big data streaming tools",
        "big data streaming ana",
        "Big data stream analysis",
        "big data stream computing",
        "big data streams analysis",
        "big data batch computing",
        "Creative Commons license",
        "big data computing",
        "big data tool",
        "J Big Data",
        "several computational challenges",
        "process- ing requirements",
        "inherent dynamic characteristics",
        "Three major databases",
        "first search string",
        "standard benchmark dataset",
        "creat iveco mmons",
        "load balancing issues",
        "nificant research efforts",
        "technologies Open Access",
        "SURVEY PAPER Kolajo",
        "empirical analysis",
        "real-time analysis",
        "future computing",
        "data sources",
        "parallelization issues",
        "research questions",
        "literature review",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "two types",
        "huge amount",
        "methodical approach",
        "global view",
        "exclusion criteria",
        "preprocessing stage",
        "iterative jobs",
        "scalable frameworks",
        "growing size",
        "unrestricted use",
        "appropriate credit",
        "original author",
        "Information Sciences",
        "Covenant University",
        "Full list",
        "author information",
        "doi.org",
        "orcid.org",
        "information technology",
        "large volume",
        "great velocity",
        "systematic review",
        "initial 2295 papers",
        "Taiwo Kolajo",
        "47 papers",
        "Introduction",
        "Advances",
        "high-velocity",
        "ability",
        "nature",
        "terms",
        "variety",
        "veracity",
        "volatility",
        "value",
        "new",
        "trend",
        "Abstract",
        "fact",
        "number",
        "applications",
        "methods",
        "techniques",
        "rigorous",
        "comparisons",
        "Scopus",
        "ScienceDirect",
        "EBSCO",
        "journals",
        "conferences",
        "entities",
        "IEEE",
        "ACM",
        "SpringerLink",
        "Elsevier",
        "inclusion",
        "study",
        "privacy",
        "attention",
        "key",
        "features",
        "lytics",
        "conclusion",
        "algorithms",
        "complexity",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "changes",
        "Correspondence",
        "fulokoja",
        "1 Department",
        "Computer",
        "Ota",
        "Nigeria",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "Page",
        "30Kolajo",
        "scalable com- puting platforms",
        "big data streaming analytics",
        "big data stream analysis",
        "Big data batch processing",
        "Streaming processing frameworks",
        "stream processing paradigms",
        "Stream processing solutions",
        "data flow graph",
        "real-time application scenarios",
        "real-time, high volume",
        "time data analysis",
        "real-time data stream",
        "batch computing",
        "stream computing",
        "stream processor",
        "high-velocity flow",
        "incoming data",
        "data generating",
        "market data",
        "real-time sources",
        "changing conditions",
        "common understanding",
        "ben- efit",
        "scientific community",
        "key issues",
        "detailed evaluation",
        "massive amount",
        "high- velocity",
        "multiple sources",
        "low latency",
        "new sources",
        "location services",
        "mobile devices",
        "sensor pervasiveness",
        "fundamental assumption",
        "potential value",
        "parallel architectures",
        "decision-making process",
        "static questions",
        "continuous queries",
        "diverse sources",
        "sideration availability",
        "fault tolerance",
        "infinite tuple",
        "actionable results",
        "interconnected streams",
        "related work",
        "research works",
        "Research method",
        "crucial need",
        "real contrasts",
        "ming paradigm",
        "Discussion” section",
        "Result” section",
        "addition",
        "output",
        "low-latency",
        "seconds",
        "demand",
        "reason",
        "huge",
        "organisations",
        "businesses",
        "paper",
        "purpose",
        "overview",
        "findings",
        "implications",
        "practice",
        "update",
        "state",
        "areas",
        "challenges",
        "rest",
        "Background",
        "information",
        "Limitation",
        "Conclusion",
        "ubiquity",
        "Internet",
        "Things",
        "Sensors",
        "freshness",
        "Storm",
        "Kafka",
        "Spark",
        "Table",
        "motion",
        "essence",
        "fly",
        "scalability",
        "assimilation",
        "production",
        "operations",
        "Fig.",
        "Dimension Batch processing Streaming processing Input Data chunks",
        "Hardware Multiple CPUs Typical single limited amount",
        "big data stream computing environments",
        "big data streaming analysis",
        "domain Web mining",
        "streaming analytics system",
        "big data streams",
        "Data flow graph",
        "high data rates",
        "data processing node",
        "memory Storage Store",
        "new incoming data",
        "ble efficient operations",
        "typical example",
        "multiple rounds",
        "Such processing",
        "single pass",
        "new data",
        "missing data",
        "data normalization",
        "distributed system",
        "High fault-tolerance",
        "life-critical systems",
        "data tuples",
        "Data size",
        "new information",
        "new tuple",
        "new trends",
        "model predictions",
        "feature extraction",
        "external feeds",
        "non-trivial portion",
        "traffic monitoring",
        "sensor networks",
        "Key issues",
        "useful knowledge",
        "current happenings",
        "speedy manner",
        "organisa- tions",
        "load balancing",
        "privacy issues",
        "exponential growth",
        "computer resources",
        "research efforts",
        "effective resource",
        "cation strategy",
        "com- plexity",
        "small number",
        "different datasets",
        "component failure",
        "time-sensitive processes",
        "security threats",
        "data Time",
        "latest tuples",
        "analytic applications",
        "sliding window",
        "milliseconds Applications",
        "logical container",
        "main challenges",
        "Fault‑tolerance",
        "integration technique",
        "results",
        "operators",
        "models",
        "access",
        "idea",
        "duplicates",
        "parsing",
        "windows",
        "Table 1",
        "Comparison",
        "updates",
        "advance",
        "passes",
        "figure",
        "need",
        "order",
        "problems",
        "performance",
        "timeliness",
        "consistency",
        "heterogeneity",
        "incompleteness",
        "accuracy",
        "The",
        "way",
        "processors",
        "Moore",
        "law",
        "fore",
        "views",
        "interruption",
        "big data stream computing system",
        "organisational resource management specifi",
        "big data stream dataset",
        "Big data stream analytics",
        "good multiple instances replication",
        "big data analytics literature",
        "big data analytics methods",
        "good system structure",
        "distributing envi- ronment",
        "effective tech- niques",
        "Big data streams",
        "competent data presentation",
        "national Data Cooperation",
        "big data analysis",
        "data streams changes",
        "partial data streams",
        "previous research efforts",
        "big data streaming",
        "big data challenges",
        "systematic literature review",
        "High throughput Decision",
        "big threat",
        "streaming analytics",
        "streaming” analytics",
        "streaming data",
        "natural disaster",
        "continuous processing",
        "local views",
        "accessi- bility",
        "meaningful content",
        "Load balancing",
        "load shedding",
        "peak loads",
        "average load",
        "global centre",
        "entire information",
        "needs protection",
        "main objectives",
        "future observations",
        "processing algorithms",
        "stream-specific requirements",
        "various tools",
        "research focus",
        "business values",
        "high consistency",
        "high accuracy",
        "main challenge",
        "communicating nodes",
        "individual privacy",
        "ent characteristics",
        "Related work",
        "The study",
        "reviews",
        "fraud",
        "tures",
        "platforms",
        "architecture",
        "minimal",
        "latency",
        "stability",
        "Heterogeneity",
        "semantics",
        "granularity",
        "correlate",
        "real-time",
        "diversity",
        "hierarchy",
        "resources",
        "variance",
        "result",
        "respect",
        "sub-graph",
        "replicas",
        "portion",
        "issue",
        "opportunities",
        "IDC",
        "half",
        "volume",
        "velocity",
        "variability",
        "consideration",
        "section",
        "technologies",
        "Authors",
        "definitions",
        "types",
        "technology",
        "cally",
        "four big data streaming tools",
        "big data stream analysis tools",
        "big data stream literature review",
        "authoritative, full-text scientific, technical",
        "data stream analysis reviews",
        "big data stream framework",
        "big data stream processing",
        "big data stream algorithms",
        "big data stream analytics",
        "three standard database indexes",
        "big data stream technologies",
        "big data analytics",
        "machine learning algorithms",
        "full-text data- bases",
        "large data- base",
        "smart intuitive functionality",
        "electronic journal service",
        "big data technologies",
        "academic journal articles",
        "leading information solution",
        "systematic mapping method",
        "following research questions",
        "good search string",
        "Data sources",
        "four categories",
        "peer-reviewed literature",
        "citation database",
        "standard databases",
        "information professionals",
        "bibliographic database",
        "empirical research",
        "same vein",
        "particular focus",
        "anomaly detection",
        "tech- niques",
        "clear explanation",
        "Relevant publications",
        "Science Direct",
        "hensive overview",
        "research output",
        "social sciences",
        "largest abstract",
        "open access",
        "health publications",
        "14 million publications",
        "Life Sciences",
        "Physical Sciences",
        "Health Sciences",
        "wide range",
        "academic researchers",
        "application areas",
        "healthcare profes",
        "evaluation techniques",
        "peer-reviewed journals",
        "3800 journals",
        "status",
        "survey",
        "scope",
        "comprehensive",
        "differences",
        "concept",
        "capabilities",
        "limitations",
        "strengths",
        "benchmarks",
        "population",
        "son",
        "intervention",
        "outcome",
        "keywords",
        "searches",
        "EBSCOhost",
        "rich",
        "abstracts",
        "citations",
        "36,377 titles",
        "11,678 publishers",
        "world",
        "arts",
        "humanities",
        "students",
        "teachers",
        "35,000 books",
        "Engineering",
        "porate",
        "Third Search string refinement result",
        "free online professional network",
        "First search string result",
        "Second search string result",
        "Scopus ScienceDirect EBSCOhost Total",
        "major academic publishers",
        "nine (9) search strings",
        "high impact journals",
        "111 seemingly relevant papers",
        "Inclusion criteria Papers",
        "Table 5 Final Selection",
        "Further refinement",
        "Data retrieval",
        "easy analysis",
        "high-quality e-books",
        "100 million publications",
        "secondary source",
        "rich databases",
        "Boolean ‘OR",
        "2295 arti- cles",
        "three databases",
        "computer science",
        "subject domain",
        "quick overview",
        "Microsoft Excel",
        "three categories",
        "black colour",
        "similar investigations",
        "following categories",
        "primary study",
        "source language",
        "relevant” papers",
        "11 million researchers",
        "year range",
        "total number",
        "peer-reviewed conferences",
        "recent papers",
        "16,711 journals",
        "Table 2",
        "Table 3",
        "Table 4",
        "1989 papers",
        "315 papers",
        "111 papers",
        "45 papers",
        "18 papers",
        "magazine",
        "titles",
        "60,000 audiobooks",
        "iv.",
        "ResearchGate4",
        "scientists",
        "collaborators",
        "authors",
        "subscription",
        "set",
        "reseaarchgate",
        "sources",
        "interest",
        "stage",
        "PDF",
        "introduction",
        "green",
        "red",
        "colours",
        "end",
        "workshops",
        "technical",
        "symposium",
        "case",
        "part",
        "English",
        "contributions",
        "900,000",
        "1500",
        "alternative big data streaming solutions",
        "effective data management decisions",
        "spike load profile platform",
        "Big data stream platforms",
        "Streaming data sources",
        "many NoSQL databases",
        "specific application interfaces",
        "data loading procedure",
        "enterprise technology vendors",
        "open source community",
        "open source solutions",
        "Workload profile",
        "proprietary solutions",
        "stream applications",
        "high-velocity data",
        "data stores",
        "recent technology",
        "Such platforms",
        "platform distribution",
        "Data access",
        "Several tools",
        "sary tools",
        "single flow",
        "growing demand",
        "pro- jection",
        "different structures",
        "different ways",
        "CAP theorem",
        "consistent loads",
        "consistent flows",
        "web-based services",
        "soft- ware",
        "critical functions",
        "Latency requirement",
        "minimal delay",
        "key-value stores",
        "memory solution",
        "licensing issues",
        "limited maturity",
        "developer communities",
        "modification challenges",
        "large datasets",
        "large scale",
        "network partition",
        "service deployment",
        "service cloud",
        "premise approach",
        "easy integration",
        "consistency requirement",
        "careful selection",
        "serialization technologies",
        "execution",
        "functionalities",
        "response",
        "factors",
        "Shape",
        "capturing",
        "storing",
        "rep",
        "instance",
        "room",
        "flexibility",
        "storage",
        "users",
        "Availability",
        "presence",
        "break",
        "scenario",
        "requests",
        "Infrastructure",
        "option",
        "processing",
        "predictable",
        "workloads",
        "spikes",
        "combination",
        "go",
        "Tables",
        "pricing",
        "innovation",
        "development",
        "lack",
        "support",
        "outdating",
        "problem",
        "big data stream analysis Tools",
        "Multinomial latent dirichlet allocation",
        "Elastic streaming processing engine",
        "real-time social media data",
        "Microsoft azure stream analytics",
        "old-based stream clustering approaches",
        "Incremental algorithm threshold setting",
        "Table 6 Open source tools",
        "social media analysis",
        "social media streams",
        "high dimensional data",
        "Markov Random Field",
        "Sentiment brand monitoring",
        "IBM InfoSphere streams",
        "Density-based clustering algorithm",
        "Voltage clustering algorithm",
        "maximum similarity threshold",
        "little research efforts",
        "incremental clustering approaches",
        "Online Spherical K-means",
        "Incremental approaches",
        "Splunk stream",
        "incoming stream",
        "data grouping",
        "Proprietary tools",
        "WSO2 analytics",
        "Microsoft StreamInsight",
        "clustering algorithms",
        "Spark streaming",
        "Complete Clustering",
        "hierarchical clustering",
        "online clustering",
        "Research Question",
        "true costs",
        "Apache storm",
        "Apache Samza",
        "Apache Aurora",
        "Apache Kylin",
        "dynamic nature",
        "desirable number",
        "prior knowledge",
        "scalable graph",
        "limited space",
        "apriori number",
        "Google MillWheel",
        "TIBCO StreamBase",
        "Kyvos insights",
        "Lambda architecture",
        "Much work",
        "fragmentation issues",
        "tive approach",
        "static values",
        "metric learning",
        "partitioning algorithms",
        "technology Article",
        "balanced partitioning",
        "time window",
        "Condensed Clusters",
        "existing clusters",
        "Threshold-based techniques",
        "Table 8 Methods",
        "Table 7",
        "understanding",
        "benefits",
        "BlockMon",
        "NoSQL",
        "Photon",
        "MavEStream",
        "EsperTech",
        "Redis",
        "C-SPARQL",
        "SAMOA",
        "CQELS",
        "ETALIS",
        "XSEQ",
        "k-median",
        "k-medoid",
        "expectation-maximization",
        "tendency",
        "decisions",
        "DenStream",
        "OpticStream",
        "Exclusive",
        "outliers",
        "HDDStream",
        "PreDeCon-Stream",
        "PKS-Stream",
        "memory",
        "face",
        "CodeBlue",
        "Anodot",
        "Cloudet",
        "Numenta",
        "Artemis",
        "Striim",
        "AtScale",
        "efficiency",
        "SPADE",
        "LSML",
        "KTS",
        "Dynamic prime-number based security verification",
        "User profile vector update algorithm",
        "Incremental MI outlier detection algorithm",
        "Singular spectrum matrix completion",
        "Temporal fuzzy concept analysis",
        "Tag assignment stream clustering",
        "QRS detection algorithm",
        "cloud computing algorithm",
        "Parallel K-means clustering",
        "Locality sensitive hashing",
        "Forward chaining rule",
        "Continuous query processing",
        "Multi-query optimization strategy",
        "Outlier method",
        "Density cognition",
        "Inc I-MLOF",
        "Adaptive windowing",
        "online ensemble",
        "Nearest neighbour",
        "Markov chains",
        "LSH",
        "TASC",
        "StreamMap",
        "CluStream",
        "HPClustering",
        "D-Stream",
        "DCStream",
        "P-Stream",
        "ADStream",
        "CQR",
        "FPSPAN-growth",
        "OMCA",
        "MQOS",
        "automata",
        "VPA",
        "AWOE",
        "K-anonymity",
        "closeness",
        "ECM-sketch",
        "Block-QuickSort-AdjacentJobMatch",
        "Block-QuickSort-OverlapReplicate"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.1670632,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1434817,
      "metadata_storage_name": "s40708-020-00109-x.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDcwOC0wMjAtMDAxMDkteC5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Ferdousi Sabera Rawnaque ",
      "metadata_title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "metadata_creation_date": "2020-09-18T02:02:41Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Khondaker Abdullah Al Mamun",
        "high time resolution advantages",
        "Physiological response measuring techniques",
        "consumer emotion recognition-based experiments",
        "other third party material",
        "neural record- ing techniques",
        "video advertisement-based Neuromarketing experiments",
        "other machine learning algorithms",
        "Creative Commons licence",
        "Support Vector Machine",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "empirical research findings",
        "magnetic resonance imaging",
        "heart rate monitoring",
        "traditional filtering methods",
        "independent component analysis",
        "Linear Discriminant Analysis",
        "highest average accuracy",
        "market- ing research",
        "consumer response prediction",
        "Artificial Neural Network",
        "prefrontal alpha band",
        "skin conductance recording",
        "Brain computer interface",
        "brain–computer interface",
        "prevalent marketing stimuli",
        "first systematic review",
        "Neural recording",
        "unspoken response",
        "Neuromarket- ing",
        "consumer goods",
        "neural signal",
        "consumer preferences",
        "Brain Inf.",
        "brain recordings",
        "doi.org",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "commercial area",
        "last 5 years",
        "valid databases",
        "promotion forms",
        "asymmetry theory",
        "many researchers",
        "low cost",
        "eye tracking",
        "facial mapping",
        "artifact removal",
        "future researchers",
        "vital information",
        "novel contributions",
        "The Author",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "BCI) technology",
        "interdisciplinary bridge",
        "ultimate sale",
        "targeted audiences",
        "expanding economy",
        "effective marketing",
        "57 relevant literatures",
        "intended use",
        "permitted use",
        "good product",
        "new businesses",
        "Neuromarketing field",
        "Technological advancements",
        "opportunities",
        "Abstract",
        "academic",
        "interest",
        "interpreting",
        "tool",
        "consumers",
        "article",
        "purpose",
        "authors",
        "total",
        "basic",
        "trend",
        "nals",
        "electroencephalogram",
        "EEG",
        "functional",
        "fMRI",
        "parallel",
        "classification",
        "ANN",
        "SVM",
        "LDA",
        "Keywords",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "1 Introduction",
        "application",
        "invasive",
        "neuroscience",
        "perception",
        "changing",
        "non-invasive brain signal record- ing techniques",
        "1 Advanced Intelligent Multidisciplinary Systems Lab",
        "transcranial mag- netic stimulator",
        "Functional mag- netic resonance",
        "Open Access Brain Informatics",
        "market- ing stimuli",
        "image processing techniques",
        "nal recording techniques",
        "functional near-infrared spectroscopy",
        "com- petitive market",
        "United International University",
        "positron emission tomography",
        "machine learning algorithms",
        "skin conductance measurements",
        "neuronal activ- ity",
        "Different stimuli trigger",
        "marketing research techniques",
        "ventional market research",
        "traditional survey methods",
        "neural recording devices",
        "various marketing stimuli",
        "consumers’ purchase decisions",
        "Neuromarketing World Forum",
        "Advanced Research",
        "neuronal signals",
        "human brain",
        "traditional marketing",
        "Neuro- marketing",
        "academic research",
        "qualitative assessment",
        "posteriori analysis",
        "Full list",
        "author information",
        "group discussion",
        "personal interviews",
        "consumer feedback",
        "time requirement",
        "high cost",
        "unreliable information",
        "inaccurate results",
        "possible meanings",
        "new door",
        "buying behavior",
        "tal states",
        "physiological signals",
        "heart rate",
        "physiological responses",
        "spectral analysis",
        "accurate depiction",
        "Early years",
        "contro- versy",
        "high promises",
        "consumer mind",
        "buy buttons",
        "consumer behavior",
        "Business Association",
        "annual event",
        "Neuromarketing research",
        "field trials",
        "unspoken cognitive",
        "emotional responses",
        "Neuromarketing Science",
        "consumer response",
        "associated response",
        "quantitative",
        "products",
        "surveys",
        "Correspondence",
        "frawnaque",
        "umassd",
        "Institute",
        "Dhaka",
        "Bangladesh",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "19Rawnaque",
        "observations",
        "approaches",
        "limitations",
        "contrast",
        "customer",
        "electroencephalography",
        "magnetoencephalography",
        "MEG",
        "TMS",
        "fNIRS",
        "examples",
        "withdrawal",
        "change",
        "brainwaves",
        "researchers",
        "help",
        "excitement",
        "engagement",
        "stress",
        "insight",
        "audience",
        "preferences",
        "likes",
        "academician",
        "marketers",
        "due",
        "lack",
        "groundwork",
        "claim",
        "scrutiny",
        "scope",
        "NMSBA",
        "gap",
        "dialogue",
        "other physiological signal recording device",
        "best brain signal recording tool",
        "address inclusion–exclusion criteria",
        "relevant academic journal articles",
        "neural recording tools",
        "industry–academia collaboration",
        "brief intro- duction",
        "unbiased evidence collection",
        "clear con- clusions",
        "Numerous literature reviews",
        "other literature review",
        "mar- keting stimuli",
        "date research scopes",
        "systematic literature review",
        "new research field",
        "signal processing",
        "inclusion criteria",
        "Academic research",
        "relevant inference",
        "new perspective",
        "brain signals",
        "cal tools",
        "systematic review",
        "research question",
        "brain regions",
        "150 consumer neuroscience",
        "big brands",
        "efficient way",
        "lytical accuracy",
        "building blocks",
        "theoretical aspect",
        "business ethics",
        "current methods",
        "tech- niques",
        "art findings",
        "synthesized result",
        "impartial conclusion",
        "exhaustive search",
        "experimental findings",
        "neu- ral",
        "biometric data",
        "Book chapters",
        "marketing stimuli",
        "empirical experiments",
        "engineering part",
        "pretational methodologies",
        "important results",
        "lowing questions",
        "Neuromarketing experiments",
        "platform",
        "panies",
        "globe",
        "Google",
        "Microsoft",
        "Unilever",
        "insights",
        "tailored",
        "breakthrough",
        "acceptance",
        "world",
        "capacities",
        "management",
        "psychology",
        "focus",
        "regard",
        "premises",
        "types",
        "analysis",
        "techniques",
        "comprehensive",
        "knowledge",
        "methodology",
        "state",
        "synthesis",
        "recommendation",
        "body",
        "objective",
        "obligation",
        "investigation",
        "existing",
        "literatures",
        "evance",
        "studies",
        "analytical",
        "BCI",
        "language",
        "English",
        "brain signal recording techniques",
        "Neural response recording techniques",
        "Brain signal processing",
        "relevant existing literatures",
        "Machine learning applications",
        "autonomous nervous system",
        "Neuromarket- ing field",
        "different brain regions",
        "galvanic skin response",
        "Wiley Online Library",
        "Neuromarketing Product Chew",
        "neural data",
        "3  Systematic review",
        "database source",
        "book chapters",
        "six databases",
        "Taylor Francis",
        "initial article",
        "critical analy",
        "action coding",
        "successful means",
        "withdrawal actions",
        "engineering perspective",
        "statistical analy",
        "lyze activities",
        "Price Çakar",
        "Grigaliūnaitė",
        "Soria Morillo",
        "marketing strategies",
        "Neuromarketing questions",
        "search item",
        "Science Direct",
        "IEEE Xplore",
        "Table 1 Number",
        "database Results",
        "Emerald insight",
        "Neuromarketing studies",
        "following dimensions",
        "Marketing stimuli",
        "Table 2 Studies",
        "57 research articles",
        "931 articles",
        "internet",
        "Neuro-marketing",
        "publications",
        "aggregation",
        "Sage",
        "mulation",
        "title",
        "abstract",
        "keywords",
        "sis",
        "terms",
        "trends",
        "advancements",
        "Activation",
        "facial",
        "consumer",
        "arousal",
        "attention",
        "study",
        "Name",
        "tools",
        "paper",
        "form",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Touchette",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "al.",
        "Gong",
        "Pilelienė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Promotion",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "neural recording systems",
        "signal capturing technologies",
        "heart rate Cherubino",
        "EEG Soria Morillo",
        "brain region",
        "brain functions",
        "brain anatomy",
        "brain areas",
        "brain activity",
        "EMG Missagila",
        "Eye tracking",
        "Galvanic skin",
        "atic review",
        "human behavior",
        "cal functions",
        "true essence",
        "anatomical functionality",
        "high accuracy",
        "external stimuli",
        "fNIRS Çakir",
        "Neuromarketing Cherubino",
        "fMRI Venkatraman",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "subjects",
        "matter",
        "interconnection",
        "actions",
        "physiological",
        "hidden",
        "ences",
        "section",
        "targeted",
        "signals",
        "associated",
        "number",
        "3D (three dimensional) image",
        "machine learning techniques",
        "tral nervous system",
        "predictive modeling framework",
        "3D visualiza- tion",
        "other marketing models",
        "consumer behavior researchers",
        "E-commerce prod- ucts",
        "gift card limit",
        "virtual 3D bracelet",
        "30 existing image",
        "3D shapes",
        "consumer demand",
        "Consumer response",
        "consumer choice",
        "different strategies",
        "market- ing",
        "quantitative assessment",
        "rological data",
        "thinking procedures",
        "last 5  years",
        "two forms",
        "physical object",
        "important role",
        "user esthetics",
        "new area",
        "mathematical model",
        "lis superformula",
        "school bags",
        "first-time buyers",
        "partici- pants",
        "commerce website",
        "negative emotion",
        "necessary buttons",
        "sorting options",
        "Retail businesses",
        "large amount",
        "large number",
        "mock shop",
        "right-handed participants",
        "test participants",
        "different designs",
        "shoe designs",
        "E-commerce products",
        "retail products",
        "test subjects",
        "normal vision",
        "brain response",
        "user experience",
        "42 product images",
        "product search",
        "wrong product",
        "possible preference",
        "60 bracelet",
        "medications",
        "history",
        "neuropathology",
        "Nemorin",
        "price",
        "promotions",
        "service",
        "beverage",
        "decision",
        "objects",
        "recognition",
        "bracelets",
        "advantage",
        "apparels",
        "accessory",
        "items",
        "shirts",
        "sweaters",
        "shoes",
        "wrist",
        "watches",
        "view",
        "maximum",
        "money",
        "thousands",
        "blueprints",
        "manufacturing",
        "failures",
        "3.1",
        "corporate social respon- sibilities",
        "frontal asymmetry theory",
        "female undergraduate students",
        "capital white letter",
        "luxury) prod- ucts",
        "high emotional value",
        "higher emotional value",
        "shoe selection time",
        "high-resolution computer screen",
        "60 basic brand items",
        "wine tasting experiment",
        "regular brand products",
        "luxury brand products",
        "brain response-based prediction",
        "consumer brain response",
        "60 luxury items",
        "gift value",
        "brand images",
        "brand names",
        "brand logos",
        "national brand",
        "social atmosphere",
        "apparel products",
        "branded products",
        "shoe experiment",
        "EEG signals",
        "Likert scale",
        "self-report-based methods",
        "sales data",
        "12.1% profit growth",
        "36.4% profit growth",
        "young adults",
        "random order",
        "alone atmosphere",
        "brain responses",
        "consumer attitude",
        "Tahoma font",
        "black background",
        "other hand",
        "implicit response",
        "different types",
        "important factor",
        "interesting concept",
        "tional companies",
        "responsible companies",
        "decision-making time",
        "passive role",
        "keting strategies",
        "lower degree",
        "con- trol",
        "pivotal role",
        "purchase decision",
        "two wines",
        "lesser price",
        "rank",
        "simulation",
        "choice",
        "Davidson",
        "comparison",
        "brands",
        "blocks",
        "popular",
        "experiments",
        "branding",
        "user",
        "first-time",
        "promotional",
        "relation",
        "author",
        "company",
        "socially",
        "influence",
        "discounts",
        "gifts",
        "gift-giving",
        "ambiguity",
        "40",
        "mon experimental design procedure",
        "merchandise product advertisement clips",
        "wine tasting session",
        "older adults’ reaction",
        "mated review systems",
        "different aged population",
        "facial biometric sensors",
        "TV commer- cials",
        "six smartphone commercials",
        "super bowl commercials",
        "promotion-based Neuromarketing experiments",
        "The Neuromarketing experiments",
        "sustainable product designs",
        "seven TV commercials",
        "unfavorable TV commercials",
        "cognitive neurophysiological indices",
        "consecutive advertise- ments",
        "experimental setting",
        "conventional product",
        "14 TV commercials",
        "100 TV commercials",
        "different brands",
        "ral indices",
        "Nestle advertisement",
        "television commercials",
        "consecutive advertisements",
        "TV advertisements",
        "equal price",
        "short movies",
        "key focus",
        "higher rate",
        "ful promotions",
        "memory rate",
        "time distance",
        "neutral stimuli",
        "white screen",
        "green scenario",
        "electrical activity",
        "advanced algorithms",
        "emotional changes",
        "cerebral activity",
        "strong images",
        "pulse analysis",
        "print media",
        "animated commercial",
        "verbal narrative",
        "ing commercial",
        "visual narrative",
        "graduate students",
        "class rank",
        "emotional states",
        "visual attentions",
        "lit- eratures",
        "standard time",
        "video advertisements",
        "neural response",
        "consumer engagement",
        "audience brain",
        "web page",
        "user preference",
        "audiences’ taste",
        "user attention",
        "favorable advertisements",
        "static advertisements",
        "50 commercials",
        "communication",
        "Neuromarketers",
        "air",
        "extract",
        "happiness",
        "surprise",
        "10 s",
        "participants",
        "observation",
        "Krugman",
        "success",
        "neuroimaging",
        "gies",
        "TVC",
        "30 s",
        "other",
        "videos",
        "social adver- tisements",
        "social advertisement stimuli",
        "social groups",
        "documentary film",
        "gaming video",
        "true response",
        "different purposes",
        "gender-related advertisements",
        "appli- cation",
        "cigarette commercials",
        "smoking cessa",
        "atten- tion",
        "substantial role",
        "advertisement industry",
        "gender-targeted marketing",
        "tion frames",
        "drama",
        "Neuromarketing",
        "The",
        "ads",
        "messages",
        "smokers",
        "celebrity",
        "endorsement",
        "Missaglia",
        "research"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 2.9058723,
      "content": "\nSentiment analysis and the complex \nnatural language\nMuhammad Taimoor Khan1*, Mehr Durrani2, Armughan Ali2, Irum Inayat3, Shehzad Khalid1 and Kamran \nHabib Khan4\n\nIntroduction\nSentiment analysis (Pang and Lillian 2008) is a type of text classification that deals with \nsubjective statements. It is also known as opinion mining, since it processes opinions in \norder to learn about public perception. Sentiment analysis and opinion mining are the \nsame, and are used interchangeably throughout the document. It uses natural language \nprocessing (NLP) to collect and examine opinion or sentiment words. SA is explained \nas identifying the sentiments of people about a topic and its features (Pang and Lillian \n2008). The reason for the popularity of opinion mining is because people prefer to take \nadvice from others in order to invest sensibly. Determining subjective attitudes in big \nsocial data is a hotspot in the field of data mining and NLP (Hai et al. 2014).\n\nAbstract \n\nThere is huge amount of content produced online by amateur authors, covering a \nlarge variety of topics. Sentiment analysis (SA) extracts and aggregates users’ senti-\nments towards a target entity. Machine learning (ML) techniques are frequently used \nas the natural language data is in abundance and has definite patterns. ML techniques \nadapt to domain specific solution at high accuracy depending upon the feature set \nused. The lexicon-based techniques, using external dictionary, are independent of data \nto prevent overfitting but they miss context too in specialized domains. Corpus-based \nstatistical techniques require large data to stabilize. Complex network based tech-\nniques are highly resourceful, preserving order, proximity, context and relationships. \nRecent applications developed incorporate the platform specific structural information \ni.e. meta-data. New sub-domains are introduced as influence analysis, bias analysis, and \ndata leakage analysis. The nature of data is also evolving where transcribed customer-\nagent phone conversation are also used for sentiment analysis. This paper reviews \nsentiment analysis techniques and highlight the need to address natural language \nprocessing (NLP) specific open challenges. Without resolving the complex NLP chal-\nlenges, ML techniques cannot make considerable advancements. The open issues and \nchallenges in the area are discussed, stressing on the need of standard datasets and \nevaluation methodology. It also emphasized on the need of better language models \nthat could capture context and proximity.\n\nKeywords: Sentiment analysis, Machine learning, Sentiment orientation, Complex \nnetworks\n\nOpen Access\n\n© 2016 Khan et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nREVIEW\n\nKhan et al. Complex Adapt Syst Model  (2016) 4:2 \nDOI 10.1186/s40294-016-0016-9\n\n*Correspondence:   \ntaimoor.muhammad@gmail.\ncom \n1 Bahria University, Shangrilla \nRoad, Sector E-8, Islamabad, \nPakistan\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40294-016-0016-9&domain=pdf\n\n\nPage 2 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nManufacturers are also interested to know which features of their products are more \npopular in public, in order to make profitable business decisions. There is a huge reposi-\ntory of opinion content available at various online sources in the form of blogs, forums, \nsocial media, review websites etc. They are growing, with more opinionated content \npoured in continuously. It is, therefore, beyond the control of manual techniques to \nanalyze millions of reviews and to aggregate them towards a rapid and efficient deci-\nsion. Sentiment analysis techniques perform this task through automated processes with \nminimal or no user support. The online datasets may also contain objective statements, \nwhich do not contribute effectively in sentiment analysis. Such statements are filtered at \npre-processing.\n\nOpinion mining deals with identifying opinion patterns and presenting them in a \nway that is easy to understand. The outcome of sentiment analysis can be in the form \nof binary classification, such as categorizing opinions as recommended or not recom-\nmended. It can be considered as a multi-class classification problem on a given scale of \nlikeness. Cambria et al. (2013) used common-sense knowledge to improve the results of \nsentiment analysis. The results can be presented in the form of a short summary gen-\nerated from the overall analysis. Sentiment analysis has various sub streams including \nemotion analysis, trend analysis, and bias analysis etc. Its applications has outgrown \nfrom business to social, political and geographical domains. Sentiment analysis is \napplied to emails for gender identification through emotion analysis (Mohammad and \nYang 2011). Emotion is applied to fairy tales to draw interesting patterns (Mohammad \n2011). Considering text a complex network of words that are associated to each other \nwith sentiments, graph based analysis techniques are used for NLP tasks.\n\nNatural language processing\n\nOpinion mining requires NLP, to extract semantics of opinion words and sentences. \nHowever, NLP has open challenges that are too complex to be handled accurately till \ndate. Since sentiment analysis makes extensive use of NLP, it has this complex behav-\nior reflected. The assumptions in NLP for text categorization do not work with opinion \nmining, as they are different in nature. Documents having high frequency of matching \nwords may not necessarily possess same sentiment polarity. It is because, a fact in text \ncategorization could be either correct or incorrect, and is well known to all. Unlike facts, \na variety of opinions can be correct about the same product, due to its subjective nature. \nAnother difference is that, opinion mining is sensitive to individual words, where a sin-\ngle word like NOT may change the whole context. The open challenges are negations \nwithout using NOT word, sarcastic and comparative sentences etc. The later section has \na detailed discussion on NLP issues that affect sentiment analysis.\n\nThe subjective content from the online sources have simple, compound or complex \nsentences. Simple sentences possess single opinion about a product, while compound \nsentences have multiple opinions expressed together. Complex sentences have implicit \nmeaning and are hard to evaluate. Regular opinions pertain to a single entity only, while \ncomparative opinions have an object or some of its aspects discussed in comparison to \nanother object. Comparative opinions can either be objective or subjective. An example \nof a subjective sentence having comparison is “The sound effects of game X are much \nbetter than that of game Y” whereas an example of objective sentence with comparison is \n\n\n\nPage 3 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\n“Game X has twice as many control options as that of Game Y”. Opinion mining expects \na variety of sentence types, since people follow different writing styles in order to express \nthemselves in a better way.\n\nSentiment analysis\n\nThe machine learning (ML) based techniques are supervised, semi supervised or unsu-\npervised. The supervised techniques require labeled data, while the semi supervised \ntechniques need manual tuning from domain experts. The unsupervised techniques \nmake use of statistical analysis on large volume of data. ML techniques has a large fea-\nture set using Bag-of-words (BOW). Results are improved by pruning repetitive and \nlow quality features. The opinion words are extracted to identify the polarity of opinion \nexpressed for a feature. The performance of a classifier is measured through its effective-\nness at the cost of efficiency. Effectiveness is calculated as precision/recall and F-meas-\nure, which are measurements of relevance.\n\nSentiment analysis can also be considered as a complex network. It consists of nodes \nand edges joining them. Many complex systems from a variety of domains are repre-\nsented as network including environmental modeling (Niazi et al.  2010), business sys-\ntems (Aoyama 2002), wireless sensors, and ad-hoc networks (Niazi and Hussain 2009). \nNetworks are rich in information, having a range of local and global properties. Text cor-\npora can be used with words as nodes and edges representing the structural or seman-\ntic association between them. The adjacent nodes sharing a link are closely associated \nand directly affect each other through the weight of the link they share. Representing \ntext as complex network, various properties like centrality, degree distribution, com-\nponents, communities, paths etc. can be used to explore the data thoroughly. Through \nmulti-partite graphs, nodes can be distributed among various clusters with inter-cluster \nedges only. It separates different types of entities discussed in comparison. Entities are \nlinked to their respective aspects/features and then to the sentiments associated. The \nsentiments can be linked with the reasons shared in support of those sentiments.\n\nData sources\n\nOpinion mining has diverse subjective data sources that are available online. They cover \na large number of topics and are up-to-date with current issues. Introduction of Web2.0 \nin the last decade has enabled people to post their thoughts and opinions on a range of \ntopics. The data produced online is growing all the time produced by people from differ-\nent backgrounds (Katz et al. 2015). Opinion mining makes use of this data generated by \nmillions of users all over the world. According to Business Week survey in 2009, 70 % of \nthe people consult online reviews and ratings to make a purchase. Comscore/The Kelsey \ngroup in 2007 reported that 97 % of the people who made purchases based on online \nreviews, found them to be honest.\n\nThe user generated subjective content is of value to be assessed and summarized for \nprospective customers. These online data sources are in the form of blogs, reviews and \nsocial media websites. The popularity of blogging is on the rise, where people from dif-\nferent walks of life express their opinions about various entities and events and get com-\nments on them. At times, it leads to a form of discussion among the author and various \nusers commenting on them. A detailed analysis on blogging styles of authors, as they \n\n\n\nPage 4 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nfollow their own unique approaches for expressing their feelings is provided in (Chau \nand Xu 2007). Blogs contain opinions about various products, services, their features, \npackages and promotions. Most of the online studies on opinion extraction use blogs as \ndatasets (Qiang and Rob 2009) to perform detailed analysis.\n\nThere are professional review websites providing customers’ feedbacks, used for sen-\ntiment analysis. E-commerce websites allow customers to comment on their products. \nSocial media is another popular medium of sharing information among like-minded \npeople. Here, a variety of subjects are discussed where people express their opinions, \nbased on their own experience. Social media websites have a very complex struc-\nture for extracting information having user opinions. They allow users to express their \nviews through sharing articles and other media sources as an external link. Twitter, also \nreferred to as microblogging, has the problem of reviews being too short and at times \nmiss the context.\n\nThis review article is organized into the following divisions. Section 2 reviews the Sen-\ntiment analysis techniques and the NLP issues. Section 3 provides a discussion on the \nreview studied and Sect. 4 list the application areas for sentiment analysis. Section 5 has \nconcluded the study to important issues drawn from the study. Section 6 has distribu-\ntion of the work carried out by the authors.\n\nReview\nThe sentiment analysis techniques categorize reviews into positive and negative bins or \nmultiple degrees of it. The social data can be analyzed at three different levels i.e. user \ndata, relationship data and content (Tang et al.  2014). In survey (Guellil and Boukhalfa \n2015) these categories are further elaborated. Recommender systems are extended to \nsupport textual content using knowledge (Tang et al. 2013). In our previous work (Khan \nand Khalid 2015) sentiment analysis is highlighted to address health care problems from \nthe view point of a user. The issues faced in SA also depend on the data sources and \nnature of analysis required. An important aspect of social data analysis is the identifi-\ncation of sentiments and sentiment targets (Tuveri and Angioni 2014; Zhang and Liu \n2014). Opinion mining also consider the additional features of opinion holder and time. \nSentiment analysis techniques can be separated into three groups: supervised, semi-\nsupervised and unsupervised techniques.\n\nThe supervised techniques are the machine learning classifiers. They are more accu-\nrate, however, need to be trained on a relevant domain. The unsupervised statistical \ntechniques do not require training. They are efficient in dynamic environment but at the \ncost of accuracy. Sentiment analysis techniques analyze opinion datasets to generate a \ngeneral perception that people have about a product. The classification of sentiments in \na review document is performed through identifying and separating all the positive and \nnegative opinion words. Considering the strength of these words, along with their polar-\nity, helps in multi-class classification. Machine learning classifiers such as Naive-Bayes, \nk-nearest neighbor and centroid based classifier etc., are successfully used for this pur-\npose. Semantic orientation based techniques used for opinion mining are Lexicon based \nand statistical analysis. Lexicon based technique works with individual words while sta-\ntistical analysis incorporates words co-occurrence using point wise mutual information \n(PMI) and latent semantic analysis (LSA). Semi-supervised techniques start with a small \n\n\n\nPage 5 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nset of opinion words from the given domain, and expand on it. More opinion words are \nexplored by querying the starting seeds. The newly found words are queried again to find \nmore words until no new words are returned. Orientation of the opinion word form the \nbasis for classification. Other attributes used are frequency of occurrence, location and \nco-occurrence with other words. The taxonomy of these approaches is shown in Fig. 1.\n\nSentiment classification\n\nThese are the machine learning classifiers used for sentiment analysis. They can be \napplied to text documents at three levels for analysis. A document level approach, \nwhich studies the whole document as a single entity is appropriate for text categoriza-\ntion. However, document level approach is not viable for sentiment analysis with docu-\nments having multiple opinions. Therefore, sentiment analysis is performed extensively \nat sentence or word level. Word level analysis is also known as sentiment level analysis. \nML techniques suits sentiment analysis as the data is in abundance and there is obvious \npresence of patterns (Schouten and Frasincar 2015). The classifiers are trained on label \ndataset having samples representing all classes. A test dataset is used to evaluate the per-\nformance of the classifiers for the given task. Let the set of documents as {D = d1,…,dn}, \nand set of classes labeled as {C = c1,…,cn}, then the task is to classify document di in D \nwith a label ci in C. This task can be performed using supervised classifiers. The more \nfrequently used classifiers for sentiment analysis are discussed below.\n\nNaïve Bayes\n\nNaive Bayes (NB) classifier is extensively used for text classification. It learns from a \ntraining dataset of annotated feature vectors, with labels as positive and negative (in case \nof binary classification). The probability of a feature vector is calculated with each label \nusing the annotated training dataset. The feature vector is assigned a label that has high-\nest probability for it. If this information is preserved, it can be used to show confidence \nin a label for a feature vector. In further modifications of NB a fuzzy region is defined \nin which feature vectors hold both labels with a certain level of confidence. Text data \nnormally have high dimensional feature vectors. Therefore, the process of calculating \nprobability is repeated for each feature vector, and then all the probabilities contribute \ntowards the final decision. The feature set is represented as F = f1, f2…fm}, where prob-\nability of a document belonging to a class shown as:\n\nFig. 1 Taxonomy of expository literature on sentiment analysis\n\n\n\nPage 6 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nShows the probability of a document dj represented by its vector dj* belonging to a class \nci. It is the product of probabilities for all the features in the feature set. The document \nvector dj\n\n* is assigned to a class ci in order to maximize P\n(\n\nci\n\n∣\n\n∣\n\n∣\nd∗j\n\n)\n\n. The logarithm of prob-\nabilities are summed up to classify an opinion document. It is preferred over product of \nprobabilities to avoid underflow. It addresses the missing value problem as well. Slack \nvariables add smoothing effect against noisy data. Weights can also be assigned to fea-\ntures which define their contribution towards the classification. It is a biased approach, \nwhere prominent features are given high weights to play a major role in choose a senti-\nment label.\n\nNaive Bayes works on the assumption that all the sentences of a review document are \nopinion sentences. It also assumes that features of a document are independent of each \nother. Despite of this unrealistic assumption, Naïve Bayes is very successful and is used \nin various practical applications. The assumption of treating features as independent of \neach other makes Naive Bayes highly efficient (Dai et al. 2007). Although, Naive Bayes \nclassifier is simple, yet it is effective because of its robustness to irrelevant features. It \nperforms well in domains with many equally important features. It is considered to be \nmore reliable for text classification and sentiment analysis. The accuracy of the classifier \nimproves with pre-processing noise. It also used as transfer learning when trained on a \ndataset similar to the target dataset.\n\nNearest neighbor\n\nk-nearest neighbor classifier has been frequently used in literature for text classifica-\ntion. It considers the labels of k nearest neighbors to classify a test document. A special \ncase of the k-NN problem is typically referred to as classimbalance problem identified \nin (Yang and Liu 1999). Classes with more training data have higher influence to predict \nsame label for the new document. There are fewer chances of acquiring a class label if \nthat class has fewer training examples. (Li et al. 2003) catered this problem by using vari-\nable value of k for each class. Thus, the class having more training data will have higher \nvalue of k as compared to the one having few samples. This solution is helpful in online \nclassification, where there is time constraint on trying different values of k.\n\nA study on performance of k-NN using pre-processed dataset is conducted in (Shin \net al. 2006) claiming 10 % improvement when noise and outliers are filtered out. An opti-\nmum value is chosen as threshold to separate regular data from noise. Sentiment analy-\nsis is performed with a reduced set of feature vector in (Sreemathy and Balamurugan \n2012) to avoid the curse of dimensionality. Accuracy of the model improves as irrelevant \nfeatures were removed. Features are assigned weights to vary their contribution towards \ndecision making. Weights are extracted from probability of information in documents \nacross different categories. Tree-fast k-NN is introduced as fast kNN model (Soucy and \nMineau 2001). This tree based indexing of retrieval system improves the accuracy of \nk-NN in distance calculation. Its effective against large feature sets. The order of features \nand their thresholds are identified from within the training data. k-NN has promising \n\n(1)P(ci\n∣\n\n∣dj\n∗\n) =\n\np(ci)(\n∏m\n\ni=1 p(fi|ci) )\n\np(d∗j )\n\n\n\nPage 7 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nresults in sentiment analysis; however, it is more susceptible to noise and high dimen-\nsional feature set. Therefore, more of the work in k-NN for text classification has focused \non feature selection and reduction techniques as they are the driving factors of k-NN’s \nperformance.\n\nCentroid based\n\nCentroid based (CB) classifier calculates centroid vector or prototype vector for each \nclass in the training dataset. Centroid vector is the central point of the class and may not \nrepresent an actual training data. The distance of each test document is calculated with \nthe prototype vector of the class and is classified based on similarity with it. Its perfor-\nmance depends on the chosen centroid vectors. It is efficient since time and space com-\nplexities are proportional to the number of classes rather than training documents. To \ndouble the training data reverse of reviews are generated in (Xia et al. 2015) by invert-\ning the sentiment terms and their labels. Using both sets of training data with Mutual \nInformation (MI) the results were improved when only selected reviews were inverted. \nExternal dictionary WordNet is used to generate inverse for sentiment terms, however, \npseudo-antonyms can be generated internally using the corpus.\n\nms, however, pseudo-antonyms can be generated internally using the corpus. A variety \nof approaches have been used for CB classifier. Rocchio algorithm calculates centroid \nto represent feature space of documents (Ana and Arlindo 2007; Tan 2007a, b). Cen-\ntroid is computed through average of positive examples in (Han and Karypis 2000) and \nsum of positive cases i.e. the related training examples (Chuang et al. 2000). Normalized \nsum of positive vectors used in (Lertnattee and Theeramunkong2004), cosine similar-\nity between the test document and the Centroid of a class (Hidayet and Tunga 2012). \nCentroid is used with inverse of class similarity as well improving the accuracy close to \n100 % on the given dataset when characters are chosen as features instead of n-grams.\n\nCentroid evaluation is sensitive to noise in the training dataset which affects the over-\nall performance of the classifier. This shortfall is exposed when Centroid classifier is \napplied to a slightly different domain. The reason for this drawback is that some opinion \nwords are domain dependent. They have different polarity or strength of polarity when \nused in a different domain. Smoothing techniques have being proposed in (Tan 2007a, b;  \nLertnattee and Theeramunkong 2006; Guan 2009) that minimizes the effect of noise \nin the dataset. (Chizi et al. 2009) defined a weighting scheme giving higher weights to \nexplicit opinion words. Characters and special characters for feature selection are used \nin (Ozgur and Gungor 2009). The work in (Shankar and Karypis 2000; Tan et al. 2005) \nis focused on adjusting the value of centroid based with feedback looping, hypothesis \nmargin and weight-adjustment respectively. They try to rectify class Centroid, if it is not \ncalculated accurately. Centroid based classifier performs efficiently as it doesn’t consider \ntraining data each time to decide a test document.\n\nSupport vector machine\n\nSupport vector machine classifier is used for text classification in various studies. It finds \na separation among the data using the annotated training dataset. The margin of sep-\naration between classes, which is known as hyperplane, is used to classify the incom-\ning data. The hyperplane should give maximum separation between the classes. It is \n\n\n\nPage 8 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\napplicable even in the presence of high dimensional feature set representation. It classify \nbased o hyperplane among classes. Like centroid-based, SVM also consider the hyper-\nplane to classify a test document. (Brown et  al. 1997) has compared SVM with artifi-\ncial neural networks for text classification and has found it better. Since it has promising \nresults in text classification, it also performs well for opinion mining. They have also \nclaimed in (Brown et al. 1997) that SVM is better than Naive Bayes and decision trees \nclassification algorithms. However, SVM consumes more resources at the training \nstage. Although, it is efficient with large feature set, Feldman et al.(2011) has shown that \ndimensionality reduction in feature set further improves the performance of SVM. It \nexhibits linear complexity and can scale up to a large dataset.\n\nSVM has a limitation of over-reliance on selection of suitable kernel function. Kernel \nis calculated through Linear, Polynomial, Gaussian or sigmoid methods but they tend to \nbe domain specific. Kernel functions that perform well for one domain may not repeat it \nfor next. Its accuracy is also sensitive to number of training samples close to hyperplane. \nSlack variables are introduced to limit the impact of boundary samples by generalizing \nthe classifier, known as soft margin classification. They also help to avoid over-fitting the \ntraining data.\n\nUnsupervised techniques\n\nThe unsupervised sentiment analysis techniques do not require training data and rather \nrely on semantic orientation. They make use of lexicons to identify the positive or neg-\native semantics of opinion words. The meaning of the word, expressed by its use in a \ncontext is called lexicon. An online or off-line dictionary is consulted for this purpose. \nStatistical analysis techniques are also unsupervised, identifying the orientation of senti-\nment words through statistical evaluations. They require large volume of data for high \naccuracy.\n\nLanguages consists of lexicons that are the words used for a particular sense, and a \ngrammar that connect these lexicons. Part-of-speech rules are used to extract senti-\nment phrases from text document. Search engines are used to identify the orientation \nof sentiment words that are missing in the dictionary. Its polarity is identified through \nthe nearby words brought by search engines. They purely rely on external sources and \ntherefore cannot address the context. Lexicon based techniques perform well for general \ndomains while statistical techniques addresses the context and are useful in specialized \ndomains. The two types of approaches are discussed in detail.\n\nDictionary (Lexicon) based techniques\n\nLexicon based techniques extract opinion lexicons from the document and analyzes \nits orientation without the support of any training data. These techniques process the \nopinion words separately, ignoring the relationship between them. Lexicons refer to the \nsemantic orientation. Lexicons are independent of the source data and therefore it does \nnot fall for over-fitting. But context not addressed either in this approach (Katz et  al. \n2015; Cambria 2013). Search engines are used to find the meaning of unknown opinion \nlexicons. They are searched and the top N results are accepted to identify its orientation. \nThe semantics of lexicons can be categorized as positive or negative with weights rep-\nresenting their strength. This approach struggles with lexicons having domain specific \n\n\n\nPage 9 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\npolarity. For example, good has positive polarity in any type of domain but “heavy \nweight” has positive polarity for bike domain but negative for the domain of electronic \ndevices.\n\nIn its simplest form, sentiment words are split into positive and negative as binary \ndistribution. A more sophisticated approach has fuzzy lexicons, introducing a grey area \nbetween the two categories. These fuzzy lexicons exist in both the classes with a score \nassociated to it, representing the strength of each label. Various manual and semi-auto-\nmatic techniques can be used for building lexicons. Princeton University’s WordNet is \na popular lexicon source available for sentiment analysis. Dictionaries like WordNet, \nextracts synonyms and antonyms for the provided opinion words. Manual cleansing is \nemployed to rectify the lists generated for the unknown sentiment words. These opinion \nwords are used to classify a review as positive or negative.\n\nFixed syntactic patterns are also used for expressing opinions which are composed of \npart-of-speech (POS) tags. The basic idea of this technique is to identify the patterns in \nwhich words co-occur with each other and to exploit those patterns for understanding \nits semantic orientation. One example of such pattern is an adverb followed by an adjec-\ntive. A more sophisticated approach was proposed by (Mohammad and Yang 2011), \nwhich used a WordNet distance based method to determine the sentiment orientation. \nThe distance d(t1, t2) between terms t1 and t2 is the length of the shortest path that con-\nnects them in WordNet, as shown in Eq. 2. The semantic orientation (SO) of an adjective \nterm t is determined by its relative distance from two reference (or seed) terms good and \nbad. The polarity of opinion term t is resolved through eq.\n\nStatistics (Corpus) based techniques\n\nStatistical analysis of large corpus of text can also be used to determine the sentiment \norientation of words. Co-occurrence of words is evaluated without consulting any exter-\nnal support. Two methods are used for this purpose which are point wise mutual infor-\nmation (PMI) and latent semantic analysis (LSA). PMI method for co-occurrence is \ngiven as:\n\nwhere w1 and w2 refers to two words in a given sentence. The main concept behind PMI \nbased techniques is that the semantic orientation of a word has a tendency of being \nclosely related to that of its neighbors. Equation 3 gives the probability of words w1 and \nw2 to co-exist, based on the measure of degree of statistical dependence between the \ntwo. This approach is, however, implemented differently in LSA based techniques. In \nLSA, matrix factorization technique is used with singular value decomposition to dem-\nonstrate the statistical co-occurrence of words. More formally, this process can be speci-\nfied as:\n\n(2)SO(t) =\nd(t, bad)− d(t, good)\n\nd(bad, good)\n\n(3)p(w1,w2) =\np(w1,w2)\n\np(w1) p(w2)\n\n(4)LSA(w) = LSA(w, {+paradigms})− LSA(w, {−paradigms})\n\n\n\nPage 10 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nwhere a word w is passed to LSA with positive and negative paradigms. LSA based tech-\nniques develop a matrix having rows as words and columns as sentences or paragraphs. \nEach cell possesses a weight corresponding to the relation of the word in row with the \nsentence or paragraph in columns. This matrix is decomposed into three matrices using \nsingular value decomposition (SVD).\n\nComplex challenges\n\nOpinion mining is a relatively new area of research and there are open challenges that \nneed to be answered. Some of the challenges are common to opinion mining in general \nwhile others are related to their own sources and context depending upon the domain of \nthe dataset. These issues affect the performance of machine learning techniques, but it \nhas little control on them. Figure 2 gives NLP challenges faced in sentiment analysis, dis-\ntributing them into their logical groups. The groupings are based on the parsing level, at \nwhich these issues occur. The following sub section has detailed discussion on the NLP \nissues.\n\nDocument level\n\nDocument level NLP challenges are the ones that are faced at the document or review \nlevel. They deal in general with the review document or the reviewer style. It is common \nto find reviews that have the information about an object, given in an informal manner. \nCapitalization is over or under used. Spelling mistakes are ignored or words being short-\nened. It makes the analysis very difficult for the automatic techniques to identify features \nand associate them. The unknown words (shortened/miss spelled) are matched with \nsimilar words to identify the aspect or opinion words. Slang specific to a certain region \nare also occasionally used in reviews and discussions. Reviews having sarcastic expres-\nsions are the hardest to deal with. Even though they have the opinion words explicitly \nmen",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1582435,
      "metadata_storage_name": "s40294-016-0016-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDI5NC0wMTYtMDAxNi05LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Muhammad Taimoor Khan",
      "metadata_title": "Sentiment analysis and the complex natural language",
      "metadata_creation_date": "2016-02-02T06:54:52Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "customer- agent phone conversation",
        "Complex networks Open Access",
        "Complex Adapt Syst Model",
        "Creative Commons license",
        "users’ senti- ments",
        "domain specific solution",
        "profitable business decisions",
        "various online sources",
        "Complex network based",
        "specific structural information",
        "original author(s",
        "natural language processing",
        "specific open challenges",
        "natural language data",
        "data leakage analysis",
        "Muhammad Taimoor Khan1",
        "sentiment analysis techniques",
        "open issues",
        "author information",
        "language models",
        "influence analysis",
        "bias analysis",
        "ML) techniques",
        "ML techniques",
        "lexicon-based techniques",
        "statistical techniques",
        "sentiment words",
        "Sentiment orientation",
        "Mehr Durrani",
        "Armughan Ali",
        "Irum Inayat",
        "Shehzad Khalid1",
        "Habib Khan4",
        "text classification",
        "subjective statements",
        "subjective attitudes",
        "social data",
        "huge amount",
        "amateur authors",
        "large variety",
        "target entity",
        "Machine learning",
        "definite patterns",
        "high accuracy",
        "feature set",
        "external dictionary",
        "specialized domains",
        "large data",
        "Recent applications",
        "New sub-domains",
        "considerable advancements",
        "standard datasets",
        "evaluation methodology",
        "unrestricted use",
        "appropriate credit",
        "1 Bahria University",
        "Shangrilla Road",
        "Sector E",
        "Full list",
        "social media",
        "data mining",
        "opinionated content",
        "public perception",
        "opinion mining",
        "opinion content",
        "REVIEW Khan",
        "Kamran",
        "Introduction",
        "Pang",
        "Lillian",
        "type",
        "opinions",
        "order",
        "document",
        "NLP",
        "sentiments",
        "people",
        "topic",
        "features",
        "reason",
        "popularity",
        "advice",
        "others",
        "big",
        "hotspot",
        "field",
        "Hai",
        "Abstract",
        "abundance",
        "overfitting",
        "context",
        "Corpus-based",
        "proximity",
        "relationships",
        "platform",
        "meta-data",
        "nature",
        "paper",
        "need",
        "area",
        "better",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "DOI",
        "Correspondence",
        "Islamabad",
        "Pakistan",
        "end",
        "crossmark",
        "org",
        "Page",
        "19Khan",
        "Manufacturers",
        "products",
        "tory",
        "blogs",
        "forums",
        "websites",
        "graph based analysis techniques",
        "various sub streams",
        "different writing styles",
        "multi-class classification problem",
        "ML) based techniques",
        "Natural language processing",
        "many control options",
        "same sentiment polarity",
        "Sentiment analysis techniques",
        "binary classification",
        "complex network",
        "manual techniques",
        "supervised techniques",
        "overall analysis",
        "trend analysis",
        "statistical analysis",
        "complex sentences",
        "automated processes",
        "user support",
        "online datasets",
        "objective statements",
        "Such statements",
        "common-sense knowledge",
        "geographical domains",
        "gender identification",
        "fairy tales",
        "interesting patterns",
        "open challenges",
        "high frequency",
        "same product",
        "later section",
        "detailed discussion",
        "subjective content",
        "online sources",
        "single entity",
        "subjective sentence",
        "sound effects",
        "game X",
        "game Y",
        "objective sentence",
        "sentence types",
        "machine learning",
        "manual tuning",
        "domain experts",
        "large volume",
        "Opinion mining",
        "opinion patterns",
        "single opinion",
        "emotion analysis",
        "text categorization",
        "comparative sentences",
        "multiple opinions",
        "Regular opinions",
        "comparative opinions",
        "extensive use",
        "subjective nature",
        "gle word",
        "individual words",
        "NLP tasks",
        "NLP issues",
        "opinion words",
        "Simple sentences",
        "millions",
        "reviews",
        "rapid",
        "minimal",
        "way",
        "outcome",
        "form",
        "scale",
        "likeness",
        "Cambria",
        "results",
        "applications",
        "business",
        "political",
        "emails",
        "Mohammad",
        "Yang",
        "semantics",
        "date",
        "ior",
        "assumptions",
        "Documents",
        "matching",
        "fact",
        "variety",
        "difference",
        "negations",
        "NOT",
        "sarcastic",
        "compound",
        "implicit",
        "meaning",
        "aspects",
        "comparison",
        "example",
        "Bag",
        "BOW",
        "diverse subjective data sources",
        "seman- tic association",
        "Business Week survey",
        "Many complex systems",
        "complex struc- ture",
        "professional review websites",
        "other media sources",
        "social media websites",
        "The Kelsey group",
        "low quality features",
        "online data sources",
        "E-commerce websites",
        "online studies",
        "effective- ness",
        "Sentiment analysis",
        "environmental modeling",
        "wireless sensors",
        "global properties",
        "degree distribution",
        "multi-partite graphs",
        "different types",
        "large number",
        "current issues",
        "last decade",
        "ferent walks",
        "detailed analysis",
        "unique approaches",
        "popular medium",
        "various properties",
        "various clusters",
        "opinion extraction",
        "online reviews",
        "prospective customers",
        "customers’ feedbacks",
        "ad-hoc networks",
        "blogging styles",
        "external link",
        "various products",
        "adjacent nodes",
        "various entities",
        "user opinions",
        "Results",
        "repetitive",
        "polarity",
        "performance",
        "classifier",
        "cost",
        "efficiency",
        "Effectiveness",
        "precision/recall",
        "measurements",
        "relevance",
        "edges",
        "domains",
        "Niazi",
        "Aoyama",
        "Hussain",
        "information",
        "range",
        "local",
        "Text",
        "pora",
        "structural",
        "weight",
        "centrality",
        "ponents",
        "communities",
        "paths",
        "inter-cluster",
        "reasons",
        "support",
        "topics",
        "Web2.0",
        "thoughts",
        "backgrounds",
        "Katz",
        "users",
        "world",
        "ratings",
        "purchase",
        "Comscore",
        "value",
        "rise",
        "life",
        "events",
        "times",
        "discussion",
        "author",
        "feelings",
        "Chau",
        "Xu",
        "services",
        "packages",
        "promotions",
        "datasets",
        "Qiang",
        "Rob",
        "minded",
        "subjects",
        "experience",
        "articles",
        "Twitter",
        "point wise mutual information",
        "Semantic orientation based techniques",
        "centroid based classifier",
        "health care problems",
        "latent semantic analysis",
        "Lexicon based technique",
        "machine learning classifiers",
        "document level approach",
        "timent analysis techniques",
        "three different levels",
        "unsupervised statistical techniques",
        "text categoriza- tion",
        "Word level analysis",
        "sentiment level analysis",
        "social data analysis",
        "negative opinion words",
        "unsupervised techniques",
        "view point",
        "three levels",
        "sentiment analysis",
        "Semi-supervised techniques",
        "negative bins",
        "sentiment targets",
        "three groups",
        "opinion holder",
        "opinion datasets",
        "following divisions",
        "application areas",
        "multiple degrees",
        "relationship data",
        "Recommender systems",
        "data sources",
        "important aspect",
        "identifi- cation",
        "additional features",
        "dynamic environment",
        "general perception",
        "polar- ity",
        "k-nearest neighbor",
        "starting seeds",
        "Other attributes",
        "text documents",
        "docu- ments",
        "label dataset",
        "test dataset",
        "Sentiment classification",
        "review document",
        "new words",
        "other words",
        "important issues",
        "review article",
        "user data",
        "textual content",
        "previous work",
        "relevant domain",
        "multi-class classification",
        "Section 2",
        "Sect.",
        "Section 5",
        "study",
        "Section 6",
        "authors",
        "positive",
        "Tang",
        "survey",
        "Guellil",
        "Boukhalfa",
        "categories",
        "knowledge",
        "Khan",
        "Khalid",
        "SA",
        "Tuveri",
        "Angioni",
        "Zhang",
        "Liu",
        "rate",
        "training",
        "accuracy",
        "product",
        "strength",
        "Naive-Bayes",
        "pose",
        "occurrence",
        "PMI",
        "small",
        "More",
        "basis",
        "frequency",
        "location",
        "taxonomy",
        "approaches",
        "Fig.",
        "sentence",
        "presence",
        "patterns",
        "Schouten",
        "Frasincar",
        "classes",
        "formance",
        "task",
        "many equally important features",
        "high dimensional feature vectors",
        "various practical applications",
        "Naïve Bayes",
        "annotated feature vectors",
        "text classifica- tion",
        "k-nearest neighbor classifier",
        "fewer training examples",
        "senti- ment label",
        "k nearest neighbors",
        "missing value problem",
        "Naive Bayes classifier",
        "document vector dj",
        "high weights",
        "fewer chances",
        "Text data",
        "document dj",
        "training data",
        "fuzzy region",
        "final decision",
        "prob- abilities",
        "Slack variables",
        "smoothing effect",
        "noisy data",
        "fea- tures",
        "biased approach",
        "major role",
        "transfer learning",
        "classimbalance problem",
        "able value",
        "time constraint",
        "different values",
        "document di",
        "opinion document",
        "test document",
        "new document",
        "prominent features",
        "irrelevant features",
        "target dataset",
        "online classification",
        "processed dataset",
        "NB) classifier",
        "same label",
        "supervised classifiers",
        "expository literature",
        "opinion sentences",
        "pre-processing noise",
        "special case",
        "k-NN problem",
        "higher influence",
        "unrealistic assumption",
        "class ci",
        "class label",
        "k.",
        "C.",
        "labels",
        "probability",
        "confidence",
        "modifications",
        "level",
        "probabilities",
        "Taxonomy",
        "The",
        "logarithm",
        "underflow",
        "contribution",
        "Dai",
        "robustness",
        "samples",
        "solution",
        "Shin",
        "10 % improvement",
        "outliers",
        "opti",
        "External dictionary WordNet",
        "fast kNN model",
        "explicit opinion words",
        "related training examples",
        "sional feature set",
        "actual training data",
        "training data reverse",
        "large feature sets",
        "reduced set",
        "positive examples",
        "regular data",
        "feature vector",
        "feature selection",
        "decision making",
        "different categories",
        "retrieval system",
        "reduction techniques",
        "driving factors",
        "prototype vector",
        "central point",
        "Rocchio algorithm",
        "Cen- troid",
        "positive cases",
        "positive vectors",
        "different domain",
        "Smoothing techniques",
        "weighting scheme",
        "feedback looping",
        "hypothesis margin",
        "training dataset",
        "feature space",
        "sentiment terms",
        "training documents",
        "Centroid based",
        "centroid vector",
        "Centroid evaluation",
        "mum value",
        "distance calculation",
        "Mutual Information",
        "different polarity",
        "higher weights",
        "special characters",
        "Tree-fast k-NN",
        "Centroid classifier",
        "CB classifier",
        "class Centroid",
        "class similarity",
        "threshold",
        "noise",
        "Sreemathy",
        "Balamurugan",
        "curse",
        "dimensionality",
        "Accuracy",
        "Soucy",
        "Mineau",
        "indexing",
        "dj",
        "work",
        "plexities",
        "number",
        "Xia",
        "inverse",
        "pseudo-antonyms",
        "corpus",
        "Arlindo",
        "average",
        "Karypis",
        "sum",
        "Chuang",
        "Lertnattee",
        "Theeramunkong2004",
        "cosine",
        "Hidayet",
        "Tunga",
        "n-grams",
        "shortfall",
        "drawback",
        "Guan",
        "effect",
        "Chizi",
        "Ozgur",
        "Gungor",
        "Shankar",
        "∏",
        "high dimensional feature set representation",
        "decision trees classification algorithms",
        "Support vector machine classifier",
        "unsupervised sentiment analysis techniques",
        "large feature set",
        "cial neural networks",
        "Statistical analysis techniques",
        "suitable kernel function",
        "top N results",
        "incom- ing data",
        "soft margin classification",
        "Lexicon based techniques",
        "senti- ment words",
        "Centroid based classifier",
        "unknown opinion lexicons",
        "Unsupervised techniques",
        "ment phrases",
        "large dataset",
        "statistical evaluations",
        "various studies",
        "sep- aration",
        "Naive Bayes",
        "training stage",
        "dimensionality reduction",
        "sigmoid methods",
        "Kernel functions",
        "training samples",
        "boundary samples",
        "particular sense",
        "speech rules",
        "Search engines",
        "nearby words",
        "external sources",
        "two types",
        "source data",
        "text document",
        "maximum separation",
        "linear complexity",
        "one domain",
        "ative semantics",
        "general domains",
        "semantic orientation",
        "line dictionary",
        "hyperplane",
        "SVM",
        "Brown",
        "resources",
        "Feldman",
        "limitation",
        "over-reliance",
        "selection",
        "Polynomial",
        "Gaussian",
        "impact",
        "use",
        "online",
        "purpose",
        "Languages",
        "grammar",
        "detail",
        "relationship",
        "negative",
        "weights",
        "WordNet distance based method",
        "popular lexicon source",
        "singular value decomposition",
        "machine learning techniques",
        "Fixed syntactic patterns",
        "LSA based techniques",
        "matrix factorization technique",
        "unknown sentiment words",
        "Complex challenges",
        "relative distance",
        "PMI method",
        "Statistical analysis",
        "electronic devices",
        "simplest form",
        "grey area",
        "two categories",
        "Various manual",
        "Princeton University",
        "Manual cleansing",
        "POS) tags",
        "basic idea",
        "shortest path",
        "adjective term",
        "two reference",
        "nal support",
        "Two methods",
        "main concept",
        "statistical dependence",
        "three matrices",
        "new area",
        "little control",
        "sentiment orientation",
        "NLP challenges",
        "opinion term",
        "sophisticated approach",
        "fuzzy lexicons",
        "One example",
        "eq. Statistics",
        "large corpus",
        "statistical co-occurrence",
        "two words",
        "domain specific",
        "bike domain",
        "negative paradigms",
        "positive polarity",
        "good",
        "score",
        "label",
        "Dictionaries",
        "synonyms",
        "antonyms",
        "lists",
        "review",
        "speech",
        "understanding",
        "adverb",
        "t2",
        "length",
        "seed",
        "text",
        "mation",
        "w1",
        "w2",
        "tendency",
        "neighbors",
        "Equation",
        "measure",
        "degree",
        "process",
        "rows",
        "columns",
        "paragraphs",
        "cell",
        "relation",
        "SVD",
        "research",
        "general",
        "sources",
        "dataset",
        "issues",
        "Figure 2",
        "Document level NLP challenges",
        "following sub section",
        "parsing level",
        "review level",
        "logical groups",
        "reviewer style",
        "informal manner",
        "Spelling mistakes",
        "automatic techniques",
        "unknown words",
        "similar words",
        "groupings",
        "object",
        "Capitalization",
        "analysis",
        "aspect",
        "Slang",
        "region",
        "discussions"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 2.615263,
      "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 2 of 19\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n�\ni K xi; x\n\n� �þ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2;…; ∂�N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n� �\n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK xi; x j\n\n� �� �\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 3 of 19\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 4 of 19\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 5 of 19\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 6 of 19\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 7 of 19\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 8 of 19\n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi�−R0∅i\n�−b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 9 of 19\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 10 of 19\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2713106,
      "metadata_storage_name": "s13640-018-0373-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMTgtMDM3My04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Boping Zhang",
      "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform",
      "metadata_creation_date": "2018-11-23T11:51:48Z",
      "keyphrases": [
        "accurate feature point alignment model",
        "new online shopping mode",
        "Augmented reality virtual glasses",
        "external propaganda channels",
        "actual wearing effects",
        "Creative Commons Attribution",
        "lates physical information",
        "phys- ical information",
        "head pose estimation",
        "traditional online shopping",
        "virtual glasses model",
        "input device-monocular camera",
        "key research issue",
        "virtual world scene",
        "user online glasses",
        "RESEARCH Open Access",
        "1 Introduction Network virtual",
        "iOS mobile platform",
        "local face features",
        "virtual model",
        "feature points",
        "scale-invariant feature",
        "new way",
        "2018 Open Access",
        "research key",
        "local features",
        "virtual information",
        "Face information",
        "Information Engineer",
        "real world",
        "Virtual try",
        "favorite glasses",
        "different glasses",
        "user senses",
        "face detection",
        "human face",
        "iOS glasses",
        "Boping Zhang",
        "optimal purchase",
        "robust SIFT",
        "rapid achievement",
        "sponding images",
        "vector machine",
        "EURASIP Journal",
        "Video Processing",
        "The Author",
        "iOS platform",
        "AR glasses",
        "camera image",
        "quick try",
        "human senses",
        "AR principle",
        "Computer vision",
        "real time",
        "real-time interaction",
        "Xuchang University",
        "APP system",
        "descent method",
        "computer technology",
        "SVM classifier",
        "Abstract",
        "development",
        "commerce",
        "gap",
        "goods",
        "speed",
        "immersion",
        "SDM",
        "identification",
        "Keywords",
        "merchants",
        "consumers",
        "important",
        "part",
        "field",
        "experiential",
        "position",
        "angle",
        "corre",
        "screen",
        "sound",
        "taste",
        "touch",
        "space",
        "sensory",
        "perience",
        "faces",
        "bopingzhang",
        "yeah",
        "School",
        "Henan",
        "China",
        "article",
        "terms",
        "real-time head mo- tion tracking",
        "Two corresponding isosceles triangles",
        "General face recognition process",
        "original author(s",
        "charge-coupled device) camera",
        "Android mobile platform",
        "Creative Commons license",
        "real envir- onment",
        "foreign glasses sellers",
        "3D modeling approach",
        "2D image overlay",
        "2D image superposition",
        "AR technology design",
        "2D face images",
        "virtual optician system",
        "image processing technology",
        "real-time requirements",
        "2D images",
        "accurate tracking",
        "International License",
        "The superposition",
        "3D glasses",
        "glasses images",
        "network virtual",
        "virtual try",
        "virtual objects",
        "Virtual glasses",
        "unrestricted use",
        "appropriate credit",
        "Research status",
        "Smart Look",
        "wearing effect",
        "research hotspots",
        "research results",
        "four categories",
        "Three points",
        "fine transformation",
        "unrealistic deformation",
        "inter- action",
        "good results",
        "different defects",
        "depth research",
        "large extent",
        "new ideas",
        "false ratios",
        "important indicator",
        "feature extraction",
        "dimension reduc",
        "first step",
        "cific location",
        "Glasses companies",
        "glasses model",
        "AR-based glasses",
        "face features",
        "face area",
        "face needs",
        "matching recognition",
        "doi.org",
        "orcid.org",
        "shopping experience",
        "input sensor",
        "experience senses",
        "input image",
        "computer vision",
        "augmented reality",
        "video stream",
        "system performance",
        "online shopping",
        "based detection",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Zhang",
        "crossmark",
        "crossref",
        "dialog",
        "USA",
        "Camirror",
        "Ipoint",
        "Kisok",
        "Xview",
        "function",
        "Users",
        "domestic",
        "Meijing",
        "Kede",
        "Biyao",
        "present",
        "Huang",
        "eyes",
        "pose",
        "scale",
        "method",
        "realism",
        "Cheng",
        "monocular",
        "CCD",
        "marker",
        "kinds",
        "techniques",
        "advantages",
        "problems",
        "application",
        "precondition",
        "frame",
        "posture",
        "lumination",
        "occlusion",
        "omission",
        "people",
        "Fig.",
        "gion",
        "range",
        "The Gaussian kernel function Kðx",
        "three-layer feedforward neural network",
        "Face recognition process Zhang",
        "high-dimensional face feature vector",
        "Sigmoid kernel functions",
        "local binary patterns",
        "local structural content",
        "Common kernel func",
        "high-dimensional feature space",
        "spatial fre- quency",
        "machine learning method",
        "high-dimensional feature vector",
        "online matching models",
        "low-dimensional subspace information",
        "support vector machine",
        "optimal parameter vector",
        "Feature dimension reduction",
        "different scale spaces",
        "face feature identification",
        "sional face features",
        "face detection methods",
        "3.1 SVM-based face detection",
        "¼ sgn XN train",
        "low-dimensional input space",
        "robust SIFT algorithm",
        "classic SVM algorithm",
        "K xi",
        "Nsv X",
        "detection process",
        "vector dimension",
        "width function",
        "quadratic function",
        "discriminant function",
        "Common features",
        "space complexity",
        "3.2 Face recognition",
        "Matching recognition",
        "Feature extraction",
        "Gabo feature",
        "spatial position",
        "offline learning",
        "optimization parameter",
        "input vector",
        "face database",
        "face alignment",
        "matching strategy",
        "ginal features",
        "network output",
        "detection result",
        "∂�N train",
        "oriented gradient",
        "illumination changes",
        "small displacements",
        "ture changes",
        "brightness invariance",
        "direction selectivity",
        "previous links",
        "final decision",
        "virtual glasses",
        "Markov model",
        "statistical theory",
        "nonlinear mapping",
        "mal hyperplane",
        "largest interval",
        "inner product",
        "Mercer condition",
        "Optimization problem",
        "training sample",
        "face image",
        "video sequence",
        "description ability",
        "output layer",
        "Figure 3 shows",
        "tected image",
        "LBP",
        "HOG",
        "histogram",
        "Gabor",
        "insensitiveness",
        "overall",
        "Page",
        "time",
        "premise",
        "technology",
        "scholars",
        "HMM",
        "AdaBoost",
        "structure",
        "Equation",
        "polynomial",
        "zÞ",
        "Eq.",
        "ÞT",
        "jy",
        "XNtrain",
        "∂i",
        "j � �� � ð3Þ",
        "conditions",
        "rotation",
        "ð1Þ",
        "2σ",
        "∗",
        "Figure 4 shows flow block diagram",
        "stable image fea- tures",
        "other feature extraction functions",
        "higher positioning accuracy",
        "SVM network structure",
        "highest gradient value",
        "ian Laplace function",
        "potential fixed points",
        "variable scale parameter",
        "Establishing Gaussian pyramid",
        "SVM classifier Zhang",
        "Gaussian differential functions",
        "Spatial extreme detection",
        "gradient direction histogram",
        "stitute tower model",
        "two adjacent layers",
        "Key point matching",
        "image matching model",
        "scale space sequence",
        "scale space factor",
        "Gaussian difference pyramid",
        "DoG operator curve",
        "Gaussian difference function",
        "local extreme points",
        "② Position key points",
        "tower top image",
        "local feature",
        "Gaussian function",
        "feature construction",
        "candidate position",
        "feature vector",
        "Gauss-Laplacian curve",
        "local gradients",
        "Gaussian template",
        "Gaussian blur",
        "Laplacian operator",
        "ence space",
        "jacent points",
        "brightness changes",
        "sta- bility",
        "affine transformation",
        "angle change",
        "Basic principle",
        "multiple details",
        "image search",
        "stability degree",
        "main contour",
        "edge features",
        "larger scale",
        "blurred image",
        "human eye",
        "original image",
        "first layer",
        "new image",
        "second layer",
        "minimum dimension",
        "red line",
        "blue line",
        "same group",
        "SIFT algorithm",
        "generation process",
        "main direction",
        "formation process",
        "pixel point",
        "convolution operation",
        "different sizes",
        "scale normalization",
        "The relationship",
        "noise",
        "scales",
        "selection",
        "kind",
        "symbol",
        "target",
        "retina",
        "dimensions",
        "location",
        "down-sampling",
        "pyramids",
        "bottom",
        "number",
        "¼ log2",
        "qð",
        "logarithm",
        "maxima",
        "minima",
        "2G",
        "constant",
        "images",
        "3.2.1",
        "3.2.2",
        "σ",
        "ð6Þ",
        "∇",
        "Fig. 4 SIFT algorithm flow chart",
        "Terravic Re- search Corporation",
        "red intermediate detection point",
        "N+ 3-layer Gaussian pyramid",
        "N+ 2-layer DoG pyramid",
        "Fig. 5 Gaussian pyramid Zhang",
        "match- ing performance",
        "SIFT matching algorithm",
        "face infrared database",
        "feature point field",
        "20 infrared image sequences",
        "initial matching method",
        "N extreme points",
        "Face recognition experiment",
        "scale space value",
        "lower scale spaces",
        "Gaussian difference image",
        "Gradient histogram statistics",
        "7 Gaussian pyramid",
        "key point",
        "Ransac method",
        "group Zhang",
        "adjacent upper",
        "adjacent points",
        "correct matching",
        "gradient value",
        "successful matching",
        "matching process",
        "edge response",
        "independent characteristic",
        "direc- tion",
        "square column",
        "head rotation",
        "light-illuminated pictures",
        "mismatch rate",
        "correct matches",
        "mismatched points",
        "peak value",
        "matching accuracy",
        "gradient direction",
        "auxiliary direction",
        "contributive pixels",
        "Three pairs",
        "other words",
        "Error degree",
        "lower images",
        "total number",
        "26 points",
        "60 pairs",
        "120 images",
        "calculation",
        "surrounding",
        "case",
        "invariance",
        "perspective",
        "set",
        "tors",
        "descriptor",
        "probability",
        "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi",
        "neighborhood",
        "Comparison",
        "Gauss-Laplacian",
        "36 columns",
        "maximum",
        "robustness",
        "glasses",
        "hats",
        "Figure",
        "work",
        "classic",
        "grees",
        "2.5",
        "3.",
        "θ",
        "360°",
        "10°",
        "tracking registra- tion subsystem",
        "DoG space extreme point",
        "nonlinear feature extraction function",
        "many face alignment algorithms",
        "SDM-based face alignment process",
        "face feature point alignment",
        "ture point location",
        "feature vector descriptor",
        "main direction Zhang",
        "original data information",
        "initialization feature point",
        "head rotation angle",
        "n feature points",
        "① SIFT matching performance",
        "face feature points",
        "Harris matching method",
        "ginal face image",
        "face image detection",
        "3.3 Face alignment",
        "function approximation",
        "SIFT feature",
        "head posture",
        "real face",
        "face images",
        "matching re",
        "experimental data",
        "motion direction",
        "key points",
        "contour points",
        "estimation error",
        "average face",
        "number difference",
        "same number",
        "success rate",
        "overall trend",
        "inevitable errors",
        "good effect",
        "step size",
        "optimal position",
        "sonable translation",
        "shape factors",
        "optimal solution",
        "initial value",
        "training process",
        "perimental image",
        "Image normalization",
        "illumination transformation",
        "scaling transformation",
        "first sample",
        "sample size",
        "other variables",
        "Table 1 show",
        "experimental samples",
        "tween average",
        "SDM algorithm",
        "120 samples",
        "Figures",
        "sults",
        "hat",
        "light",
        "shade",
        "darkness",
        "matches",
        "finiteness",
        "positioning",
        "eyebrows",
        "nose",
        "mouth",
        "Zhu",
        "offset",
        "iteration",
        "current",
        "efficiency",
        "interference",
        "center",
        "x∗",
        "coordinates",
        "128 di",
        "mensions",
        "R128n",
        "operation",
        "Δx",
        "xk",
        "bk",
        "paths",
        "3.1",
        "∅",
        "3.3.3 SDM algorithm alignment result",
        "lateral face alignment results",
        "Face feature point location",
        "iteration step size",
        "objective function Eq",
        "principal component analysis",
        "world coordinate system",
        "rigid body invariance",
        "LFW face databases",
        "Surrey Face Model",
        "current spatial position",
        "new spatial position",
        "general 3D face",
        "3.4.1 Feature point labelling",
        "3D face model",
        "linear regression problem",
        "Local feature extraction",
        "3.4 Face pose estimation",
        "local feature points",
        "late 3D coordinates",
        "71 face feature points",
        "68 face feature points",
        "camera parame- ters",
        "3D geometric position",
        "3D rigid object",
        "geometric model parameters",
        "Sample sequence set",
        "3D position",
        "3D pose",
        "alignment accuracy",
        "132 face images",
        "206 face images",
        "803 face images",
        "input feature",
        "Matching results",
        "corresponding point",
        "The pose",
        "2D coordinates",
        "N points",
        "six points",
        "3D object",
        "test sample",
        "argminR0b0 X",
        "two parameters",
        "critical dimensions",
        "pts file",
        "head surface",
        "most cases",
        "optical properties",
        "projection relationship",
        "two movements",
        "② Rotary movement",
        "XYZ axis",
        "six degrees",
        "six numbers",
        "nose tip",
        "extraction process",
        "tion coordinates",
        "test phase",
        "test images",
        "rota- tion",
        "training set",
        "dimension reduction",
        "camera imaging",
        "Camera labeling",
        "camera relative",
        "① Translation movement",
        "hat Zhang",
        "Translation vector",
        "x0",
        "Δxi",
        "way",
        "Rk",
        "impact",
        "amount",
        "Section",
        "detail",
        "IBUG",
        "former",
        "sets",
        "orientation",
        "periments",
        "direction",
        "idea",
        "lows",
        "freedom",
        "corners",
        "SFM",
        "τ",
        "radial distortion parame- ters",
        "Match result analysis table",
        "key points Match ratio",
        "2D facial feature points",
        "False match rate",
        "y direc- tions",
        "least square method",
        "Tracking registration technology",
        "computer-generated virtual objects",
        "4.3 Feature point mapping",
        "arbitrary reference/coordinate system",
        "3.5 Tracking registration system",
        "face alignment process",
        "unknown scaling factor",
        "3D facial point",
        "tersects image plane",
        "2D image plane",
        "rotation matrix R",
        "image coordinate system",
        "2D position",
        "Head rotation",
        "P point",
        "manual labeling",
        "world coordinates",
        "focal length",
        "Variate Number",
        "Total number",
        "Wearing glasses",
        "position O",
        "linear equations",
        "Equa- tion",
        "following form",
        "correct poses",
        "tration techniques",
        "3D point",
        "3D coordinates",
        "same image",
        "pose estimation",
        "optical center",
        "translation vector",
        "camera matrix",
        "S X",
        "projection error",
        "3D model",
        "Table 1",
        "5 ¼ R",
        "5 ¼ S",
        "Yang",
        "Patras",
        "systems",
        "parameters",
        "change",
        "plenty",
        "problem",
        "fx",
        "fy",
        "ray",
        "Eqs",
        "distance",
        "tures",
        "sum",
        "squares",
        "scenes",
        "4.2",
        "664",
        "geometric transformation relation track- ing method",
        "face feature point tracking method",
        "front face alignment",
        "corresponding perspective changes",
        "poor user experience",
        "overlarge head corner",
        "good tacking effect",
        "spective transformation",
        "user head",
        "Face geometry",
        "face superposition",
        "stereoscopic changes",
        "human head",
        "first technique",
        "tech- nique",
        "two methods",
        "movement",
        "3D",
        "estimation",
        "picture"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 2.0950272,
      "content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n� The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning � Covariate shift �\nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg � X � Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg � X � Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ�Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ�Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ�Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n� min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp �cjjxtr � x\nðiÞ\nte jj22\n\n� �\n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð�cjjxtr � x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð�cjjxtr � x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k � n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð�cjjxtar � xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr � xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n� �� �\n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� y\n\nðiÞ\ntr � f x\n\nðiÞ\ntr\n\n� �� �2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n� �\n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x� CÞ or maxð0;C � xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods sho",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 456223,
      "metadata_storage_name": "s40708-015-0017-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDcwOC0wMTUtMDAxNy16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Zengda Guan",
      "metadata_title": "Local regression transfer learning with applications to users’ psychological characteristics prediction",
      "metadata_creation_date": "2015-08-12T10:56:23Z",
      "keyphrases": [
        "China A. Li Black Dog Institute",
        "weighted risk regression model",
        "Z. Guan Business School",
        "cn T. Zhu Institute",
        "Local regression transfer learning",
        "cn A. Li",
        "Adaptive parameter-setting method",
        "New South Wales",
        "Local transfer learning",
        "clustering reweighting methods",
        "Shandong Jianzhu University",
        "individual psychological characteristics",
        "online user experience",
        "transfer learning methods",
        "personal psychological characteristics",
        "independent test dataset",
        "labelled training dataset",
        "psychological characteristics prediction",
        "Beijing Forestry University",
        "Zengda Guan",
        "Ang Li",
        "Tingshao Zhu",
        "personal preference",
        "online services",
        "web user",
        "The Author",
        "open access",
        "Recent studies",
        "training instance",
        "different genders",
        "ferent districts",
        "� Covariate shift",
        "recent decades",
        "important role",
        "human lives",
        "consumer preference",
        "consistent traits",
        "changeable status",
        "key factors",
        "one type",
        "human behaviour",
        "web behaviour",
        "Many studies",
        "Most studies",
        "many cases",
        "Chinese Academy",
        "Computing Technology",
        "Brain Informatics",
        "Previous studies",
        "little attention",
        "innovative approach",
        "learning models",
        "tional models",
        "generalization capability",
        "demographic variation",
        "identical distribution",
        "users’ personality",
        "behaviours",
        "applications",
        "article",
        "Springerlink",
        "Abstract",
        "differences",
        "problem",
        "bour",
        "importance",
        "situation",
        "labels",
        "experiments",
        "depression",
        "results",
        "Keywords",
        "1 Introduction",
        "people",
        "time",
        "Internet",
        "help",
        "assumption",
        "Jinan",
        "mail",
        "guanzengda",
        "sdjzu",
        "Department",
        "Psychology",
        "Sydney",
        "Australia",
        "blackdog",
        "Sciences",
        "tszhu",
        "DOI",
        "crossmark",
        "org",
        "low",
        "performance",
        "paper",
        "3",
        "14",
        "new local regression transfer learning methods",
        "Most existing covariate shift methods",
        "local transfer learning methods",
        "bility distribution PteðX",
        "local learning views",
        "adaptive k-NN methods",
        "ytrÞ�Pte lðxtr",
        "weighted risk model",
        "PteðxtrÞ PtrðxtrÞ",
        "general solution framework",
        "local neighbour region",
        "fðxð1Þte",
        "training-test k-NN method",
        "pre- diction functions",
        "covariate shift problems",
        "empirical risk form",
        "training data instances",
        "test dataset label",
        "test dataset model",
        "Þg � X � Y",
        "¼ fðxð1Þtr",
        "ytrÞ lðxtr",
        "test dataset population",
        "psychological characteristics labels",
        "learning model",
        "regression form",
        "local region",
        "¼ PteðXÞ",
        "prediction functions",
        "risk function",
        "expected risk",
        "PtrðXÞ",
        "simple covariate",
        "ðxðntrÞ",
        "data points",
        "parameter-setting method",
        "¼ PteðyjxÞ",
        "prediction accuracy",
        "training dataset",
        "entire dataset",
        "input dataset",
        "PtrðyjxÞ",
        "ðxðnteÞ",
        "resampling weight",
        "computation cost",
        "continual values",
        "unknown parameter",
        "exper- iments",
        "experiment results",
        "last section",
        "different distributions",
        "key point",
        "loss function",
        "ntr Xntr",
        "yÞ�Ptr",
        "probability distribution",
        "probability density",
        "previous work",
        "Y.",
        "purpose",
        "researches",
        "procedure",
        "addition",
        "parameters",
        "domains",
        "dictions",
        "rest",
        "Sect.",
        "background",
        "Ztr",
        "Zte",
        "inputs",
        "process",
        "hÞ",
        "datasets",
        "ratio",
        "2",
        "Gaussian kernel basis functions",
        "kernel mean matching method",
        "many machine learning tasks",
        "adaptive K-NN reweighting method",
        "training-test K-NN reweighting method",
        "weighted machine learning model",
        "complete covariate shift process",
        "many density estimation methods",
        "Gaussian kernel functions",
        "2.3.1 K-NN reweighting method",
        "Kullback–Leibler divergence",
        "reproducing-kernel Hilbert space",
        "comparative advan- tage",
        "density estima- tion",
        "nearest-neighbour approximation method",
        "Local machine learning",
        "local classification method",
        "k-nearest test set",
        "many neighbour points",
        "k-nearest-neighbour reweighting methods",
        "less neighbour points",
        "squares importance biases",
        "ratio PteðxtrÞ",
        "kernel methods",
        "learning process",
        "k-NN methods",
        "many kinds",
        "nearest-neighbour methods",
        "local learning",
        "density similarity",
        "transfer learning",
        "neighbour region",
        "histogram methods",
        "clustering methods",
        "PtrðxtrÞ",
        "prediction model",
        "146 Z. Guan",
        "significant effect",
        "one hand",
        "little relationship",
        "target point",
        "unnecessary interferences",
        "similar property",
        "small region",
        "big region",
        "other hand",
        "strong noise",
        "small proportion",
        "given point",
        "PðxÞ",
        "region volume",
        "one volume",
        "two stages",
        "test dataset",
        "first stage",
        "impor- tance",
        "origin form",
        "test points",
        "long-distance points",
        "target data",
        "data point",
        "training distributions",
        "nearest neighbours",
        "training instances",
        "training point",
        "training data",
        "input densities",
        "common situations",
        "total number",
        "errors",
        "researchers",
        "forms",
        "formula",
        "Sugiyama",
        "series",
        "Kanamori",
        "Huang",
        "KMM",
        "Dai",
        "Pardoe",
        "list",
        "boosting",
        "rithms",
        "size",
        "effects",
        "way",
        "nV",
        "ktr",
        "kte",
        "Loog",
        "work",
        "formance",
        "neighbourhood",
        "fact",
        "knowledge",
        "extension",
        "adaptation",
        "Clustering-based",
        "view",
        "ð3Þ",
        "clustering-based reweighting meth- ods",
        "3.2 Training-test K-NN reweighting method",
        "Cross validation tech- nique",
        "Gaussian kernel function ratio",
        "empirical parameter estimation way",
        "Clustering-based reweighting method",
        "nearest-neighbour reweighting method",
        "users’ psychological characteristics",
        "other big values",
        "nearest test set",
        "other test points",
        "covariate shift methods",
        "labelled test data",
        "training data points",
        "training data xtr",
        "k-NN method",
        "training data part",
        "togram method",
        "empirical way",
        "reweighting formula",
        "Gaussian kernels",
        "validation technique",
        "big bias",
        "nearest-neighbour point",
        "WeigðxtrÞ",
        "exponential term",
        "exponential law",
        "appropriate neighbour",
        "computation time",
        "comparable probability",
        "ðjÞ tr",
        "two parts",
        "important issue",
        "smaller value",
        "similar probability",
        "expð�cjjxtar",
        "small value",
        "2nneig Pnneig",
        "clustering algorithm",
        "test neighbours",
        "validation dataset",
        "k value",
        "mine k",
        "k � n",
        "gauðnneig",
        "density distance",
        "population size",
        "ktr Pktr",
        "k.",
        "¼ ktr",
        "Xk",
        "�cjjxtr",
        "bandwidth",
        "effect",
        "denominator",
        "c.",
        "Enas",
        "Choi",
        "threshold",
        "reason",
        "1 jjxtr",
        "kind",
        "histograms",
        "ð4Þ",
        "ð5Þ",
        "147",
        "jClustrðxðiÞtr Þj",
        "The Big Five personality model",
        "accurate density-region division methods",
        "lðyðiÞtr",
        "Big Five personality framework",
        "jClusteðxðiÞtr",
        "adaptive stepwise regression method",
        "WeigðxðiÞtr",
        "ðxðiÞte",
        "local transfer methods",
        "small enough region",
        "approximate computation way",
        "local reweighting methods",
        "2.4 Weighted regression model",
        "weighted learning model",
        "web users’ depression",
        "big influence",
        "five dimensions",
        "hjðxÞ",
        "weighted model",
        "C � xÞ",
        "regression function",
        "human personality",
        "histogram method",
        "PteðxtrÞ",
        "high-dimensional difficulty",
        "high-dimensional situation",
        "previous stage",
        "real value",
        "prediction value",
        "sion splines",
        "constant denoted",
        "hinge function",
        "total steps",
        "optimal performance",
        "cients bj.",
        "logical characteristics",
        "three parts",
        "148 Z. Guan",
        "different districts",
        "unpleasant emotions",
        "intellectual curiosity",
        "CES-T scale",
        "many data",
        "one histogram",
        "same cluster",
        "Clustering method",
        "probability estimation",
        "lowing formula",
        "one cluster",
        "different weight",
        "importance weighting",
        "tr ÞÞ",
        "� CÞ",
        "instances",
        "detail",
        "number",
        "idea",
        "equal",
        "risk",
        "Xntr",
        "bias",
        "MARS",
        "Xm",
        "bjhj",
        "product",
        "two",
        "respec",
        "3 Experiments",
        "psychology",
        "agreeableness",
        "conscientiousness",
        "extraversion",
        "neuroticism",
        "openness",
        "tendency",
        "degree",
        "creativity",
        "preference",
        "novelty",
        "ð6Þ",
        "ð7Þ",
        "famous microblog service providers",
        "male users’ depression degree",
        "many feature dimensions",
        "mogorov–Smirnov test",
        "503 million registered users",
        "high dimension curse",
        "Big Five dimensions",
        "online self-report questionnaire",
        "users’ web behaviours",
        "time series data",
        "additional linguistic features",
        "Guangdong users’ personality",
        "istration time",
        "test set",
        "T test",
        "two-sample test",
        "different users",
        "female users",
        "important factor",
        "training set",
        "3.1 Experiment setup",
        "ing personality",
        "depression scales",
        "digital records",
        "pression behaviours",
        "screen name",
        "facial picture",
        "privacy settings",
        "individual privacy",
        "vate messages",
        "interpersonal behaviours",
        "social interaction",
        "punctuation marks",
        "perceptual processing",
        "stepwisefit method",
        "Matlab toolbox",
        "gender-personality experiment",
        "district-personality experiment",
        "depression experiment",
        "same distribution",
        "Gaussian distribution",
        "unknown distribution",
        "Weibo users",
        "five categories",
        "online behaviours",
        "behavioural data",
        "Sina Weibo",
        "personal image",
        "personal page",
        "sentiment words",
        "cognitive words",
        "Guangdong dataset",
        "female dataset",
        "personality experiments",
        "sion experiments",
        "dynamic features",
        "845 features",
        "20 features",
        "relationship",
        "personality/depression",
        "example",
        "suitability",
        "difference",
        "China",
        "research",
        "consent",
        "prediction",
        "May",
        "August",
        "562 participants",
        "profiles",
        "demographics",
        "expression",
        "self-statement",
        "concern",
        "comments",
        "strangers",
        "outcomes",
        "friends",
        "microblogs",
        "period",
        "apps",
        "June",
        "1000 participants",
        "characters",
        "numerals",
        "pronouns",
        "relevant",
        "Both",
        "22",
        "other local trans- fer learning methods",
        "adaptive k-NN transfer learning methods",
        "test k-NN transfer learning method",
        "other transfer learning methods",
        "famous transfer learning method",
        "regression transfer learning methods",
        "adaptive k-NN reweighting method",
        "Local kNN reweighting methods",
        "transfer learning way",
        "mean square error",
        "Kol- mogorov–Smirnov",
        "weighted risk form",
        "global transfer method",
        "Kolmogorov–Smirnov test",
        "clustering-based reweighting method",
        "single-dimension T test",
        "fixed k value",
        "other k-NN",
        "KMM reweighting method",
        "male users’ personality",
        "global k-NN",
        "regression software",
        "risk models",
        "traditional method",
        "baseline method",
        "training- test",
        "test results",
        "open source",
        "female datasets",
        "situa- tions",
        "trait A",
        "total size",
        "perfor- mances",
        "TTkNN method",
        "5 personality dimensions",
        "MARS method",
        "GkNN method",
        "best results",
        "c value",
        "result comparisons",
        "distribution divergence",
        "comparable performance",
        "Clust denotes",
        "parameter values",
        "C.",
        "4 dimensions",
        "MARS.",
        "neighbours",
        "Matlab/Octave",
        "rtu",
        "lv",
        "jekabsons",
        "tables",
        "figures",
        "nation",
        "AkNN1",
        "AkNN2",
        "MSE",
        "genders",
        "task",
        "2 features",
        "1 features",
        "probability",
        "confidence",
        "situations",
        "others",
        "advantage",
        "impact",
        "Fig.",
        "noise",
        "149",
        "25",
        "test results Condition A C E N O",
        "Local regression transfer learning results",
        "other local transfer learning methods",
        "stable density estimation method",
        "male users’ depression level",
        "k-NN transfer methods",
        "regression transfer methods",
        "clustering transfer methods",
        "other two traits",
        "least k value",
        "five personality traits",
        "other districts",
        "clustering method",
        "five traits",
        "The method",
        "best method",
        "0, outlier point",
        "strong influence",
        "optimization result",
        "three traits",
        "M S",
        "150 Z. Guan",
        "cs.rtu",
        "regular rule",
        "person- ality",
        "mogorov–Smirnov",
        "one trait",
        "3 feature dimensions",
        "5 feature dimensions",
        "different-gender datasets",
        "different-district datasets",
        "best performance",
        "optimal value",
        "Weibo data",
        "gender experiment",
        "most situations",
        "cluster number",
        "clusters increases",
        "prediction performance",
        "5 traits",
        "exception",
        "shake-up",
        "factor",
        "GkNN",
        "constant",
        "diversity",
        "Guangdong",
        "province",
        "model",
        "total",
        "95% confidence",
        "performances",
        "Table 2",
        "local k-NN family transfer learning methods",
        "global k-NN transfer learning method",
        "predic- tion bias",
        "different- distribution datasets",
        "appropriate k value",
        "transfer method",
        "test data",
        "different-distribution feature",
        "subtle nature",
        "density estimation",
        "training",
        "experiment",
        "Table",
        "result",
        "non",
        "Clust",
        "other",
        "3.5 Discussion",
        "conclusion"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 1.8872213,
      "content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchical c",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1012456,
      "metadata_storage_name": "s40493-015-0015-3.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxNS0zLnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-19T12:42:36Z",
      "keyphrases": [
        "Creative Commons Attribution License",
        "mobile ad hoc networks",
        "Günther Pernul",
        "Universitätsstraße",
        "descriptive scenario-based analysis",
        "general prob- lem",
        "one novel idea",
        "Open Access article",
        "single building blocks",
        "RESEARCH Open Access",
        "online reputation systems",
        "new reputation systems",
        "hierarchical component taxonomy",
        "most reputation systems",
        "common reputation models",
        "Johannes Sänger",
        "hierarchical taxonomy",
        "common systems",
        "peer networks",
        "social networks",
        "component repository",
        "common models",
        "Christian Richthammer",
        "various disciplines",
        "application areas",
        "promising approaches",
        "implementation level",
        "obvious utility",
        "practical point",
        "last decade",
        "wide range",
        "considerable research",
        "data accuracy",
        "several environments",
        "statistical approaches",
        "personal preferences",
        "software engineering",
        "unrestricted use",
        "effective use",
        "computation engines",
        "computation phase",
        "graph-based models",
        "natural framework",
        "design process",
        "design knowledge",
        "Design approaches",
        "computation methods",
        "Reusable components",
        "Trust Management",
        "Trust pattern",
        "reputation-based trust",
        "context information",
        "original work",
        "Journal",
        "DOI",
        "Correspondence",
        "saenger",
        "wiwi",
        "regensburg",
        "University",
        "Germany",
        "Abstract",
        "problem",
        "scratch",
        "concepts",
        "achievements",
        "others",
        "shuffle",
        "reuse",
        "respect",
        "order",
        "conceptual",
        "view",
        "properties",
        "literature",
        "aspects",
        "Keywords",
        "Reusability",
        "Introduction",
        "metrics",
        "commerce",
        "eBay",
        "buyers",
        "relevance",
        "quality",
        "arithmetic",
        "multiple",
        "factors",
        "propagation",
        "solutions",
        "authors",
        "proposals",
        "development",
        "attention",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "mailto",
        "Page",
        "community",
        "efits",
        "specialists",
        "reliability",
        "goal",
        "classification",
        "functions",
        "many applied computation techniques",
        "design science research paradigm",
        "design pattern-like solution",
        "rational choice mechanism",
        "general problem context",
        "multifaceted terms trust",
        "computational trust models",
        "current models",
        "reputation models",
        "building block",
        "web services",
        "research gap",
        "objec- tives",
        "same time",
        "increasing distribution",
        "decision making",
        "network environments",
        "common understanding",
        "various fields",
        "multidimension- ality",
        "abstract concept",
        "behavioral dimension",
        "interpersonal phenomenon",
        "particular level",
        "subjective probability",
        "Sänger",
        "Multiple authors",
        "plex definitions",
        "reputation systems",
        "conceptual level",
        "following section",
        "important artifacts",
        "bility trust",
        "particular action",
        "future work",
        "several properties",
        "means",
        "rest",
        "paper",
        "guidelines",
        "Hevner",
        "overview",
        "motivation",
        "approach",
        "contribution",
        "name",
        "plans",
        "success",
        "Internet",
        "connectivity",
        "notion",
        "regard",
        "objectives",
        "topic",
        "decades",
        "uniform",
        "Reasons",
        "circumstance",
        "credibility",
        "confidence",
        "emotional",
        "nature",
        "sociologists",
        "psychologists",
        "Economists",
        "online",
        "Gambetta",
        "distrust",
        "agent",
        "group",
        "capacity",
        "security",
        "risk",
        "variety",
        "Table",
        "basis",
        "two common ways",
        "multiple discrete levels",
        "hard security mechanism",
        "One negative experience",
        "Propagative One property",
        "one aggregated picture",
        "many trust models",
        "multiple trust chains",
        "several trust models",
        "several models",
        "propagative nature",
        "aggregated value",
        "different chains",
        "recent years",
        "new experiences",
        "time-based aging",
        "greater importance",
        "old experiences",
        "delicious meal",
        "rela- tionships",
        "final decision",
        "greater influence",
        "distance-based aging",
        "subjective nature",
        "high level",
        "binary manner",
        "maximum value",
        "human nature",
        "hard evidence",
        "one agent",
        "other agents",
        "untrustworthy agents",
        "Dynamic Trust",
        "Context-dependent Trust",
        "trust value",
        "overall trust",
        "trust information",
        "trust statements",
        "lower trust",
        "Reflexive Trust",
        "specific context",
        "particular restaurant",
        "specific aspects",
        "long time",
        "particular agent",
        "distant nodes",
        "reviewer Rachel",
        "book review",
        "trust transitivity",
        "Policy-based trust",
        "same context",
        "values",
        "Table 1",
        "Overview",
        "acteristics",
        "example",
        "Alice",
        "Bob",
        "doctor",
        "cook",
        "customer",
        "food",
        "service",
        "combination",
        "amount",
        "use",
        "propagativity",
        "turn",
        "Claire",
        "Christianson",
        "Harbison",
        "transitive",
        "reverse",
        "case",
        "Composable",
        "Composition",
        "zon",
        "opinion",
        "numbers",
        "continuous",
        "variable",
        "interval",
        "reinforcing",
        "trustworthiness",
        "consideration",
        "establishment",
        "exchange",
        "credentials",
        "contrast",
        "history",
        "interactions",
        "estimation",
        "three process steps filtering",
        "most reputation-based trust models",
        "three fundamental phases",
        "repu- tation system",
        "function-based component classification",
        "current reputation models",
        "existing trust models",
        "three steps",
        "existing models",
        "component classes",
        "generic process",
        "two steps",
        "existing approaches",
        "existing systems",
        "soft security",
        "collective measure",
        "global value",
        "personal experience",
        "many approvedmodels",
        "promising thoughts",
        "generic mechanism",
        "compo- nent",
        "accepted principles",
        "one hand",
        "other hand",
        "critical question",
        "communica- tion",
        "feedback generation/collection",
        "feedback distribution",
        "central part",
        "common trust",
        "feedback aggregation",
        "existing concepts",
        "academic community",
        "context setting",
        "single components",
        "reusable components",
        "trust properties",
        "Research gap",
        "worthiness",
        "work",
        "thing",
        "character",
        "referrals",
        "ratings",
        "reviews",
        "members",
        "someone",
        "ideas",
        "benefits",
        "years",
        "Rehak",
        "instance",
        "capabilities",
        "sound",
        "new",
        "repository",
        "artifacts",
        "developers",
        "researchers",
        "novel",
        "way",
        "implemented",
        "communication",
        "ability",
        "table",
        "computationmethods",
        "analysis",
        "identification",
        "logical",
        "Figure",
        "preparation",
        "storage",
        "weighting",
        "trustor",
        "novel hierarchical component taxonomy",
        "common rep- utation systems",
        "three generic process steps",
        "two broad categories",
        "reputa- tion systems",
        "subsequent aggre- gation",
        "current trust models",
        "second process step",
        "single process steps",
        "Common reputation systems",
        "first two steps",
        "several reputation scores",
        "common examples",
        "subsequent computing",
        "utation information",
        "second case",
        "computation process",
        "trust relation",
        "first phase",
        "reputation value",
        "personalization parameters",
        "past behavior",
        "sonal experience",
        "other sources",
        "personal collections",
        "ferent peers",
        "distributed network",
        "different sources",
        "uniform format",
        "specific situation",
        "weight factors",
        "struc- ture",
        "decentralized environments",
        "functional blocks",
        "different surveys",
        "computation engine",
        "last phase",
        "first question",
        "filtering process",
        "hard selection",
        "soft selection",
        "secondary classes",
        "reputation data",
        "information amount",
        "extra information",
        "available data",
        "Preparation techniques",
        "preparation phase",
        "Figure 2 Classes",
        "public storage",
        "input data",
        "trustee",
        "context",
        "normalization",
        "output",
        "aggregation",
        "need",
        "processing",
        "line",
        "Zhang",
        "difference",
        "algo",
        "rithm",
        "structure",
        "decentralized/hybrid",
        "meaning",
        "transparency",
        "design",
        "section",
        "detail",
        "primary",
        "three broad classes attribute-based, statistic- based",
        "simple ballot stuffing attacks",
        "payment Sänger",
        "statistical filter technique",
        "one application area",
        "value imbalance problem",
        "Bayesian reputation systems",
        "cluster analysis approaches",
        "following classes",
        "Attribute-based filtering",
        "Attribute-based filters",
        "simple logic",
        "statistical patterns",
        "Clustering-based filter",
        "filtering step",
        "Other models",
        "filtering phase",
        "single attributes",
        "reference value",
        "huge amounts",
        "initial filtering",
        "weighting phase",
        "last 12 months",
        "positive, neutral",
        "Statistic-based filtering",
        "false rumors",
        "exemplary procedure",
        "cluster filtering",
        "available information",
        "current situation",
        "various data",
        "different factors",
        "different prices",
        "product types",
        "malicious seller",
        "cheap products",
        "many current",
        "reputation calculation",
        "Reputation data",
        "high reputation",
        "negative ratings",
        "positive ratings",
        "unfair ratings",
        "false ratings",
        "Further techniques",
        "majority rule",
        "filtering techniques",
        "weighting techniques",
        "false feedback",
        "other parties",
        "common experience",
        "fake transactions",
        "obvious way",
        "crucial attribute",
        "Context comparability",
        "constraint-factor",
        "lightweight",
        "Time",
        "Sporas",
        "party",
        "robustness",
        "spread",
        "Whitby",
        "dishonest",
        "advisor",
        "rater",
        "iCLUB",
        "clusters",
        "evaluations",
        "bootstrapping",
        "Dellarocas",
        "characteristics",
        "reason",
        "discounting",
        "methods",
        "non",
        "expensive",
        "situations",
        "1.",
        "several trust/reputa- tion values",
        "biometric identity trust model",
        "human decision makers",
        "adaptive forgetting scheme",
        "distinct information sources",
        "common aggregation techniques",
        "one important factor",
        "different application areas",
        "simple aggregation techniques",
        "low feedback reputation",
        "reputation values",
        "forgetting factor",
        "low impact",
        "Simple arithmetic",
        "critical factor",
        "first-hand information",
        "Different actors",
        "different perceptions",
        "aggregation process",
        "aggregated reputation",
        "good reputation",
        "reputation scoring",
        "Reputation systems",
        "file-sharing networks",
        "successful transaction",
        "customer satisfaction",
        "network structures",
        "transitivity) rate",
        "honest feedback",
        "recursive algorithm",
        "stronger punishment",
        "bad behavior",
        "good behavior",
        "dynamic nature",
        "Old feedback",
        "Jøsang",
        "Personal preferences",
        "various end-users",
        "direct experience",
        "weight-factor(s",
        "third step",
        "iterative manner",
        "single steps",
        "first class",
        "Criteria comparability",
        "different communities",
        "Rating value",
        "trust chain",
        "Other distinctions",
        "propagation degree",
        "web graph",
        "example measure",
        "time-based weighting",
        "new referrals",
        "agents’ reliability",
        "approaches",
        "evaluation",
        "costs",
        "level",
        "anonymity",
        "number",
        "peers",
        "differences",
        "recommendation",
        "second",
        "distance",
        "reputation-factor",
        "nodes",
        "honesty",
        "concept",
        "consequence",
        "bases",
        "calculation",
        "Google",
        "PageRank",
        "position",
        "website",
        "Epinions",
        "reviewers",
        "effects",
        "Sun",
        "adaptation",
        "importance",
        "significance",
        "newcomers",
        "tuple",
        "input",
        "phase",
        "cases",
        "list",
        "algorithms",
        "fuzzy",
        "2.",
        "4.",
        "5.",
        "beta PDF function parameter tuple",
        "human decision making process",
        "beta probability density function",
        "solid mathematical basis",
        "More complex solutions",
        "The beta PDF",
        "Beta Reputation system",
        "exact reputation score",
        "priori reputation score",
        "online social networks",
        "Other aggregation techniques",
        "Bayesian probabilistic models",
        "Hidden Markov Model",
        "graph-based flow model",
        "prominent trust models",
        "beta model",
        "Bayesian probability",
        "probability distributions",
        "probabilistic approach",
        "Markov Models",
        "Applied techniques",
        "graph-based approach",
        "belief model",
        "model truth",
        "Reputation values",
        "overall reputation",
        "basic way",
        "descending order",
        "message boards",
        "citation counts",
        "academic literature",
        "understandable algorithm",
        "last years",
        "statistical approach",
        "binary events",
        "Dempster-Shafer theory",
        "subjective logic",
        "machine learning",
        "dynamic behavior",
        "classical logic",
        "linguistic approach",
        "prominent example",
        "graph theory",
        "incoming edges",
        "outgoing edges",
        "several factors",
        "Popular algorithms",
        "Eigentrust Algorithm",
        "many models",
        "fuzzy logic",
        "trust management",
        "transitive trust",
        "trust chains",
        "fuzzy models",
        "new ratings",
        "Other examples",
        "ranking algorithms",
        "impact factor",
        "different measures",
        "one edge",
        "two nodes",
        "summation",
        "average",
        "proxy",
        "systems",
        "Slashdot",
        "implementation",
        "Amazon",
        "positive",
        "composition",
        "result",
        "weakness",
        "uncertainty",
        "DST",
        "Yu",
        "Singh",
        "generalization",
        "Malik",
        "falsity",
        "degree",
        "agent/resource",
        "REGRET",
        "information",
        "people",
        "field",
        "centrality",
        "Eigenvector",
        "betweenness",
        "α",
        "β",
        "reputa- tion system",
        "single logical components",
        "aggregation phases",
        "incremental nature",
        "one component",
        "primary class",
        "aggregation techniques",
        "hybrid model",
        "new classes",
        "novel models",
        "hierarchical c",
        "taxonomy",
        "heuristic",
        "knowledge",
        "extension"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 1.623031,
      "content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker  J Big Data            (2019) 6:95  \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence:   \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker  J Big Data            (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n\n\nPage 3 of 25Sarker  J Big Data            (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker  J Big Data            (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker  J Big Data            (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker  J Big Data            (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker  J Big Data            (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker  J Big Data            (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1  = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker  J Big Data            (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker  J Big Data            (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsivel",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1660534,
      "metadata_storage_name": "s40537-019-0258-4.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDI1OC00LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Iqbal H. Sarker ",
      "metadata_title": "Context-aware rule learning from smartphone data: survey, challenges and future directions",
      "metadata_creation_date": "2019-10-30T14:24:16Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "intelligent decision making strategies",
        "advanced data analytical techniques",
        "25Sarker  J Big Data",
        "Context‑aware rule learning",
        "Intelligent systems Open Access",
        "machine learning based techniques",
        "Creative Commons license",
        "corresponding context-aware rule learning",
        "Iqbal H. Sarker",
        "User behavior modeling",
        "creat iveco mmons",
        "intelligent context-aware applications",
        "raw contextual data",
        "corresponding search interests",
        "data-driven context-aware systems",
        "phone data analytics",
        "original author(s",
        "corresponding data processing",
        "multifunctional cell phones",
        "users’ surrounding environment",
        "smartphone data",
        "Data science",
        "Predictive analytics",
        "context-aware rules",
        "Context-aware computing",
        "author information",
        "future directions",
        "recent days",
        "daily life",
        "important IoT",
        "next generation",
        "wireless connectivity",
        "work coverage",
        "developed countries",
        "recent statistics",
        "Google Trends",
        "users’ interest",
        "other platforms",
        "Desktop Computer",
        "Laptop Computer",
        "current world",
        "recent developments",
        "behavioral activities",
        "wide attention",
        "high precision",
        "traditional approaches",
        "efficient results",
        "previous work",
        "rule-based automated",
        "IoT services",
        "unrestricted use",
        "appropriate credit",
        "doi.org",
        "1 Swinburne University",
        "Full list",
        "Tablet Computer",
        "last 5 years",
        "timestamp information",
        "particular date",
        "highest point",
        "personal devices",
        "Things) devices",
        "essential part",
        "world population",
        "Mobile Phones",
        "particular term",
        "SURVEY PAPER",
        "peak popularity",
        "challenges",
        "Introduction",
        "smartphones",
        "individuals",
        "Internet",
        "capabilities",
        "number",
        "Fig.",
        "Abstract",
        "ogy",
        "academia",
        "industry",
        "order",
        "set",
        "key",
        "contexts",
        "comparison",
        "effective",
        "article",
        "area",
        "discussion",
        "Clustering",
        "Classification",
        "Association",
        "Personalization",
        "Time-series",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "msarker",
        "Melbourne",
        "Australia",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "Figure",
        "x-axis",
        "range",
        "chart",
        "instance",
        "value",
        "maximum",
        "y-axis",
        "minimum",
        "context-aware mobile notification management system",
        "other broad application areas",
        "corresponding rule-based prediction model",
        "different context-aware test cases",
        "data-driven intelligent context-aware systems",
        "optimal time-based context-aware rules",
        "rule-based intelligent context-aware systems",
        "smartphone data analytic solutions",
        "context-aware smart city services",
        "users’ behavioral activity patterns",
        "intelligent data-driven decisions",
        "THEN” logical structure",
        "corresponding data clustering",
        "corresponding behavioral activities",
        "Users’ interest trends",
        "intelligent eHealth services",
        "aware behavioral rules",
        "relevant context-aware rules",
        "relevant contextual data",
        "contextual data patterns",
        "contextual data clustering",
        "machine learning researchers",
        "machine learning techniques",
        "different surrounding contexts",
        "machine learning rules",
        "relevant contextual information",
        "data mining techniques",
        "advanced data analysis",
        "rule-based classification",
        "raw data",
        "different contexts",
        "mobile notifications",
        "context-aware  software",
        "hidden patterns",
        "data science",
        "time-series data",
        "advanced features",
        "mental learning",
        "personalized services",
        "rity services",
        "deeper analysis",
        "association analysis",
        "detailed analysis",
        "daily basis",
        "great source",
        "useful information",
        "two parts",
        "temporal context",
        "tial context",
        "social contexts",
        "family members",
        "specific exceptions",
        "particular domain",
        "prac- titioners",
        "many reasons",
        "various purposes",
        "particular timestamp",
        "numeric values",
        "world-wide popularity",
        "ture optimization",
        "dynamic updating",
        "The reason",
        "huge amount",
        "unknown correlations",
        "diverse behaviors",
        "quent part",
        "individuals’ usage",
        "phone user",
        "devices",
        "result",
        "antecedent",
        "others",
        "example",
        "Alice",
        "work",
        "evening",
        "general",
        "preferences",
        "addition",
        "cable",
        "study",
        "applications",
        "building",
        "Several",
        "selection",
        "aim",
        "machine learning based context-aware rule learning framework",
        "Suggested machine learning based framework",
        "Context-aware rule based applications",
        "Context-aware rule learning strategies",
        "smartphone data analyt- ics",
        "real world applications",
        "rule discovery techniques",
        "decision making capability",
        "rule-based intelligent systems",
        "Human Computer Interaction",
        "smartphone data analytics",
        "corresponding context-aware rules",
        "contextual smartphone data",
        "Pervasive Computing area",
        "incremental learning",
        "context-aware computing",
        "updation techniques",
        "smrtphone data",
        "uitous Computing",
        "several perspectives",
        "time-series modeling",
        "various layers",
        "various perspectives",
        "different meanings",
        "different purposes",
        "numerous areas",
        "Computer-Supported Collaborative",
        "Ambient Intelligence",
        "recent works",
        "broader collection",
        "social aspects",
        "little work",
        "earlier work",
        "brief discussion",
        "background information",
        "main characteristics",
        "existing research",
        "brief survey",
        "first article",
        "multi-dimensional contexts",
        "sion” section",
        "profit",
        "analysis",
        "discretization",
        "issues",
        "contributions",
        "paper",
        "shortcomings",
        "role",
        "knowledge",
        "remainder",
        "variety",
        "notion",
        "mobile",
        "Ubiquitous",
        "context-awareness",
        "location",
        "people",
        "objects",
        "factors",
        "physical",
        "entity",
        "activities",
        "users",
        "definitions",
        "categories",
        "daily life activities",
        "temporal informa- tion",
        "ubiquitous computing community",
        "ubiquitous computing area",
        "Contextual smartphone data",
        "user diverse activities",
        "rent social situation",
        "data source",
        "smartphone usage",
        "temporal information",
        "defi- nitions",
        "dis- cussion",
        "Several studies",
        "surrounding people",
        "Other definitions",
        "other hand",
        "current situation",
        "nearby people",
        "available processors",
        "network capacity",
        "noise level",
        "lighting level",
        "day situations",
        "cellular phones",
        "computing environment",
        "location information",
        "locational information",
        "environmental information",
        "surrounding information",
        "changing environment",
        "physical environment",
        "other researchers",
        "important aspects",
        "alternative view",
        "con- text",
        "social context",
        "following definition",
        "user location",
        "user input",
        "relevant contexts",
        "spatial context",
        "user environment",
        "pervasive",
        "section",
        "scope",
        "different",
        "perspectives",
        "Schilit",
        "Brown",
        "temperature",
        "Simi",
        "identity",
        "account",
        "Ryan",
        "synonyms",
        "computer",
        "Franklin",
        "Ward",
        "state",
        "Hull",
        "entire",
        "settings",
        "Rodden",
        "resources",
        "connectivity",
        "display",
        "costs",
        "examples",
        "Dey",
        "survey",
        "person",
        "place",
        "interaction",
        "influence",
        "decisions",
        "everything",
        "lives",
        "Mobile",
        "personalized mobile game recommendation system",
        "vidual mobile phone user behavior",
        "mobile notification management systems",
        "relevant contextual informa- tion",
        "related contextual infor- mation",
        "various smart phone applications",
        "individual mobile phone users",
        "personalized context-aware recommendation",
        "user mobile web navigation",
        "smart mobile applications",
        "smart mobile phone",
        "phone usage behavior",
        "standardized communications rules",
        "International Telecommunication Union",
        "sive commercial industry",
        "accurate context-aware predictions",
        "text communication service",
        "contextual behavioral patterns",
        "automatic spam filtering",
        "smartphone notification logs",
        "phone call activities",
        "short text messages",
        "contextual usage patterns",
        "short message service",
        "spam text messages",
        "contextual patterns",
        "unique phone",
        "individual users",
        "User navigation",
        "short messages",
        "game log",
        "voice communication",
        "major activities",
        "spam messages",
        "various types",
        "various kinds",
        "various real",
        "web log",
        "web searching",
        "particular user",
        "meta data",
        "log data",
        "outgoing calls",
        "incoming calls",
        "other dimensions",
        "day situation",
        "social relationship",
        "numerous growth",
        "dramatic increasing",
        "good time",
        "bad time",
        "rapid development",
        "smartphone apps",
        "Such logs",
        "social networks",
        "predictive suggestions",
        "portal structure",
        "computing capabilities",
        "life-logging device",
        "technical sense",
        "context data",
        "tual information",
        "context source",
        "SMS log",
        "real world",
        "life purposes",
        "contact number",
        "ability",
        "caller",
        "callee",
        "exchange",
        "protocols",
        "81 billion",
        "task",
        "Multimedia",
        "Facebook",
        "Gmail",
        "Youtube",
        "Skype",
        "notifications",
        "events",
        "news",
        "reminders",
        "alerts",
        "games",
        "promotional",
        "emails",
        "Twitter",
        "LinkedIN",
        "WhatsApp",
        "Viver",
        "intelligent",
        "entertainment",
        "chat",
        "misc",
        "TV",
        "netting",
        "travel",
        "sport",
        "banking",
        "needs",
        "action",
        "adventure",
        "puzzle",
        "RPG",
        "strategy",
        "ubiquity",
        "opportunity",
        "memories",
        "Context‑aware rule learning strategies",
        "Time interval type Number",
        "temporal context based rules",
        "existing time-series segmentation approaches",
        "time‑series smartphone data",
        "mobile phone data",
        "Google play API",
        "geographical loca- tion",
        "context-aware rule learning",
        "segment details References",
        "personalized log data",
        "user phone calls",
        "smart mobile phones",
        "life log data",
        "mobile user behavior",
        "behavioral data clusters",
        "time-based behavior modeling",
        "A time series",
        "related contextual information",
        "static time segments",
        "existing rule",
        "existing strategies",
        "important context",
        "contextual rule",
        "learning rules",
        "IoT data",
        "data points",
        "temporal, spatial",
        "behavioral rules",
        "exact time",
        "user experience",
        "surrounding environment",
        "core components",
        "life- logs",
        "SMS headers",
        "App use",
        "physical activities",
        "Bluetooth devices",
        "open challenges",
        "diverse activities",
        "effective modeling",
        "open problem",
        "Various types",
        "different applications",
        "remaining hours",
        "late morning",
        "Equal 3 Morning",
        "Equal 4 Morning",
        "Unequal 5 Morning",
        "Equal 4 Night",
        "6:00 a",
        "sensors",
        "content",
        "sapp",
        "WiFi",
        "proximity",
        "opportunities",
        "future",
        "directions",
        "discovery",
        "behaviors",
        "sequence",
        "research",
        "Table",
        "afternoon",
        "Song",
        "Rawassizadeh",
        "Mukherji",
        "Bayir",
        "Paireekreng",
        "day",
        "Jayarajah",
        "Do",
        "Xu",
        "Mehrotra",
        "Zhu",
        "forenoon",
        "Oulasvirta",
        "Next",
        "Yu",
        "midnight",
        "0:00",
        "dynamic segmenta- tion technique",
        "high confidence temporal rules",
        "unequal interval based segmentation",
        "high confidence rules",
        "four different time segments",
        "various mobile applications",
        "dynamic segmen- tation",
        "four time slots",
        "mobile phone users",
        "individual smartphone users",
        "same interval length",
        "two broad categories",
        "users’ behavioral activities",
        "dynamic segmentation techniques",
        "Such static segmentation",
        "various time intervals",
        "unequal time interval",
        "time segments decreases",
        "temporal coverage",
        "different lengths",
        "temporal contexts",
        "arbitrary categories",
        "two types",
        "behavioral characteristics",
        "corresponding segmentation",
        "population behavior",
        "ioral evidence",
        "usage records",
        "seg- ments",
        "single parameter",
        "division result",
        "base period",
        "time period",
        "static generation",
        "meaningful results",
        "variable number",
        "one case",
        "N1 number",
        "N2 number",
        "Naboulsi",
        "Dashdorj",
        "Shin",
        "11:00 a",
        "S5",
        "S8",
        "Farrahi",
        "ii",
        "researchers",
        "periods",
        "1 h",
        "works",
        "approaches",
        "goal",
        "change",
        "patterns",
        "authors",
        "Tmax",
        "24-h",
        "BP",
        "5 min",
        "2:00",
        "5:00",
        "7:00",
        "9:00",
        "important machine learning tech- niques",
        "time- series data Lu",
        "Base technique Description References",
        "time-series log data Halvey",
        "data shape based approaches",
        "squared error-based clustering algorithm",
        "time-series data analysis",
        "time-series data Hartono",
        "time-series data Das",
        "similar K-medoids method",
        "cluster- ing algorithms",
        "mean value calculation",
        "temporal context Khail",
        "different initial points",
        "base time period",
        "dynamic time segments",
        "Various calendar schedules",
        "corresponding time boundaries",
        "user behavior patterns",
        "large time segments",
        "large value",
        "initial partitions",
        "different value",
        "time segmentation",
        "time interval",
        "different contexts",
        "predefined value",
        "clustering algorithms",
        "finest granularity",
        "segments decreases",
        "iable length",
        "particular event",
        "personal calendars",
        "multiple thresholds",
        "sliding window",
        "several applications",
        "specific application",
        "convergence centroids",
        "segments Ozer",
        "segmentation Do",
        "upper boundary",
        "particular segment",
        "genetic algorithm",
        "users’ behavior",
        "users’ preferences",
        "one user",
        "users’ behave",
        "Single parameter",
        "K-means algorithm",
        "predefined number",
        "individual",
        "assumptions",
        "problems",
        "sense",
        "clusters",
        "K.",
        "outliers",
        "30 min",
        "Karatzoglou",
        "small",
        "Phithakkitnukoon",
        "Dekel",
        "Zulkernain",
        "Seo",
        "Multi-thresholds",
        "lower",
        "Zhang",
        "Shokoohi",
        "Keogh",
        "Kandasamy",
        "characteristic",
        "reason",
        "1",
        "2:30",
        "single linkage agglomerative clustering algorithm",
        "bottom-up hierarchical data processing",
        "popular agglomerative clustering",
        "other characteristic mismatches",
        "device clustering method",
        "Existing hierarchical algorithms",
        "agglomerative methods",
        "complete linkage",
        "device methods",
        "other clusters",
        "time segments",
        "behavioral clusters",
        "nearest neighbor",
        "proximity matrix",
        "new cluster",
        "matrix value",
        "outlier problem",
        "presence",
        "medoid",
        "K-means",
        "size",
        "user",
        "practice",
        "simplest",
        "distance",
        "sivel"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 1.4265128,
      "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2469691,
      "metadata_storage_name": "s41109-020-00330-x.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MTEwOS0wMjAtMDAzMzAteC5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": " Shun Kodate ",
      "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network",
      "metadata_creation_date": "2020-11-12T15:20:34Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Computational social science Open Access",
        "other third party material",
        "eight local network indices",
        "various online transaction platforms",
        "Applied Network Science",
        "Creative Commons licence",
        "random forest classifiers",
        "Appl Netw Sci",
        "eight network indices",
        "present descriptive analysis",
        "credit card fraud",
        "Network analysis",
        "present study",
        "problematic transaction",
        "appropriate credit",
        "credit line",
        "online marketplaces",
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "rapid growth",
        "electronic transactions",
        "communi- cations",
        "dramatic speed",
        "daily lives",
        "UK Parliament",
        "recent era",
        "money laundering",
        "computer intrusion",
        "illegal items",
        "fictive items",
        "typical approach",
        "individual transactions",
        "traditional approach",
        "corresponding buyer",
        "several hundreds",
        "similar number",
        "focal node",
        "twelve features",
        "four types",
        "classification performance",
        "Machine learning",
        "author(s",
        "statutory regulation",
        "copyright holder",
        "iveco mmons",
        "RESEARCH Kodate",
        "Full list",
        "malicious users",
        "fraudulent users",
        "normal users",
        "online consumer",
        "intended use",
        "permitted use",
        "doi.org",
        "orcid.org",
        "Fraud detection",
        "consumer marketplace",
        "egocentric networks",
        "author information",
        "user profiles",
        "fraud activity",
        "Introduction",
        "tandem",
        "cybercrimes",
        "billions",
        "dollars",
        "year",
        "security",
        "society",
        "McAfee",
        "system",
        "Anderson",
        "dimension",
        "ranges",
        "plagiarism",
        "Abstract",
        "Providers",
        "behavior",
        "texts",
        "background",
        "frauds",
        "seller",
        "edge",
        "up",
        "connectivity",
        "neighbors",
        "aim",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "original",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "naokimas",
        "buffalo",
        "4 Department",
        "Mathematics",
        "University",
        "USA",
        "creativecommons",
        "licenses",
        "crossmark",
        "dialog",
        "Page",
        "18Kodate",
        "13 million monthly active users",
        "credit card systems",
        "social security system",
        "various other fields",
        "online review forums",
        "large online consumer",
        "machine learning algorithms",
        "online social networks",
        "mobile phone network",
        "credit card transactions",
        "suspicious connectivity patterns",
        "online auction system",
        "fraud detec- tion",
        "fraud detection techniques",
        "online C2C marketplaces",
        "online auctions",
        "supervised learning",
        "telecommunica- tion",
        "transaction networks",
        "C2C) marketplace",
        "Standard practice",
        "transaction amount",
        "item category",
        "call duration",
        "call type",
        "geographical region",
        "transaction history",
        "advanced fraudsters",
        "Google LLC",
        "particular words",
        "anomalous behavior",
        "alternative way",
        "graph-theoretic quantities",
        "Dreżewski",
        "car- booking",
        "Van Vlasselaer",
        "cryptocurrency transaction",
        "accom- plices",
        "bipartite cores",
        "belief propagation",
        "133 billion yen",
        "1.2 billion USD",
        "quarter year",
        "previous studies",
        "anomalous users",
        "fraudulent samples",
        "health-care data",
        "empirical data",
        "data set",
        "statistical methods",
        "transaction frauds",
        "reputation frauds",
        "statistical classifier",
        "one jargon",
        "Exemplar features",
        "Computational",
        "decades",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "case",
        "day",
        "week",
        "address",
        "number",
        "calls",
        "age",
        "gender",
        "Akoglu",
        "eyes",
        "administrators",
        "authorities",
        "signature",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "example",
        "authority",
        "drug",
        "idea",
        "nodes",
        "goods",
        "scores",
        "expectation",
        "insurance",
        "Šubelj",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "advertising",
        "Ferrara",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Monamo",
        "reputations",
        "authors",
        "eBay",
        "Mercari",
        "Japan",
        "Many prior network-based fraud detection algorithms",
        "online C2C marketplace service",
        "one problematic sell",
        "local clustering coefficient",
        "local infor- mation",
        "two fraudulent users",
        "two normal users",
        "two indices",
        "sell probability",
        "local indices",
        "two users",
        "local features",
        "connected components",
        "Yan- chun",
        "commercial implementations",
        "various items",
        "United States",
        "Japanese market",
        "following types",
        "non-existing items",
        "medicinal supplies",
        "weighted network",
        "egocentric network",
        "eight indices",
        "trans- actions",
        "separate index",
        "three indices",
        "informa- tion",
        "users’ network",
        "fictive item",
        "node vi",
        "node strength",
        "methods Data",
        "normal transactions",
        "lematic transactions",
        "mean number",
        "focal user",
        "Fictive transactions",
        "directed edge",
        "networks",
        "global",
        "communities",
        "betweenness",
        "k-cores",
        "Bhat",
        "Savage",
        "Others",
        "degree",
        "triangles",
        "Materials",
        "July",
        "January",
        "addition",
        "underwear",
        "medicine",
        "weapon",
        "perspective",
        "morality",
        "hygiene",
        "law",
        "crime",
        "Table",
        "buyer",
        "Figure",
        "1a",
        "Fig.",
        "edges",
        "pairs",
        "information",
        "Barrat",
        "direction",
        "SPi",
        "local clustering coefficient Ci ranges",
        "Random forest classifier",
        "unweighted trian- gles",
        "other three indices",
        "one feedforward triangle",
        "one cyclic triangle",
        "local network",
        "one item",
        "triangle congregation",
        "other four",
        "k ini",
        "weighted version",
        "overlapping set",
        "three-node subnetworks",
        "work motifs",
        "natural interpretation",
        "three nodes",
        "unweighted triangles",
        "two triangles",
        "feedforward triangles",
        "cyclic triangles",
        "many users",
        "different directed",
        "particular neighbors",
        "Such neighbors",
        "bidirectional edges",
        "ki(ki",
        "Fig. 2c",
        "degree zero",
        "proportion",
        "kouti",
        "Examples",
        "vj",
        "contribution",
        "sini",
        "buys",
        "souti",
        "sells",
        "abundance",
        "undirected",
        "Newman",
        "increase",
        "community",
        "Radicchi",
        "Palla",
        "hypothesis",
        "extent",
        "concept",
        "mi",
        "Ciki",
        "Note",
        "Frequencies",
        "marketplace",
        "contrast",
        "index",
        "cycle",
        "CYPi",
        "CYi",
        "definition",
        "calculation",
        "weights",
        "Breiman",
        "The precision–recall (PR) curve",
        "random forest clas- sifier",
        "random forest classifier",
        "ensemble learning method",
        "classifica- tion performance",
        "receiver operating characteristic",
        "two performance measures",
        "true positive rate",
        "false positive rate",
        "piecewise linear manner",
        "Directed triangle patterns",
        "same test sample",
        "tive training samples",
        "ROC) curve",
        "ROC curve",
        "two sets",
        "three-node patterns",
        "directed triangles",
        "same order",
        "single-tree positive",
        "Cyclic triangle",
        "tive samples",
        "precision range",
        "test data",
        "positive probability",
        "multiple classifiers",
        "decision-tree classifiers",
        "best split",
        "reciprocal edges",
        "fraudulent user",
        "fraudulent types",
        "descending order",
        "horizontal axis",
        "vertical axis",
        "test samples",
        "training data",
        "fraudulent) samples",
        "terminal node",
        "negative probability",
        "candidate features",
        "Feedforward triangle",
        "decision trees",
        "300 trees",
        "Hastie",
        "scikit-learn",
        "Pedregosa",
        "overfitting",
        "basis",
        "replace",
        "ment",
        "fraction",
        "average",
        "count",
        "Five",
        "numbers",
        "figure",
        "2",
        "100 random forest classifiers",
        "accurate binary classifier",
        "random number generator",
        "ten different permutations",
        "ten permutations",
        "different types",
        "PR curves",
        "grid search",
        "10-fold cross-validation",
        "maximum depth",
        "max_depth parameter",
        "other hyperparameters",
        "default values",
        "parameter optimization",
        "sampling bias",
        "sampling scheme",
        "Descriptive statistics",
        "survival probability",
        "frac- tion",
        "one transaction",
        "clear distinction",
        "large value",
        "The AUC",
        "lent users",
        "mal users",
        "minimum number",
        "seed number",
        "single set",
        "AUC value",
        "permutation importance",
        "nal nodes",
        "scikit-learn version",
        "useful information",
        "degree distribution",
        "unweighted degree",
        "user type",
        "Fig. 3a",
        "Fig. 3c",
        "training set",
        "performance measure",
        "degree ki",
        "normal",
        "area",
        "good",
        "Strobl",
        "Altmann",
        "method",
        "decrease",
        "tree",
        "integers",
        "split",
        "max_features",
        "n_estimators",
        "Results",
        "specified",
        "classification",
        "difference",
        "many",
        "fact",
        "≥",
        "Normal Fictive Underwear Medicine Weapon",
        "unweighted sell probability",
        "different user types",
        "less frequent transactions",
        "one sell transaction",
        "Seed user type",
        "large fraction",
        "smaller value",
        "characteristic behavior",
        "first column",
        "exclusive buyer",
        "fraudulent type",
        "seed users",
        "sell transactions",
        "Fig. 4a",
        "Fig.  5a",
        "c Strength",
        "specific neighbor",
        "average number",
        "Total number",
        "Table 1 Properties",
        "conditional mean",
        "Degree",
        "ki",
        "majority",
        "result",
        "ence",
        "median",
        "FFi",
        "samples",
        "peak",
        "smallest possible weighted sell probability",
        "smallest possible out-degree",
        "corresponding frac- tions",
        "tinct frequency distributions",
        "Survival probability",
        "cycle probability",
        "Figure  5b",
        "dashed lines",
        "Figure 5b",
        "two aspects",
        "Figure  6b",
        "larger value",
        "clear difference",
        "descriptive statistics",
        "ous section",
        "horizontal line",
        "second feature",
        "Fig.  6a",
        "Fig. 8a",
        "relationships",
        "ini",
        "buyers",
        "one",
        "transactions",
        "neighbor",
        "results",
        "analysis",
        "tendency",
        "Classification",
        "12 features",
        "unweighted sell probability SPi",
        "Fig. 8 Cycle probability",
        "Fig. 7 Triangle congregation",
        "b Survival probability",
        "third feature",
        "real number",
        "fourth feature",
        "fifth feature",
        "sixth feature",
        "seventh feature",
        "eighth feature",
        "c Relationship",
        "WSPi"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 1.3131375,
      "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1373874,
      "metadata_storage_name": "s13673-019-0177-6.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTktMDE3Ny02LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Chunyong Yin ",
      "metadata_title": "Mobile marketing recommendation method based on user location feedback",
      "metadata_creation_date": "2019-04-05T10:16:31Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "current mobile marketing recommendation system",
        "Sequential behavior Open Access",
        "Mobile marketing recommendation method",
        "users’ timing preference characteristics",
        "Location-based mobile marketing recommendation",
        "potential mobile user preferences",
        "Creative Commons license",
        "convolutional neural network model",
        "users’ location feedback behavior",
        "user location feedback data",
        "mobile recommendation system",
        "traditional recommendation models",
        "user preference information",
        "creat iveco mmons",
        "users’ location-based behaviors",
        "mobile information overload",
        "user-product binary matrix",
        "original author(s",
        "different geographical locations",
        "shopping location information",
        "famous e-commerce platforms",
        "physical store products",
        "different time windows",
        "users’ location information",
        "Hum. Cent. Comput",
        "mobile network",
        "information recommendation",
        "different preferences",
        "mobile users",
        "convolutional model",
        "author information",
        "irrelevant information",
        "binary relationships",
        "different dimensions",
        "behavioral data",
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "recent years",
        "e-commerce industry",
        "one hand",
        "various products",
        "other hand",
        "large amount",
        "next consumer",
        "recommended methods",
        "third factor",
        "limited check",
        "timeli- ness",
        "hot spots",
        "accuracy rate",
        "recall rate",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Yin",
        "Inf. Sci.",
        "Communication Engineering",
        "Full list",
        "individual needs",
        "doi.org",
        "orcid.org",
        "experimental results",
        "Changsha University",
        "Introduction",
        "popularization",
        "Internet",
        "Alibaba",
        "zon",
        "home",
        "sellers",
        "goods",
        "costs",
        "wealth",
        "consumption",
        "case",
        "advantage",
        "terms",
        "Abstract",
        "attribute",
        "role",
        "paper",
        "LBCNN",
        "extractor",
        "classifier",
        "Keywords",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "jinwang",
        "csust",
        "2 School",
        "Computer",
        "Science",
        "Technology",
        "China",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "17Yin",
        "means",
        "problem",
        "implied feature-based cognitive feature collaborative filter- ing",
        "location-aware based generation probability model",
        "collaborative filtering methods",
        "biological neural networks",
        "Convolutional neural networks",
        "advanced abstract features",
        "loca- tion information",
        "accurate personalized recommendations",
        "small-scale training data",
        "detailed comparative analysis",
        "user informa- tion",
        "mobile marketing recommendation",
        "original input features",
        "business location information",
        "user preference model",
        "mobile recommendation research",
        "feature size",
        "deep learning algorithms",
        "mobile users’ preferences",
        "ing models",
        "deep model",
        "DL) model",
        "shallow model",
        "multimedia recommendations",
        "mobile environments",
        "accurate prediction",
        "local features",
        "overall features",
        "two-dimensional features",
        "user preferences",
        "location recommendation",
        "convolutional layer",
        "spatial information",
        "product information",
        "user information",
        "hot topic",
        "service resources",
        "different assumptions",
        "spatial activities",
        "location movement",
        "negative samples",
        "semantic content",
        "data size",
        "mation analysis",
        "long-term interest",
        "time stamps",
        "great achievements",
        "speech tasks",
        "novel field",
        "interventional optimization",
        "artificial intelligence",
        "great breakthroughs",
        "similar effects",
        "Weight sharing",
        "great advantages",
        "user needs",
        "positional relevance",
        "location relevance",
        "first layer",
        "previous layer",
        "convolution kernel",
        "pooling layer",
        "context information",
        "ventional methods",
        "user models",
        "many researches",
        "two layers",
        "network structure",
        "shortcomings",
        "Zhu",
        "retrograde",
        "approach",
        "Yin",
        "LA-LDA",
        "scoring",
        "items",
        "studies",
        "distance",
        "merchant",
        "area",
        "proximity",
        "importance",
        "gap",
        "core",
        "Lian",
        "impact",
        "author",
        "project",
        "Lee",
        "visual",
        "study",
        "aspects",
        "machine",
        "Experiments",
        "characteristics",
        "CNN",
        "difficulty",
        "number",
        "weights",
        "neuron",
        "Hum",
        "Cent",
        "Comput",
        "images",
        "location-based",
        "Location-based mobile marketing recommendation model",
        "Content‑based recommendation method",
        "convolutional neural network models",
        "Representative content-based recommendation systems",
        "general products recommendation system",
        "Traditional recommendation method",
        "traditional recommendation algorithm",
        "content-based recommendation algorithm",
        "similar Top-N products",
        "timing preference characteristics",
        "interest feature vector",
        "Content-based information filtering",
        "Experimental analysis” section",
        "item feature vector",
        "project feature vector",
        "other existing methods",
        "cosine similarity algorithm",
        "Related work” section",
        "user feature vector",
        "users’ timing preferences",
        "User description file",
        "feature models",
        "Content-based methods",
        "content filtering",
        "content characteristics",
        "other methods",
        "user-product information",
        "collaborative filtering",
        "feedback information",
        "hybrid methods",
        "historical content",
        "training sample",
        "next moment",
        "time window",
        "overall preferences",
        "Top-K sample",
        "four sections",
        "Necessary definitions",
        "specific implementation",
        "future progress",
        "current chapter",
        "three parts",
        "time series",
        "text documents",
        "accurate comparisons",
        "different texts",
        "lowing aspects",
        "Hum. Cent",
        "project profile",
        "user u",
        "similarity calculation",
        "sample features",
        "cast results",
        "effective application",
        "calculation formula",
        "two-class problem",
        "purchase behavior",
        "category",
        "order",
        "merchandise",
        "length",
        "test",
        "Remain",
        "sion",
        "strengths",
        "weaknesses",
        "plans",
        "establishment",
        "differences",
        "research",
        "threshold",
        "a.",
        "topics",
        "projects",
        "purchasing",
        "operation",
        "addition",
        "modulus",
        "Lops",
        "Gemmis",
        "Semeraro",
        "content filtering based recommendation method",
        "m * n matrix R",
        "traditional data mining method",
        "user historical behavior data",
        "traditional collaborative filtering method",
        "likely N objects",
        "user behavior data",
        "various draw- backs",
        "nearest neigh- bor",
        "conven- tional method",
        "association rule algorithm",
        "higher evaluation algorithms",
        "hybrid recommendation algorithm",
        "similar user sets",
        "mutual user relationships",
        "content-based recommendation method",
        "Hybrid recommendation method",
        "other similar users",
        "different recommendation algorithms",
        "The matrix  Umn",
        "content-based filtering",
        "other method",
        "hybrid method",
        "matrix decomposition",
        "different relationships",
        "One method",
        "new content",
        "evaluation standard",
        "association rules",
        "recommendation problem",
        "standard recommendation",
        "two algorithms",
        "different ways",
        "target user",
        "user neighbor",
        "cold-start issues",
        "same type",
        "same partition",
        "expert research",
        "three steps",
        "recommendation results",
        "ommended items",
        "normal methods",
        "supervised learning",
        "online stores",
        "informa- tion",
        "new users",
        "two types",
        "two specific",
        "compensa- tion",
        "different values",
        "common features",
        "rating sparsity",
        "erence information",
        "new projects",
        "mendations",
        "availability",
        "context",
        "party",
        "requirements",
        "merits",
        "quality",
        "memory",
        "following",
        "collection",
        "feedback",
        "range",
        "U11",
        "U1n",
        "U2n",
        "Um1",
        "classification",
        "survey",
        "Amazon",
        "personalized",
        "customer",
        "ratings",
        "techniques",
        "performance",
        "ally",
        "degree",
        "improvement",
        "complexity",
        "different timing behavior characteristics",
        "good recom- mendations",
        "relevant professional research",
        "T-time consumer behavior",
        "behavioral counting feature",
        "duplication counting feature",
        "feature statistics window",
        "historical behavior data",
        "timing behavior data",
        "user position feedback",
        "similar behavioral items",
        "collaborative filtering algorithm",
        "next purchase behavior",
        "feature statistics method",
        "user behavior record",
        "time series behavior",
        "timing recommendation model",
        "user interface layer",
        "U, V data",
        "high quality rules",
        "last behavior",
        "behavior count",
        "Different actions",
        "different times",
        "U data",
        "next section",
        "current window",
        "feature group",
        "product purchase",
        "recommendation process",
        "recommendation systems",
        "ready-made data",
        "data sets",
        "many years",
        "core idea",
        "rule algorithms",
        "total project",
        "disjoint sets",
        "association rule",
        "two criteria",
        "transac- tions",
        "key- word",
        "description layer",
        "main function",
        "strong dependence",
        "most systems",
        "time information",
        "time label",
        "Markov chain",
        "full use",
        "prediction problem",
        "time T",
        "three groups",
        "processing manner",
        "cumulative measure",
        "keyword layer",
        "user layer",
        "unknown items",
        "traditional algorithm",
        "preference features",
        "corresponding features",
        "location visit",
        "total number",
        "∩V",
        "V.",
        "∪ V",
        "business",
        "effect",
        "quantity",
        "focus",
        "transactions",
        "statement",
        "strength",
        "support",
        "confidence",
        "ratio",
        "Formula",
        "interest",
        "other",
        "TOP-N",
        "presentation",
        "attributes",
        "dependencies",
        "keywords",
        "resource",
        "users",
        "dataset",
        "explanation",
        "Construction",
        "Fig.",
        "example",
        "user1",
        "Table",
        "False",
        "behaviors",
        "Location‑based mobile marketing recommendation model",
        "Counting feature Mean feature Ratio feature",
        "location-based mobile marketing recom",
        "Table 1 Characteristic system diagram",
        "collaborative filtering calculation process",
        "False True User feature",
        "K user preference feature",
        "pooled feature vector",
        "evaluation crite- rion",
        "ior preference feature",
        "duplication count feature",
        "different feature maps",
        "repetitive behavioral data",
        "Specific implementa- tion",
        "pool- ing layer",
        "multi-window convolutional layer",
        "real-world work applications",
        "location visit behavior",
        "N time windows",
        "multi-window convolution layer",
        "user characteristics group",
        "products’ total visit",
        "user position features",
        "counting features",
        "Feature group",
        "True False",
        "feature expression",
        "product location",
        "behavioral count",
        "network structures",
        "input feature",
        "Model framework",
        "different lengths",
        "Product feature",
        "mean-type features",
        "output layer",
        "time axis",
        "input layer",
        "total behavior",
        "current research",
        "low difficulty",
        "mendation quality",
        "following sections",
        "important aspects",
        "next step",
        "evant definitions",
        "timing sensitivity",
        "timing behav",
        "four layers",
        "two-dimensional plane",
        "ture plane",
        "The model",
        "Specific information",
        "average number",
        "user-product behavior",
        "product preferences",
        "relevant definition",
        "K.",
        "Definition 1",
        "Definition 2",
        "visits",
        "activity",
        "popularity",
        "method",
        "usable",
        "achievement",
        "speed",
        "avgui",
        "Ij",
        "True/False",
        "sure",
        "correctness",
        "basis",
        "above",
        "analysis",
        "eigenvector",
        "commodity",
        "matrix",
        "new location-based marketing resources recommendation",
        "location-based mobile marketing resources",
        "M convolution kernels",
        "convolu- tion kernel",
        "gradient descent method",
        "convolution kernel w",
        "C × M matrix",
        "nonlinear activation function",
        "kth implicit class",
        "implicit semantic model",
        "maximum pooling operation",
        "latent factor vector",
        "training data set",
        "latent factor model",
        "traditional CNN parameters",
        "likelihood probability value",
        "i-th feature fi",
        "kth product",
        "convolutional kernel",
        "training method",
        "Dropout method",
        "CNN inputs",
        "i-th category",
        "loss function",
        "i-th sample",
        "Feature map",
        "pooled feature",
        "training results",
        "window length",
        "offset term",
        "maximum features",
        "C categories",
        "M-dimensional vector",
        "k-th offset",
        "real category",
        "two items",
        "same time",
        "same class",
        "pTu qi",
        "hidden classes",
        "Specific implementation",
        "two processes",
        "two parts",
        "top module",
        "historical data",
        "other module",
        "second pro",
        "traditional LFM",
        "L2-norm regularization",
        "smoothing problems",
        "training process",
        "dation process",
        "model parameters",
        "Probability distribution",
        "real number",
        "weight parameter",
        "many users",
        "user interest",
        "∑C",
        "j.",
        "framework",
        "ReLu",
        "Tanh",
        "Definition",
        "network",
        "fk",
        "bk",
        "teristic",
        "Zeiler",
        "fitting",
        "neurons",
        "behavior",
        "high",
        "Pool_feature",
        "log",
        "rui",
        "relationship",
        "outputs",
        "excessive",
        "σ",
        "θ",
        "existing authoritative standard training set",
        "Gradient Boosting Regression Tree Model",
        "mining users’ temporal behavior characteristics",
        "Linear Logistic Regression Classification Model",
        "offline training model phase",
        "location-based marketing resources",
        "convolutional neural network",
        "hyper parameter settings",
        "real-time recommendation stage",
        "LBCNN model structure",
        "Random Forest Model",
        "past behavior data",
        "matrix decomposition method",
        "network model parameters",
        "LFM training data",
        "user bias item",
        "item offset item",
        "regression analysis",
        "linear model",
        "classification models",
        "model training",
        "training section",
        "training features",
        "behavior information",
        "previous data",
        "language model",
        "same model",
        "real-time performance",
        "LFM results",
        "sion coefficient",
        "specification results",
        "next question",
        "two parameters",
        "intrinsic property",
        "positive samples",
        "negative sample",
        "first thing",
        "historical score",
        "implied tag",
        "Experimental analysis",
        "timing preferences",
        "same conditions",
        "Vector Machine",
        "Experimental tool",
        "sklearn kit",
        "L2 regular",
        "regularization term",
        "regularization coefficient",
        "user tag",
        "rating score",
        "User information",
        "kind",
        "thinking",
        "sparseness",
        "analyses",
        "solution",
        "premise",
        "mula",
        "calculation",
        "puk",
        "average",
        "qik",
        "public",
        "experience",
        "latter",
        "overfitting",
        "Description",
        "input",
        "problems",
        "output",
        "L1-norm",
        "part",
        "update",
        "background",
        "people",
        "products",
        "advance",
        "period",
        "advantages",
        "several",
        "LR",
        "Support",
        "SVM",
        "GBDT",
        "8 h",
        "λ",
        "∑",
        "lion users’ various behaviors",
        "mobile recommendation algorithm contest",
        "radial basis kernel function",
        "personalized recommendation model",
        "feature segmentation standard",
        "random feature ratio",
        "real business sce",
        "product category information",
        "data set",
        "Behavior types",
        "behavior record",
        "behavior time",
        "c. RF",
        "learning rate",
        "maximum depth",
        "Alibaba Group",
        "offline type",
        "RBF",
        "gamma",
        "trees",
        "entropy",
        "experiment",
        "1 month",
        "2,876,947 items",
        "clicks",
        "shopping",
        "carts",
        "purchases",
        "hour",
        "online",
        "nario",
        "subset"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 0.99738735,
      "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2424684,
      "metadata_storage_name": "s40064-015-1515-4.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDA2NC0wMTUtMTUxNS00LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Rik Das",
      "metadata_title": "Multi technique amalgamation for enhanced information identification with content based image data",
      "metadata_creation_date": "2015-11-26T05:08:05Z",
      "keyphrases": [
        "multi tech- nique feature extraction process",
        "content based image data Rik Das1",
        "Creative Commons Attribution 4.0 International License",
        "traditional text based annotation technique",
        "Dr. Camil Bulcke Path",
        "customary text based process",
        "Diverse low level features",
        "content based feature extraction",
        "content based image identification",
        "Creative Commons license",
        "robust feature vectors",
        "Multi technique amalgamation",
        "low cost storage",
        "digital photo-capture devices",
        "four public datasets",
        "test Open Access",
        "P.O. Box",
        "digital image acquisition",
        "huge image repositories",
        "gigantic image datasets",
        "image capturing devices",
        "new research challenges",
        "OT Scene) Dataset",
        "three different techniques",
        "exist- ing techniques",
        "original author(s",
        "Semantically analogous images",
        "image identification technique",
        "enhanced information identification",
        "manual process",
        "image content",
        "Diverse applications",
        "image recognition",
        "image binarization",
        "image transform",
        "Image classification",
        "Image retrieval",
        "Raventós",
        "RESEARCH Das",
        "image information",
        "author information",
        "Corel Dataset",
        "Caltech Dataset",
        "Conventional techniques",
        "art techniques",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Recent years",
        "computer power",
        "accessible internet",
        "Efficient indexing",
        "computer vision",
        "machine learning",
        "Automatic derivation",
        "severe limitations",
        "aforesaid limitations",
        "resourceful foundation",
        "social media",
        "morphological operator",
        "Oliva Torralba",
        "recognition rate",
        "Classification result",
        "average increase",
        "retrieval result",
        "Slant transform",
        "unrestricted use",
        "appropriate credit",
        "Xavier Institute",
        "Social Service",
        "Purulia Road",
        "Full list",
        "effective alternative",
        "meaningful information",
        "Information Technology",
        "Wang Dataset",
        "decision fusion",
        "Background",
        "ubiquity",
        "mass",
        "popularity",
        "Madireddy",
        "Walia",
        "words",
        "mapping",
        "perception",
        "vocabulary",
        "person",
        "nature",
        "Abstract",
        "proliferation",
        "areas",
        "biomedicine",
        "military",
        "commerce",
        "education",
        "means",
        "success",
        "paper",
        "Precision",
        "state",
        "Otsu",
        "threshold",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "SpringerPlus",
        "DOI",
        "Correspondence",
        "1 Department",
        "Ranchi",
        "Jharkhand",
        "India",
        "end",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "26Das",
        "competence",
        "color",
        "content based image recog",
        "binarization based feature extraction",
        "content based image classification",
        "feature extraction Feature extraction",
        "Z score normalization",
        "orthogonal unitary matrices",
        "Appropriate threshold selection",
        "fusion based method",
        "single feature extrac",
        "fractional energy coefficient",
        "fifteen fractional coefficients",
        "high dimensional data",
        "main contribu- tion",
        "multi-technique feature extraction",
        "efficient feature extraction",
        "feature extrac- tion",
        "efficient image binarization",
        "feature vector dimension",
        "image trans- formation",
        "multi technique fusion",
        "image binariza- tion",
        "Image binarization techniques",
        "feature vectors",
        "feature spaces",
        "image data",
        "energy spectrum",
        "subband coefficients",
        "novel techniques",
        "Diverse techniques",
        "image identification",
        "image elements",
        "image patterns",
        "seven image",
        "recognition decision",
        "hybrid architecture",
        "fusion architecture",
        "Statistical validation",
        "trans- forms",
        "morphological operators",
        "archi- tecture",
        "four subsections",
        "earlier works",
        "four topics",
        "basis functions",
        "one representation",
        "two aspects",
        "critical components",
        "compact structure",
        "cient storage",
        "radical reduction",
        "multiple scales",
        "tropic transform",
        "various factors",
        "uneven illumination",
        "research objectives",
        "research results",
        "transformation process",
        "aforesaid properties",
        "basis images",
        "Original images",
        "shape",
        "texture",
        "number",
        "features",
        "morphology",
        "Comparison",
        "retrieval",
        "correlation",
        "connection",
        "contemporary",
        "Change",
        "domain",
        "set",
        "series",
        "Annadurai",
        "Shanmugalakshmi",
        "advantages",
        "waveforms",
        "use",
        "analysis",
        "transmission",
        "Kekre",
        "Thepade",
        "subbands",
        "Prakash",
        "Luo",
        "execution",
        "inadequate",
        "Ramírez- Ortegón",
        "Gradient vector flow fields",
        "multi technique feature extraction",
        "fusion based image identification",
        "superior prediction accuracy",
        "feature level correlations",
        "image signature extraction",
        "feature extraction technique",
        "Popular contour-based descriptors",
        "Enhanced classification results",
        "curvature scale space",
        "feature selection technique",
        "region- based descriptors",
        "3 D color histogram",
        "diverse extraction techniques",
        "Two different categorization",
        "global threshold selection",
        "shape feature extraction",
        "multilevel mean threshold",
        "image binarization techniques",
        "local threshold selection",
        "retrieval process complexity",
        "feature vector",
        "identification rate",
        "Image Content",
        "different techniques",
        "shape descriptors",
        "semantic retrieval",
        "Local descriptors",
        "local decisions",
        "shape information",
        "region-based descriptors",
        "information fusion",
        "early fusion",
        "late fusion",
        "hybrid fusion",
        "intermediate fusion",
        "adverse effect",
        "Contemporary literatures",
        "unfavourable influences",
        "binarized images",
        "standard deviation",
        "gray values",
        "Commercial viability",
        "existing literatures",
        "ary lines",
        "Fourier descriptor",
        "chain codes",
        "complex shapes",
        "Information recognition",
        "Recent studies",
        "four classes",
        "higher dimensions",
        "joint model",
        "genetic algorithm",
        "optimum boundaries",
        "numerical intervals",
        "memory con",
        "filter responses",
        "distinct features",
        "concentrated features",
        "multiple features",
        "Fusion methodologies",
        "rate learner",
        "Gabor filters",
        "Color moments",
        "texture features",
        "contrast",
        "computation",
        "Valizadeh",
        "average",
        "advantage",
        "spread",
        "calculation",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Rojas",
        "Shaikh",
        "Use",
        "systems",
        "Flickner",
        "PicToSeek",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Emphasize",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "area",
        "object",
        "Kim",
        "single",
        "input",
        "sepa",
        "combiner",
        "scalability",
        "comparison",
        "mix",
        "Zhu",
        "Shyu",
        "sumption",
        "ElAlami",
        "back propagation neural net- work",
        "fuzzy set theoretic approach",
        "angular radial transform descriptor",
        "artificial neural network",
        "spatial structure descriptors",
        "spatial orientation tree",
        "lesser computational overhead",
        "neural network architecture",
        "correct semantic retrieval",
        "Higher retrieval results",
        "semantic retrieval results",
        "local edge bins",
        "non directional edge",
        "HSV color space",
        "significant point features",
        "Three different techniques",
        "color layout descriptor",
        "greater feature dimension",
        "content-based image retrieval",
        "tex- ture features",
        "edge histogram descriptor",
        "inter-class feature extraction",
        "Gabor texture descriptor",
        "fusion based classifier",
        "invariant color features",
        "invariant moments",
        "retrieval decisions",
        "retrieval performance",
        "retrieval rate",
        "retrieval purpose",
        "color histogram",
        "GIST descriptor",
        "horizontal edge",
        "Feature vectors",
        "popular feature",
        "32 feature maps",
        "feature values",
        "feature size",
        "color co",
        "color motif",
        "color moment",
        "occurrence matrix",
        "Recognition process",
        "image signatures",
        "Multi view",
        "Wavelet packets",
        "Eigen values",
        "sub repository",
        "main image",
        "right neighbourhood",
        "query image",
        "scan pattern",
        "equal weights",
        "Precision values",
        "individual techniques",
        "Six semantics",
        "sub- image",
        "tion techniques",
        "same size",
        "16 average value",
        "other hand",
        "morphological technique",
        "vertical edge",
        "shape features",
        "EHD) features",
        "edge images",
        "45° edge",
        "135° edge",
        "Hiremath",
        "Pujari",
        "Yue",
        "points",
        "similarity",
        "Banerjee",
        "Jalab",
        "increased",
        "Shen",
        "Wu",
        "authors",
        "Irtaza",
        "kind",
        "training",
        "response",
        "intra-class",
        "Rahimi",
        "Moghaddam",
        "CCM",
        "difference",
        "pixels",
        "DBPSP",
        "ANN",
        "Subrahmanyam",
        "MCMCM",
        "colour",
        "CMs",
        "sub-image",
        "unique",
        "Methods",
        "4 scales",
        "8 orientations",
        "Douze",
        "region",
        "Red Component Green Component Blue Component",
        "popular global threshold selection method",
        "local threshold selec- tion method",
        "image feature point extraction",
        "different gray level pixels",
        "four subsec- tions",
        "three color components",
        "necessary image information",
        "squared Euclidian distance",
        "document image binarzation",
        "Conventional BoW model",
        "alloca- tion",
        "feature extraction",
        "level histogram",
        "thresholding method",
        "parametric method",
        "optimal threshold",
        "words model",
        "information losses",
        "fifth subsection",
        "test images",
        "binari- zation",
        "dant details",
        "two classes",
        "foreground pixels",
        "background pixels",
        "class variance",
        "The separation",
        "Comprehensive investigation",
        "class probabilities",
        "Total variance",
        "two terms",
        "erate bag",
        "SIFT algorithm",
        "SIFT descriptors",
        "cluster members",
        "codebook generation",
        "final step",
        "huge computational",
        "BoW generation",
        "class means",
        "tering process",
        "descriptor dimension",
        "classification",
        "techniques",
        "methods",
        "fusion",
        "architecture",
        "following",
        "description",
        "datasets",
        "Fig.",
        "way",
        "combined",
        "sum",
        "variances",
        "Eq.",
        "Eqs.",
        "q1",
        "effect",
        "contributions",
        "Zhao",
        "size",
        "problem",
        "omissions",
        "stability",
        "clustering",
        "codewords",
        "overhead",
        "∑",
        "σ",
        "N*N feature vector Feature Vector Dimension Reduction",
        "Compute binary image maps",
        "N unitary slant matrix",
        "Blue (B) color component",
        "N × N matrix",
        "higher intensity feature vectors",
        "two dimensional slant transform",
        "N × 1 vector",
        "higher intensity group",
        "three different color",
        "discrete Fourier transform",
        "One dimensional transform",
        "lower intensity group",
        "Slant transform matrices",
        "frequency domain information",
        "local threshold value",
        "spatial domain information",
        "efficient algorithm design",
        "insignificant transform coefficients",
        "drastic reduction",
        "smaller dimension",
        "Energy compaction property",
        "unitary transform",
        "spatial information",
        "matrix multiplication",
        "two groups",
        "orthogonal transform",
        "inverse transform",
        "transform operation",
        "grey values",
        "two codewords",
        "large fraction",
        "faster execution",
        "tion operation",
        "computational overhead",
        "energy conservation",
        "signal energy",
        "significant role",
        "Partial Coefficients",
        "sequential transformations",
        "average energy",
        "test image",
        "image features",
        "monochrome image",
        "image quality",
        "image line",
        "components R",
        "tain operations",
        "real components",
        "pixel values",
        "average coding",
        "Transforms",
        "approach",
        "cluster",
        "mean",
        "codebook",
        "Tx",
        "Method",
        "End",
        "xhi",
        "cer",
        "capacity",
        "example",
        "characteristic",
        "Green",
        "8 bits",
        "1 bit",
        "images",
        "24–2 bits",
        "Pratt",
        "less",
        "column",
        "row",
        "forward",
        "Eqs",
        "The energy compaction property",
        "gray scale opening operation",
        "lower intensity feature vector",
        "higher intensity feature vector",
        "entire feature vector set",
        "feature vector extraction process",
        "shorter dis- tance",
        "feature vector formation",
        "highest classification result",
        "Blue color components",
        "top-hat transform technique",
        "feature vector database",
        "significant image information",
        "image transform End",
        "image similarity measures",
        "Higher similarity",
        "database images",
        "working process",
        "Similar process",
        "classification results",
        "slant transform",
        "complete set",
        "partial coefficient",
        "fractional coefficient",
        "small patch",
        "Human perception",
        "shape context",
        "point correspondences",
        "considerable contribution",
        "closing operations",
        "hat transformation",
        "bright peaks",
        "tophat operator",
        "code- book",
        "noteworthy image",
        "original image",
        "visual words",
        "series form",
        "one cluster",
        "codebook size",
        "peak detector",
        "Red, Green",
        "coefficients",
        "2. Peak",
        "algorithm",
        "query",
        "block",
        "12 elements",
        "dimension",
        "variant",
        "Sridhar",
        "Exit",
        "bag",
        "BoW",
        "methodology",
        "clusters",
        "Determination",
        "distance",
        "Dunham",
        "σ   Red Component Green Component Blue Component",
        "Three different distance measures",
        "forward Neural Network Classifier",
        "support vector machine",
        "nearest neigh- bors",
        "city block distance",
        "MSE) distance metric",
        "Canberra Distance measure",
        "fusion based classification",
        "first hidden layer",
        "input feature vectors",
        "mean squared error",
        "Data standardization technique",
        "back propagation technique",
        "supervised learning procedure",
        "multi layer perceptron",
        "database image T",
        "query image Q",
        "color component",
        "SVM) classifier",
        "classifier types",
        "network prediction",
        "Euclidian distance",
        "final distance",
        "classification decision",
        "Multilayer Perceptron",
        "input layer",
        "input features",
        "input units",
        "classified query",
        "clas- sifier",
        "following sections",
        "tophat transform",
        "morphological operation",
        "xloF.V.",
        "common range",
        "Top-Hat operator",
        "weighted sum",
        "appropriate output",
        "sification performance",
        "training tuple",
        "backward direction",
        "normalization process",
        "classified image",
        "higher values",
        "precision values",
        "stdev xhi",
        "qp background",
        "greater effect",
        "individual distances",
        "class majority",
        "similar images",
        "top 20 images",
        "target value",
        "stdev xlo",
        "VF xlo",
        "foreground",
        "xmean",
        "xhimeanxhi",
        "xlomeanxlo",
        "Qi",
        "attributes",
        "possibilities",
        "Dcityblock",
        "Deuclidian",
        "DMSE",
        "distn",
        "disti",
        "weights",
        "interest",
        "Ranking",
        "feed",
        "MLP",
        "Alsmadi",
        "optimization",
        "modifications",
        "The",
        "µ",
        "feature vector dataset",
        "Support vector machine",
        "original training data",
        "Optimal separating hyperplane",
        "two different classes",
        "bias node",
        "subsequent layer",
        "hidden layer",
        "Weighted output",
        "final layer",
        "output layer",
        "forward property",
        "higher dimension",
        "nonlinear mapping",
        "new dimension",
        "appropriate nonlinear",
        "class labels",
        "summation",
        "utes",
        "nodes",
        "half",
        "inputs",
        "processing"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 0.073349304,
      "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data  (2018) 5:33  \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence:   \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n  • K anonymity\n  • L diversity\n  • T closeness\n  • Randomization\n  • Data distribution\n  • Cryptographic techniques\n  • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n  • More number of Mappers and Reducers were used as data volume increased.\n  • Results before and after randomization were significantly different.\n  • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n  • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018   Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Prote ction",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1156098,
      "metadata_storage_name": "s40537-018-0141-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTgtMDE0MS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "P. Ram Mohan Rao ",
      "metadata_title": "Privacy preservation techniques in big data analytics: a survey",
      "metadata_creation_date": "2018-09-20T05:58:23Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "A. P. Siva Kumar3",
        "P. Ram Mohan Rao",
        "third party data analyst",
        "modernistic privacy preservation technique",
        "Privacy preservation Open Access",
        "Creative Commons license",
        "Ram Mohan Rao",
        "S. Murali Krishna2",
        "diverse applica- tions",
        "person specific private",
        "value added services",
        "web site logs",
        "creat iveco mmons",
        "original author(s",
        "J Big Data",
        "Privacy preservation techniques",
        "privacy preserving techniques",
        "The data holder",
        "various privacy threats",
        "user activity data",
        "big data analytics",
        "The Author",
        "privacy violations",
        "privacy concerns",
        "various organizations",
        "various sources",
        "author information",
        "domain areas",
        "able availability",
        "network connectivity",
        "scale data",
        "sensitive data",
        "zip code",
        "shopping cart",
        "public domain",
        "deeper insights",
        "hidden patterns",
        "important decisions",
        "ecommerce sites",
        "buying habits",
        "Face book",
        "inference attacks",
        "Incredible amounts",
        "supply chain",
        "sion streaming",
        "social media",
        "smart phones",
        "voluminous data",
        "decision making",
        "dation systems",
        "data lake",
        "unstructured data",
        "unrestricted use",
        "appropriate credit",
        "MLR Institute",
        "Full list",
        "research challenges",
        "data utility",
        "movie recommendation",
        "digital technology",
        "exponential growth",
        "prominent applications",
        "doi.org",
        "computer technology",
        "Flip kart",
        "SURVEY PAPER",
        "Introduction",
        "volume",
        "variety",
        "computers",
        "storage",
        "gender",
        "disease",
        "caste",
        "religion",
        "businesses",
        "customers",
        "prediction",
        "casting",
        "Amazon",
        "products",
        "friends",
        "places",
        "interest",
        "number",
        "merits",
        "Abstract",
        "hospitals",
        "banks",
        "retail",
        "virtue",
        "humans",
        "machines",
        "Tons",
        "serious",
        "models",
        "limitations",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "rammohan04",
        "1 Department",
        "Science",
        "Engineering",
        "Hyderabad",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "trade",
        "Multidimensional Sensitivity Based Anonymization",
        "Many Privacy preserving techniques",
        "many smart phones users",
        "social networking application",
        "social media sites",
        "hospital holding patient",
        "12Ram Mohan Rao",
        "regular business model",
        "serious privacy breach",
        "Privacy preservation laws",
        "Privacy preservation methods",
        "privacy preservation techniques",
        "smart phone users",
        "maximum data utility",
        "external data sources",
        "private informa- tion",
        "key privacy threats",
        "Data analytics activity",
        "serious privacy threat",
        "person specific data",
        "Cryptographic techniques",
        "Many organizations",
        "Many countries",
        "many apps",
        "users data",
        "medica- tion",
        "specific problem",
        "regular basis",
        "privacy statement",
        "data Privacy",
        "privacy attacks",
        "data holder",
        "mobile apps",
        "ecommerce site",
        "data leakage",
        "Personal embracement",
        "opinion data",
        "new friends",
        "census data",
        "electoral results",
        "medical shop",
        "family member",
        "major reason",
        "Previous research",
        "K anonymity",
        "L diversity",
        "T closeness",
        "Data distribution",
        "private information",
        "individual privacy",
        "sentiment analysis",
        "statistical analysis",
        "access control",
        "various offers",
        "one community",
        "ability",
        "websites",
        "responsibility",
        "contacts",
        "files",
        "camera",
        "conditions",
        "need",
        "Surveillance",
        "Disclosure",
        "crimination",
        "abuse",
        "habits",
        "recommendations",
        "people",
        "transactions",
        "Zip",
        "sensitive",
        "disorder",
        "bias",
        "inequality",
        "instance",
        "government",
        "example",
        "medicines",
        "reminder",
        "Lack",
        "awareness",
        "list",
        "Randomization",
        "MDSBA",
        "Sno Zip Age Salary Disease",
        "Table 3 L diversity privacy preservation technique",
        "Sno Zip Age Disease",
        "equivalence classes attribute disclosure",
        "minimal gener- alization",
        "low income group",
        "sensi- tive attribute",
        "ground knowledge attack",
        "three equivalence classes",
        "huge data loss",
        "40 20k Skin allergy",
        "T closeness measure",
        "2* 6k Cardiac problem",
        "7k Cardiac problem",
        "22k Cardiac problem",
        "K anonymity algorithm",
        "K indistinguishable records",
        "Personalized privacy",
        "age attributes",
        "sensitive attribute",
        "40 Skin allergy",
        "Identity disclosure",
        "three persons",
        "40 Cardiac problem",
        "k value",
        "57677 zip codes",
        "larity attack",
        "de identification",
        "two attacks",
        "particular person",
        "two attributes",
        "vacy threat",
        "important aspect",
        "overall distribution",
        "entire records",
        "close- ness",
        "data analytics",
        "anonymized data",
        "patient data",
        "homogeneity attack",
        "40 24k Cancer",
        "576** zip",
        "3 indistinguishable",
        "Table 1",
        "40 Cancer",
        "Anonymization",
        "process",
        "attempt",
        "algorithms",
        "Incognito",
        "drian",
        "result",
        "generalization",
        "John",
        "Achieving",
        "suppression",
        "values",
        "salaries",
        "others",
        "improvement",
        "distance",
        "distributions",
        "threshold",
        "Table 4 T closeness privacy preservation technique",
        "Hadoop Distributed File System",
        "2* 9k Skin allergy",
        "same attrib- utes",
        "pre processing time",
        "third party analyst",
        "40 42k Cardiac problem",
        "Map Reduce Job",
        "Person specific information",
        "Data distribution technique",
        "age groups",
        "privacy breach",
        "time complexity",
        "one party",
        "Randomization technique",
        "low income",
        "large datasets",
        "employee database",
        "data- base",
        "More number",
        "adversary attack",
        "40 8k Flu",
        "many sites",
        "two ways",
        "aggregate functions",
        "retail store",
        "various branches",
        "Clustering algorithms",
        "different sites",
        "different organizations",
        "other parties",
        "participating sites",
        "crime investigations",
        "police officials",
        "particular criminal",
        "other records",
        "10k records",
        "5k records",
        "proper distribution",
        "probability distribution",
        "Horizontal distribution",
        "Vertical distribution",
        "attribute disclosure",
        "one site",
        "data collection",
        "data volume",
        "aggregate data",
        "respect",
        "ficult",
        "noise",
        "surveys",
        "sis",
        "knowledge",
        "anonymization",
        "head",
        "experiment",
        "employees",
        "order",
        "observations",
        "Mappers",
        "Reducers",
        "Results",
        "outliers",
        "cost",
        "Fig.",
        "operations",
        "sales",
        "analytics",
        "computations",
        "Classification",
        "situations",
        "custodian",
        "details",
        "sion",
        "attributes",
        "conventional Hadoop Distrib- uted Files System",
        "Apache Pig scripting language",
        "large scale data sets",
        "Apache MAP REDUCE",
        "large data sets",
        "tional Anonymization techniques",
        "data collection time",
        "two data items",
        "personal embarrass- ment",
        "four quasi identifiers",
        "conventional encryption techniques",
        "Differential privacy techniques",
        "conventional methods",
        "cryptographic techniques",
        "Data Anonymization",
        "sales data",
        "Data utility",
        "data records",
        "different parties",
        "police investigation",
        "health department",
        "privacy preservation",
        "aggregate computations",
        "single location",
        "single organization",
        "multiparty computation",
        "reduced loss",
        "informa- tion",
        "different nodes",
        "different bags",
        "sensitive information",
        "aggregate information",
        "information loss",
        "vertical distribution",
        "investigating officer",
        "similar technique",
        "generali- zation",
        "class values",
        "vulnerability",
        "datasets",
        "employer",
        "bank",
        "insights",
        "character",
        "overlaps",
        "inputs",
        "function",
        "case",
        "Generalization",
        "issues",
        "scalability",
        "version",
        "improved",
        "predefined",
        "blocks",
        "64 MB",
        "128 MB",
        "filters",
        "bottom",
        "Cryptographic techniques Data distribution Randomization",
        "novel privacy preservation model",
        "Various privacy preservation techniques",
        "soft computing techniques",
        "script- ing language",
        "systematic literature review",
        "large scale heterogeneous",
        "serious privacy threats",
        "identity dis- closure",
        "vacy preservation law",
        "large scale data",
        "stream- ing data",
        "data mining algorithms",
        "Apache Pig script",
        "Data Lake concept",
        "Map Reduce job",
        "various vulnerabilities",
        "Anonymization techniques",
        "robust techniques",
        "attribute preservation",
        "New techniques",
        "privacy problems",
        "privacy hazards",
        "privacy breaches",
        "identity disclosure",
        "law enforcement",
        "data privacy",
        "different groups",
        "development effort",
        "code efficiency",
        "concrete solution",
        "clustering problems",
        "Machine learning",
        "appropriate solution",
        "personal embarrassment",
        "European Union",
        "technological solutions",
        "smart phone",
        "raw format",
        "data sets",
        "Data ingestion",
        "external sources",
        "specific information",
        "personal information",
        "diverse sources",
        "strong need",
        "Table 5 Comparison",
        "native form",
        "time people",
        "bag",
        "method",
        "implementation",
        "rest",
        "Analysis",
        "features",
        "type",
        "complexity",
        "Suitability",
        "Damage",
        "Accuracy",
        "results",
        "discussions",
        "part",
        "nisms",
        "More",
        "challenges",
        "Scalable",
        "transformation",
        "protection",
        "discrimination",
        "lance",
        "Conclusion",
        "Conventional",
        "classification",
        "governments",
        "countries",
        "selves",
        "Lot",
        "messages",
        "chats",
        "leakage",
        "repository",
        "Novel privacy preservation model",
        "rigorous train- ing",
        "Advanced computing techniques",
        "privacy preserving algorithms",
        "massive parallel computing",
        "social media pages",
        "closed circuit television",
        "Sri Venkateswara College",
        "deep learning techniques",
        "existing data sets",
        "SQOOP relational data",
        "social media data",
        "A Data lake",
        "Data Lake Sqoop",
        "Ph.D. work",
        "data processing techniques",
        "machine learning",
        "Hadoop map",
        "sensitive attributes",
        "memory processing",
        "fast processing",
        "proposed model",
        "website logs",
        "customers opinion",
        "different sources",
        "HIVE tables",
        "Abbreviations CCTV",
        "literature survey",
        "necessary amendments",
        "final manuscript",
        "Computer Science",
        "Andhra Pradesh",
        "necessary corrections",
        "Competing interests",
        "many repositories",
        "Springer Nature",
        "jurisdictional claims",
        "institutional affiliations",
        "Ducange Pietro",
        "Pecori Riccardo",
        "Mezzina Paolo",
        "marketing strategies",
        "Soft Comput",
        "Chauhan Arun",
        "Kummamuru Krishna",
        "Toshniwal Durga",
        "transactional data",
        "Load data",
        "Apache Spark",
        "Author details",
        "Apache Flume",
        "intelligent algorithm",
        "Authors’ contributions",
        "JNTU Anantapur",
        "4 JNTU",
        "layers",
        "training",
        "HDFS",
        "job",
        "tokenization",
        "PRMR",
        "paper",
        "SMK",
        "APSK",
        "Technology",
        "Tirupati",
        "Anantapuramu",
        "Acknowledgements",
        "guides",
        "RDBMS",
        "�cal",
        "Availability",
        "materials",
        "one",
        "Funding",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "References",
        "glimpse",
        "framework",
        "Prediction",
        "visit",
        "tweets",
        "23rd ACM SIGMOD-SIGACT-SIGART symp. principles",
        "Privacy-preserving social media data publishing",
        "Diane L. Disclosure-limited data dissemination",
        "eighth ACM SIGKDD international conference",
        "IEEE Trans Knowl Data Eng",
        "Ramakrishnan R. Mondrian multidimensional k-anonymity",
        "J Am Stat Assoc",
        "Int J Comput Vision",
        "IEEE Trans Ind Inf.",
        "Future Gen Comput Syst",
        "practical privacy-preserving data aggregation",
        "22nd international confer- ence",
        "privacy-preserving data mining models",
        "ACM SIGMOD international conference",
        "Ramakrishnan R. Incognito",
        "J Off Stat",
        "Int J Uncertain",
        "Knowl Inf Syst.",
        "scalable multidimensional anonymization",
        "IEEE international conference",
        "self personal data",
        "big metering data",
        "efficient full-domain k-anonymity",
        "data mining solution",
        "Disclosure limitation methods",
        "big data privacy",
        "Personalized privacy preservation",
        "k-anonymity privacy protection",
        "privacy-preserving query",
        "Lambert Diane",
        "SRI International",
        "practical applications",
        "Fuzziness Knowl",
        "Williams R.",
        "Jiang R",
        "Lu R",
        "tabular data",
        "data access",
        "data engineering",
        "optimal k-anonymity",
        "Yang D",
        "Bingqing Q",
        "Cudre-Mauroux P",
        "Liu Y",
        "PDA) scheme",
        "smart grid",
        "Duncan GT",
        "statistical agencies",
        "Spiller K",
        "Hettig M",
        "Kiss E",
        "Kassel J-F",
        "Weber S",
        "Harbach M",
        "android apps",
        "Smith M",
        "usable privacy",
        "Bayardo RJ",
        "Agrawal A.",
        "optimal k-anonymization",
        "Iyengar S.",
        "privacy constraints",
        "knowledge discovery",
        "New York",
        "LeFevre K",
        "DeWitt DJ",
        "Latanya Sweeney",
        "Technical report",
        "Sweeney Latanya",
        "database systems",
        "Machanavajjhala A",
        "Xiao X",
        "Yufei T.",
        "Rubner Y",
        "Tomasi T",
        "Guibas LJ.",
        "earth mover",
        "image retrieval",
        "Aggarwal CC",
        "Philip SY",
        "general survey",
        "Choo KK",
        "high performance",
        "Wang K",
        "Yu PS",
        "Chakraborty S",
        "Fung BCM",
        "down specialization",
        "Zhang X",
        "A MapReduce",
        "disclosure risk",
        "Bottom-up generalization",
        "recommendation",
        "ISSN",
        "Print",
        "Electronic",
        "Confidentiality",
        "theory",
        "Measures",
        "harm",
        "users",
        "thoughts",
        "Self-Tracking",
        "Cham",
        "lan",
        "threats",
        "editor",
        "Symposium",
        "security",
        "SOUPS",
        "Newcastle",
        "UK",
        "July",
        "Proceedings",
        "ICDE",
        "Piscataway",
        "management",
        "Samarati",
        "Pierangela",
        "enforcement",
        "Proc.",
        "PODS",
        "L-diversity",
        "metric",
        "Springer",
        "Fourth",
        "ICDM",
        "Top",
        "approach",
        "scalable two-phase top-down specialization approach",
        "IEEE Trans Parallel Distrib Syst",
        "Third international conference",
        "analytics access control",
        "Secur ity/Abstr act",
        "Priva cy-Prote ction",
        "big data anonymity",
        "green computing",
        "Al-Zobbi M",
        "Shahrestani S",
        "Ruan C",
        "29. Schneider C.",
        "government regulations",
        "techn ologi",
        "Stren gthen",
        "data anonymization",
        "MapReduce privacy",
        "IBM Blogs",
        "preservation",
        "cloud",
        "CGC",
        "Trustcom/BigDataSE/ICESS",
        "bigge",
        "chall",
        "TCS",
        "conte",
        "Cyber"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 0.073349304,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope wit",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1702203,
      "metadata_storage_name": "s40537-020-00292-y.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "metadata_creation_date": "2020-02-24T16:27:45Z",
      "keyphrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "big data processing technologies",
        "A Feature Information Gain",
        "other online shopping sites",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "RESEARCH Narayanan",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "doi.org",
        "prediction accuracy",
        "Introduction",
        "Extracting",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "new prod- uct",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "Consumer product success",
        "product pre-launch prediction",
        "future work” section",
        "user rating matrix",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "low dimensionality",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "Related work",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "marketing strategies",
        "embedded process",
        "prediction classifiers",
        "Page",
        "15Narayanan",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "gener",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "MF",
        "Hao",
        "item",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "ity related matrix factorization",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "flip cart customer reviews",
        "ith customer review Ri",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "port Vector Machine",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "product rating scale",
        "categorical, real",
        "Regression Testing Dataset",
        "Dataset pre‑processing",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "data pre",
        "best information",
        "prediction classifier",
        "multivariate data",
        "pre-launch prediction",
        "Product feature",
        "input dataset",
        "Product categories",
        "feature selection",
        "processing Feature",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "particular review",
        "user requirements",
        "feature impurity",
        "market industry",
        "Battery life",
        "Review Content",
        "user features",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "fk",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "launch predic- tion",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "�G value",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "saveAsSequenceFile",
        "path",
        "map",
        "groupBykey",
        "ReduceBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "Support Vector Machine classifiers",
        "t training feature vectors",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "Prediction classifiers",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "+ b",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "mth review",
        "ith review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2 G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "system perfor- mance",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "system response time",
        "Spark python API",
        "several case studies",
        "system design factors",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "redundant customer reviews",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "proposed system",
        "separate servers",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "predic- tion",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "DMRDF model 740",
        "other gini-index",
        "Product price",
        "scalability requirements",
        "other state",
        "Gini-index model",
        "application development",
        "16 GB",
        "gramming",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Months LSA-based DMRDF Gini-index",
        "LSA-based meth- ods",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "Classifier Logistic regression",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "Dataset size",
        "Gini-index approaches",
        "significant features",
        "LSA-based approaches",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "1GB 5GB",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "The DMRDF",
        "large dataset",
        "many features",
        "Performance comparison",
        "future work",
        "PA measures",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "18GB",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "7",
        "other product feature identification",
        "different reliable online websites",
        "prediction model- ling",
        "data processing domains",
        "information fusion approach",
        "statistical prop- erties",
        "memory computation method",
        "time streaming predictions",
        "dataset model performance",
        "important role",
        "DMRDF model",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "Proposed design",
        "unified API",
        "customer comments",
        "learning algorithms",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "machine",
        "wit"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 0.06378691,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1688565,
      "metadata_storage_name": "s40537-019-0184-5.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDE4NC01LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tu Nguyen Thi Ngoc ",
      "metadata_title": "Mining aspects of customer’s review on the social network",
      "metadata_creation_date": "2019-02-26T14:27:28Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Naive Bayes Open Access",
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Vietnam Electric Power University",
        "Creative Commons license",
        "Nguyen Thi Ngoc",
        "Viet Anh Nguyen2",
        "commerce web sites",
        "useful informa- tion",
        "real world datasets",
        "creat iveco mmons",
        "235 Hoang Quoc Viet",
        "Bayes classification method",
        "original author(s",
        "5-star overall rating",
        "cus- tomer reviews",
        "social network",
        "recent years",
        "significant role",
        "huge amount",
        "efficient methods",
        "ferent level",
        "important role",
        "conditional probability",
        "bootstrap technique",
        "sentiment words",
        "Experimental results",
        "good performance",
        "other state",
        "art methods",
        "Core term",
        "unrestricted use",
        "appropriate credit",
        "Full list",
        "author information",
        "aspect ratings",
        "aspect weights",
        "aspect extraction",
        "aspect words",
        "aspect consistency",
        "three tasks",
        "positive opinions",
        "customers’ opinion",
        "doi.org",
        "Mining aspects",
        "different aspects",
        "acidity aspects",
        "user sentiments",
        "users’ opinions",
        "product aspects",
        "user review",
        "Introduction",
        "lot",
        "people",
        "things",
        "products",
        "services",
        "quality",
        "challenge",
        "problem",
        "paper",
        "study",
        "attributes",
        "components",
        "concept",
        "positivity",
        "negativity",
        "example",
        "Fig.",
        "coffee",
        "Abstract",
        "solutions",
        "satisfaction",
        "degree",
        "importance",
        "manufacturers",
        "approach",
        "features",
        "frequencies",
        "comparison",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "METHODOLOGY",
        "Correspondence",
        "tunn",
        "dhdl",
        "1 Department",
        "E-Commerce",
        "Hanoi",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "body",
        "taste",
        "aroma",
        "instance",
        "Several complex filter-based approaches",
        "Turkish -style cardamon coffee",
        "Turkish-style cardamon coffee",
        "copper stove-top pot",
        "sweetened condensed milk",
        "21Nguyen Thi Ngoc",
        "Three tasks Extracting",
        "Trung Nguyen coffee",
        "overall rat- ing",
        "Hidden Markov Model",
        "aspect extraction task",
        "conditional probability technique",
        "enough core terms",
        "quency-based approaches",
        "aspect candidates",
        "Aspect Rate",
        "Estimating Aspect",
        "overall rating",
        "previous work",
        "non-aspect concepts",
        "earliest work",
        "relevant words",
        "low cost",
        "frequent nouns",
        "noun phrases",
        "dom Field",
        "manu- ally",
        "main challenge",
        "many reviews",
        "big fan",
        "thorough understanding",
        "general impression",
        "specific rating",
        "implicit aspects",
        "important aspects",
        "low-frequent aspects",
        "possible aspects",
        "wrong aspects",
        "multiple aspects",
        "training data",
        "learning techniques",
        "chocolate-like note",
        "frequency-based methods",
        "ter results",
        "universal set",
        "explicit aspects",
        "acidity",
        "difficulty",
        "noise",
        "rare",
        "concerns",
        "statistics",
        "high",
        "HMM",
        "CRF",
        "product",
        "assumption",
        "number",
        "domain",
        "experts",
        "existing",
        "sentences",
        "new",
        "MYOB",
        "January",
        "flared",
        "Istanbul",
        "stuff",
        "cream",
        "sugar",
        "1 Comment",
        "discussion",
        "section",
        "user",
        "Weight",
        "Dark",
        "flared copper stove",
        "J Big Data",
        "future research directions",
        "based, machine learning",
        "different real-life datasets",
        "data mining algorithm",
        "two important tasks",
        "aspect-based opinion mining",
        "associated overall rating",
        "aspect-based rating inference",
        "many researches work",
        "aspect weighting tasks",
        "Different approach",
        "earliest researches",
        "Related work",
        "top pot",
        "condensed milk",
        "supervised approach",
        "sophisticated state",
        "Problem definition",
        "last decade",
        "increasing attention",
        "sentiment analysis",
        "topic modeling",
        "label assignment",
        "commercial companies",
        "improve- ments",
        "filtering approach",
        "Method” sections",
        "frequency threshold",
        "frequency-based approach",
        "aspect extracting",
        "aspect identification",
        "aspect terms",
        "review content",
        "art approaches",
        "The Fig. 2",
        "similar solution",
        "information distance",
        "frequency-based method",
        "regression methods",
        "interesting methods",
        "words frequency",
        "chocolate",
        "note",
        "Body",
        "Aroma",
        "Acidity",
        "bitter",
        "weights",
        "aspects",
        "reviews",
        "fact",
        "accuracy",
        "Results",
        "details",
        "methodology",
        "experimental",
        "evaluation",
        "Conclusion",
        "area",
        "Researchers",
        "survey",
        "nouns",
        "Hu",
        "Liu",
        "part",
        "speech/POS",
        "occurrence",
        "quencies",
        "frequent",
        "spite",
        "simplicity",
        "business",
        "limita",
        "high-frequency",
        "problems",
        "filters",
        "seed",
        "0.",
        "many practical sentiment analysis applications",
        "abilistic Latent Semantic Analysis",
        "real-life sentiment analysis applications",
        "two main basic models",
        "current topic modeling methods",
        "two parameter vectors",
        "Latent Dirichlet allocation",
        "topic mod- eling",
        "large document collec",
        "topic- modeling approaches",
        "mining textual reviews",
        "Learning aspect labels",
        "traditional topic models",
        "The FLDA method",
        "aspect-specific sentiment words",
        "TripAdvisor data set",
        "latent aspect ratings",
        "many types",
        "lexicon-based methods",
        "rule-based approaches",
        "large collection",
        "latent topics",
        "negative topic",
        "principled method",
        "multi-domain reviews",
        "new method",
        "short reviews",
        "manual effort",
        "Joint Sentiment-Topic",
        "Lee [7] dataset",
        "bipartite graph",
        "small number",
        "Rating model",
        "overall ratings",
        "word distribution",
        "probabilistic inference",
        "significant amount",
        "specific entities",
        "mining aspects",
        "unlabeled data",
        "aspect price",
        "information models",
        "weak supervision",
        "full supervision",
        "AIR model",
        "various parameters",
        "reasonable results",
        "frequent topics",
        "unsupervised approach",
        "review rating",
        "LDA model",
        "frequent aspects",
        "labeled sentences",
        "distance",
        "other",
        "dollars",
        "generalization",
        "practice",
        "limitations",
        "texts",
        "researches",
        "pLSA",
        "authors",
        "adjectives",
        "JST",
        "Both",
        "positive",
        "Pang",
        "distributions",
        "addition",
        "Moghaddam",
        "Ester",
        "fac",
        "item",
        "work",
        "fication",
        "sampling",
        "extraction",
        "reviewers",
        "unbalance",
        "tuning",
        "order",
        "Such",
        "Gini Index based feature selection method",
        "The Gini Index method",
        "large movie review data set",
        "Naïve Bayes",
        "Support Vector Machine",
        "machine learning approaches",
        "multiple layer architecture",
        "different sentiment levels",
        "adjectival modifying relations",
        "knowl- edge representation",
        "aspect-based sentiment analysis",
        "Machine learning methods",
        "product overall ratings",
        "possible K aspects",
        "associated sentiment words",
        "annotated data",
        "supervised learning",
        "associated orientations",
        "Movie Ontology",
        "multiple sentences",
        "possible worlds",
        "wk| k",
        "sentiment score",
        "sentiment lexicon",
        "Sentiment classification",
        "dependency relations",
        "predicate relations",
        "verb-object relations",
        "multiple words",
        "dictionary-based approach",
        "Xiaowen Ding",
        "Minqing Hu",
        "eRank algorithm",
        "Synonym lexicon",
        "relative clause",
        "rela- tions",
        "Non-features nouns",
        "proper nouns",
        "brand names",
        "verbal nouns",
        "personal nouns",
        "Peñalver-Martinez",
        "domain ontology",
        "critical issue",
        "supervised methods",
        "two sets",
        "Decision Tree",
        "Neural Network",
        "Maximum Entropy",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "two parts",
        "core terms",
        "movie reviews",
        "topic words",
        "nouns/noun phrases",
        "higher accuracy",
        "SVM) classifier",
        "input text",
        "product features",
        "product coffee",
        "word dictionary",
        "taste aspect",
        "Opinions",
        "respect",
        "polarity",
        "strength",
        "intensification",
        "negation",
        "document",
        "Yan",
        "subject",
        "list",
        "synonyms",
        "basis",
        "cost",
        "money",
        "dictionaries",
        "training",
        "testing",
        "classifiers",
        "DT",
        "Asha",
        "research",
        "model",
        "prediction",
        "techniques",
        "attribute",
        "component",
        "aj",
        "A(.",
        "operator",
        "aftertaste",
        "mouth",
        "supervised learning method",
        "Naive Bayes method",
        "many other people",
        "Aspect core terms",
        "field experts",
        "probability distribution",
        "multiple labels",
        "Notation Description",
        "non-negative weights",
        "R K",
        "K-dimensional vector",
        "corresponding aspect",
        "Extracting aspect",
        "one aspect",
        "aspect rate",
        "aspect expressions",
        "j-th aspect",
        "same aspect",
        "aspect labels",
        "higher weight",
        "Major notations",
        "K aspect",
        "negative words",
        "aspect aj",
        "vector ri",
        "same review",
        "reviews’ text",
        "positive words",
        "review i",
        "∑K",
        "αi",
        "riK",
        "rij",
        "opinion",
        "assessment",
        "rmin",
        "range",
        "Definition",
        "emphasis",
        "associated",
        "Cj",
        "wjk",
        "Table",
        "goal",
        "task",
        "sentence",
        "1,Q",
        "yi",
        "wk",
        "corpus",
        "Sj",
        "Tj",
        "aij",
        "idea",
        "observations",
        "formula",
        "account",
        "occurrences",
        "frequency",
        "subset",
        "initial aspect core terms",
        "flavor taste aftertaste mouthfeel",
        "conditional probabilistic model",
        "initial aspect labeling",
        "initial core terms",
        "original core terms",
        "Aspect Extraction Algorithm",
        "universal label set",
        "new core terms",
        "corresponding aspect words",
        "new-found aspect words",
        "new aspect word",
        "body acidity acid",
        "bootstrapping algorithm",
        "aspect weight",
        "Aspect ratings",
        "new words",
        "existing methods",
        "Bootstrap technique",
        "bol O",
        "symbol X",
        "high probability",
        "sion-based methods",
        "two parameters",
        "following equation",
        "new set",
        "four aspects",
        "K aspects",
        "two aspects",
        "reviews’ texts",
        "new sentences",
        "four circles",
        "maximum number",
        "incorrect labels",
        "coffee product",
        "richer set",
        "Bayes",
        "Figure",
        "sets",
        "ishing",
        "smell",
        "threshold",
        "one",
        "adverbs",
        "process",
        "procedure",
        "step",
        "iterations",
        "sum",
        "θ",
        "most sentiment analysis work",
        "Naïve Bayes method",
        "Speech tech- nique",
        "two consecutive words",
        "linear regression methods",
        "good grassy note",
        "following two sentences",
        "two noun phrases",
        "aspect rating problem",
        "two patterns",
        "candidate sentiment",
        "other methods",
        "same time",
        "key point",
        "important point",
        "weighted sum",
        "multi-label classification",
        "different contexts",
        "big problem",
        "big room",
        "syntactic patterns",
        "fixed patterns",
        "JJ tags",
        "NN tags",
        "RB tags",
        "VB tags",
        "Laplace transformation",
        "class c",
        "one word",
        "first word",
        "k-th aspect",
        "opposite sentiments",
        "same adjective",
        "POS tags",
        "P(rij",
        "class label",
        "second word",
        "rating label",
        "rating rij",
        "rijαij",
        "third rules",
        "feature fk",
        "word features",
        "probability rij",
        "review",
        "content",
        "requirement",
        "Eq.",
        "labels",
        "known",
        "Part",
        "staff",
        "naj",
        "smoothing",
        "∑",
        "POS labeled rules",
        "RBS JJ",
        "JJ JJ",
        "RBS VB",
        "third word",
        "JJ NN",
        "NNS JJ",
        "fq",
        "∏q",
        "fk",
        "fj",
        "Table 2",
        "RBR",
        "VBD",
        "VBN",
        "VBG",
        "method"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    }
  ]
}

### Query 4 ###
{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 14.878597,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope wit",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1702203,
      "metadata_storage_name": "s40537-020-00292-y.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "metadata_creation_date": "2020-02-24T16:27:45Z",
      "keyphrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "big data processing technologies",
        "A Feature Information Gain",
        "other online shopping sites",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "RESEARCH Narayanan",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "doi.org",
        "prediction accuracy",
        "Introduction",
        "Extracting",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "new prod- uct",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "Consumer product success",
        "product pre-launch prediction",
        "future work” section",
        "user rating matrix",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "low dimensionality",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "Related work",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "marketing strategies",
        "embedded process",
        "prediction classifiers",
        "Page",
        "15Narayanan",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "gener",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "MF",
        "Hao",
        "item",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "ity related matrix factorization",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "flip cart customer reviews",
        "ith customer review Ri",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "port Vector Machine",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "product rating scale",
        "categorical, real",
        "Regression Testing Dataset",
        "Dataset pre‑processing",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "data pre",
        "best information",
        "prediction classifier",
        "multivariate data",
        "pre-launch prediction",
        "Product feature",
        "input dataset",
        "Product categories",
        "feature selection",
        "processing Feature",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "particular review",
        "user requirements",
        "feature impurity",
        "market industry",
        "Battery life",
        "Review Content",
        "user features",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "fk",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "launch predic- tion",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "�G value",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "saveAsSequenceFile",
        "path",
        "map",
        "groupBykey",
        "ReduceBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "Support Vector Machine classifiers",
        "t training feature vectors",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "Prediction classifiers",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "+ b",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "mth review",
        "ith review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2 G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "system perfor- mance",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "system response time",
        "Spark python API",
        "several case studies",
        "system design factors",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "redundant customer reviews",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "proposed system",
        "separate servers",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "predic- tion",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "DMRDF model 740",
        "other gini-index",
        "Product price",
        "scalability requirements",
        "other state",
        "Gini-index model",
        "application development",
        "16 GB",
        "gramming",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Months LSA-based DMRDF Gini-index",
        "LSA-based meth- ods",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "Classifier Logistic regression",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "Dataset size",
        "Gini-index approaches",
        "significant features",
        "LSA-based approaches",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "1GB 5GB",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "The DMRDF",
        "large dataset",
        "many features",
        "Performance comparison",
        "future work",
        "PA measures",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "18GB",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "7",
        "other product feature identification",
        "different reliable online websites",
        "prediction model- ling",
        "data processing domains",
        "information fusion approach",
        "statistical prop- erties",
        "memory computation method",
        "time streaming predictions",
        "dataset model performance",
        "important role",
        "DMRDF model",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "Proposed design",
        "unified API",
        "customer comments",
        "learning algorithms",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "machine",
        "wit"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 13.303898,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 593265,
      "metadata_storage_name": "Köchling-Wehner2020_Article_DiscriminatedByAnAlgorithmASys.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L0slQzMlQjZjaGxpbmctV2VobmVyMjAyMF9BcnRpY2xlX0Rpc2NyaW1pbmF0ZWRCeUFuQWxnb3JpdGhtQVN5cy5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Alina Köchling ",
      "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "metadata_creation_date": "2020-11-19T15:45:16Z",
      "keyphrases": [
        "200 artificial intelligence (AI) specialists",
        "two essential HR functions",
        "Marius Claus Wehner1",
        "Heinrich-Heine-University Düsseldorf",
        "major driving forces",
        "human resource management",
        "Alina Köchling1",
        "routinized workplace decisions",
        "current human resource",
        "crucial future research",
        "HRM � Literature review",
        "Abstract Algorithmic decision-making",
        "Keywords Fairness � Discrimination",
        "human biases",
        "systematic review",
        "HR recruitment",
        "HR development",
        "HR) practices",
        "ORIGINAL RESEARCH",
        "new source",
        "unfair treatment",
        "implicit discrimination",
        "implicit) discrimination",
        "current state",
        "research gaps",
        "36 journal articles",
        "possible pitfalls",
        "important theoretical",
        "practical implications",
        "fruitful avenues",
        "fairness � Ethics",
        "rapid growth",
        "Business Administration",
        "Universitätsstrasse",
        "Business Research",
        "automated decision-making",
        "remote control",
        "Möhlmann",
        "important individual",
        "societal implications",
        "organizational optimization",
        "hidden talented",
        "large number",
        "German companies",
        "competitive advantages",
        "commercial providers",
        "algorithmic platforms",
        "performance measurements",
        "large companies",
        "economic reasons",
        "personal beliefs",
        "doi.org",
        "orcid.org",
        "context",
        "Author",
        "advice",
        "firms",
        "costs",
        "efficiency",
        "objectivity",
        "groups",
        "people",
        "unfairness",
        "knowledge",
        "threats",
        "goal",
        "directions",
        "applications",
        "researchers",
        "practitioners",
        "1 Introduction",
        "information",
        "importance",
        "digitalization",
        "organizations",
        "koechling",
        "hhu",
        "1 Faculty",
        "Economics",
        "40225 Dusseldorf",
        "Germany",
        "crossmark",
        "crossref",
        "standardization",
        "Zalmanson",
        "Algorithms",
        "humans",
        "Chalfin",
        "Lee",
        "Lindebaum",
        "changes",
        "favor",
        "employees",
        "Silverman",
        "Waller",
        "Carey",
        "Smith",
        "Savage",
        "Bales",
        "survey",
        "Deloitte",
        "Several",
        "Google",
        "IBM",
        "SAP",
        "Microsoft",
        "systems",
        "hiring",
        "Walker",
        "turn",
        "Vodafone",
        "Unilever",
        "Ikea",
        "Daugherty",
        "Wilson",
        "Precire",
        "savings",
        "time",
        "risks",
        "productivity",
        "certainty",
        "Suen",
        "McDonald",
        "McColl",
        "Michelotti",
        "Woods",
        "prejudices",
        "15",
        "American e-commerce specialist",
        "algorithmic decision- making",
        "new research avenues",
        "HR development processes",
        "unrepresentative input data",
        "algorithmic decision-making system",
        "complete algorithmic decision-making",
        "training) data",
        "algorithmic outcomes",
        "same attention",
        "same requirements",
        "first glance",
        "human decision-making",
        "unequal treatment",
        "different groups",
        "qualitative differences",
        "individual performance",
        "biased outcomes",
        "biased decisions",
        "current debate",
        "extreme disadvantage",
        "796 Business Research",
        "Previous studies",
        "common procedures",
        "labor shortages",
        "one hand",
        "other side",
        "computer science",
        "neous state",
        "distinct challenges",
        "future research",
        "practical point",
        "known companies",
        "negative consequences",
        "potential downsides",
        "potential dangers",
        "possible threat",
        "prominent example",
        "female applicants",
        "hiring decision",
        "employees’ acceptance",
        "existing knowledge",
        "current literature",
        "hiring algorithms",
        "potential biases",
        "HRM context",
        "consistency",
        "fairness",
        "Langer",
        "Florentine",
        "Raghavan",
        "application",
        "criteria",
        "Lepri",
        "discrimination",
        "Simbeck",
        "gender",
        "ethnicity",
        "Arrow",
        "Kim",
        "Barocas",
        "Selbst",
        "Suresh",
        "Guttag",
        "Chander",
        "issue",
        "transparency",
        "Dwork",
        "Diakopoulos",
        "Amazon",
        "Dastin",
        "Miller",
        "lack",
        "accountability",
        "factors",
        "Citron",
        "Pasquale",
        "question",
        "Kaibel",
        "discrepancy",
        "enthusiasm",
        "panacea",
        "inefficiencies",
        "field",
        "infancy",
        "digitization",
        "automation",
        "view",
        "aim",
        "study",
        "awareness",
        "The Oxford Living Dictionary",
        "two important HR areas",
        "several different research areas",
        "various algorithmic decision-making tools",
        "several AI decision tools",
        "other problem-solving operations",
        "two HR functions",
        "explicit human interference",
        "natural language processing",
        "several research gaps",
        "future research avenues",
        "systematic literature review",
        "HR development methods",
        "future research directions",
        "several ways",
        "workplace decision",
        "human intelligence",
        "algorithmic selection",
        "career development",
        "future performance",
        "person-organization fit",
        "serious consequences",
        "Ötting",
        "existing body",
        "ethical issues",
        "illustrative examples",
        "timely topic",
        "enormous importance",
        "reputational risk",
        "ment process",
        "key terms",
        "descriptive analysis",
        "subsequent discussion",
        "theoretical implications",
        "Conceptual background",
        "computational mechanism",
        "umbrella term",
        "wide array",
        "machine learning",
        "image recognition",
        "existing literature",
        "AI applications",
        "current employees",
        "HRM field",
        "lacking focus",
        "statistical models",
        "new data",
        "present paper",
        "distributive fairness",
        "understanding",
        "end",
        "potential",
        "Huselid",
        "Decisions",
        "algorithms",
        "individuals",
        "company",
        "society",
        "ethics",
        "procedural",
        "Maier",
        "Tambe",
        "Cappelli",
        "bias",
        "fact",
        "Companies",
        "legal",
        "applicants",
        "reason",
        "guidance",
        "detailed",
        "definitions",
        "methodology",
        "illustration",
        "processes",
        "sets",
        "rules",
        "calculations",
        "computer",
        "routinized",
        "basis",
        "prescriptions",
        "acquisition",
        "environment",
        "Russell",
        "Norvig",
        "ML",
        "speech",
        "NLP",
        "Kaplan",
        "Haenlein",
        "Paschen",
        "2.1",
        "many AI decision-making tools",
        "three major types",
        "artificial neural networks",
        "random variable Y",
        "three core elements",
        "three different levels",
        "neural network models",
        "deep learning models",
        "fixed input/output data",
        "reinforcement type learning",
        "Unsupervised ML algorithms",
        "reinforcement learning",
        "ML models",
        "network structures",
        "probabilistic models",
        "unsupervised settings",
        "learning tasks",
        "ML approach",
        "regression-type problems",
        "ground truth",
        "Human experts",
        "human decisions",
        "future instances",
        "same problem",
        "priori labeling",
        "structural behaviors",
        "theme analysis",
        "798 Business Research",
        "separate group",
        "error interactions",
        "dynamic environment",
        "Single nodes",
        "human brain",
        "human thinking",
        "other settings",
        "prediction tasks",
        "expected values",
        "systematic error",
        "human judgment",
        "algorithmic evaluations",
        "computer systems",
        "black box",
        "glass boxes",
        "making sense",
        "human involvement",
        "Cognitive biases",
        "entire model",
        "nonlinear transformation",
        "individual components",
        "input data",
        "Shin",
        "predictions",
        "outputs",
        "labels",
        "patterns",
        "Canhoto",
        "contrast",
        "Murphy",
        "variables",
        "methods",
        "trial",
        "bling",
        "methodologies",
        "Examples",
        "complex",
        "relationship",
        "phases",
        "layers",
        "neurons",
        "Bengio",
        "multiple",
        "stages",
        "features",
        "representations",
        "advantage",
        "Deng",
        "Yu",
        "Reason",
        "estimation",
        "bY",
        "difference",
        "Kauermann",
        "Kuechenhoff",
        "Goodfellow",
        "uncertainty",
        "Kahneman",
        "others",
        "Friedman",
        "Nissenbaum",
        "HRM",
        "Cheng",
        "Hackett",
        "theory",
        "consideration",
        "distinction",
        "interpretability",
        "explainability",
        "Roscher",
        "combination",
        "former",
        "simulatability",
        "parameters",
        "2.2",
        "predictive algorithmic decision-making tool",
        "accurate algorithmic output",
        "explicit human judgments",
        "input data set",
        "historical employment data",
        "algorithmic transparency",
        "historical data",
        "ML model",
        "different sets",
        "HR department",
        "employee satisfaction",
        "turnover intentions",
        "specific criteria",
        "underlying reasons",
        "main reasons",
        "learning process",
        "exposed examples",
        "one way",
        "subsequent analysis",
        "worst case",
        "discriminatory outputs",
        "manual programming",
        "recruitment process",
        "white men",
        "protected group",
        "systematic disadvantage",
        "biased data",
        "historical biases",
        "different biases",
        "preexisting biases",
        "Contextual information",
        "emergent bias",
        "representation bias",
        "specific information",
        "personality characteristics",
        "ML algorithm",
        "valid example",
        "implicit bias",
        "decomposability",
        "level",
        "training",
        "Interpretability",
        "element",
        "domain",
        "interpretations",
        "derive",
        "conclusions",
        "results",
        "prediction",
        "interest",
        "technical",
        "quality",
        "Danks",
        "London",
        "keyword",
        "garbage",
        "stereotypes",
        "Barfield",
        "Pagallo",
        "creation",
        "kind",
        "racist",
        "Veale",
        "Binns",
        "tendencies",
        "past",
        "Datta",
        "Hispanics",
        "applicant",
        "member",
        "job",
        "interview",
        "designer",
        "categories",
        "disadvantages",
        "absence",
        "selection",
        "measurements",
        "information systems Leventhal",
        "new societal knowledge",
        "different fairness definitions",
        "new knowledge",
        "several steps",
        "feature engineering",
        "free lunch",
        "discriminatory outcomes",
        "technical constraints",
        "technical consideration",
        "several reasons",
        "important conditions",
        "human constructs",
        "human interpretation",
        "cultural values",
        "two reasons",
        "different values",
        "design process",
        "cooperative efforts",
        "competitive behavior",
        "equal treatment",
        "meso- level",
        "statistical parity",
        "true outcomes",
        "fair treatment",
        "equal opportunity",
        "individual subjects",
        "representative bias",
        "Technical bias",
        "algorithmic system",
        "system design",
        "measurement data",
        "model testing",
        "best model",
        "800 Business Research",
        "training data",
        "Individual fairness",
        "group membership",
        "computer technology",
        "specific context",
        "cultural context",
        "group fairness",
        "real users",
        "protected) groups",
        "preprocessing",
        "theorems",
        "Wolpert",
        "Macready",
        "choice",
        "benchmark",
        "performance",
        "rotations",
        "example",
        "females",
        "comparison",
        "overrepresented",
        "limited",
        "hardware",
        "software",
        "peripherals",
        "Bozdag",
        "formalization",
        "computers",
        "problem",
        "judgments",
        "intuitions",
        "law",
        "litigation",
        "construction",
        "result",
        "population",
        "shift",
        "mismatch",
        "Table",
        "state",
        "art",
        "competition",
        "individualistic",
        "needs",
        "overview",
        "measures",
        "hand",
        "micro-level",
        "Hardt",
        "notions",
        "sense",
        "positives/negatives",
        "sources",
        "disadvantage",
        "2.3",
        "fairness Name Author Definition Individual fairness Dwork",
        "similar’’ classifications Group fairness",
        "descriptive personal evaluation",
        "considerable adverse consequences",
        "individual differences",
        "actual fairness",
        "fairness perceptions",
        "objective fairness",
        "Subjective fairness",
        "Discriminatory categories",
        "working experience",
        "conscious process",
        "current position",
        "personnel selection",
        "prediction systems",
        "selection context",
        "differential validity",
        "subgroup validity",
        "differential prediction",
        "biased results",
        "Table 1 Definitions",
        "P bY",
        "Pð bY",
        "opportunity Hardt",
        "False-negative rates",
        "1f g",
        "random variable",
        "802 Business Research",
        "selection process",
        "applicant pool",
        "training process",
        "meaningful effects",
        "negative experiences",
        "recruitment context",
        "job offer",
        "job turnover",
        "explicit discrimination",
        "selection measure",
        "essential role",
        "equal probability",
        "conscientious behavior",
        "managerial decisions",
        "Implicit discrimination",
        "implicit attitudes",
        "HR literature",
        "years",
        "Persson",
        "Bertrand",
        "aversion",
        "support",
        "characteristics",
        "Frijters",
        "Cascio",
        "Aguinis",
        "probabilities",
        "estimates",
        "slopes",
        "intercepts",
        "subgroups",
        "Meade",
        "Fetzer",
        "Roth",
        "Bobko",
        "Bartlett",
        "usage",
        "algorithmic",
        "decision-making",
        "regard",
        "justice",
        "reality",
        "Cropanzano",
        "employers",
        "subjects",
        "positive",
        "1jG",
        "recidivism",
        "estimator",
        "likelihood",
        "altruisms",
        "Cohen-Charash",
        "Spector",
        "candidates",
        "Bauer",
        "morale",
        "intentions",
        "acceptance",
        "rejection",
        "dissatisfaction",
        "reduction",
        "elimination",
        "conflicts",
        "Gilliland",
        "McCarthy",
        "Hausknecht",
        "image",
        "systematic literature review approach",
        "systematic literature reviews",
        "Several online platforms",
        "extensive keyword searching",
        "defined individual concepts",
        "three core dimensions",
        "fruitful research avenues",
        "high procedural justice",
        "three fairness dimensions",
        "three dimensions",
        "replicable approach.1",
        "individual fairness",
        "research question",
        "rating companies",
        "Van Hoye",
        "most urgency",
        "equal opportunities",
        "ethical norms",
        "important role",
        "relevant information",
        "algorithmic decision-making",
        "information processing",
        "human raters",
        "employee evaluation",
        "personal contact",
        "reliable picture",
        "discrimination potential",
        "particular field",
        "overall picture",
        "two authors",
        "Distributive justice",
        "informational justice",
        "fairness measures",
        "Search terms",
        "interactional justice",
        "development process",
        "organizational context",
        "evidence-based decisions",
        "decision process",
        "iterative process",
        "interactional fairness",
        "same treatment",
        "possibility",
        "outcome",
        "Rules",
        "equality",
        "equity",
        "accordance",
        "contributions",
        "extent",
        "Leventhal",
        "Consistency",
        "accuracy",
        "representation",
        "parties",
        "correction",
        "appropriateness",
        "dignity",
        "courtesy",
        "respect",
        "share",
        "general",
        "procedures",
        "errors",
        "3 Methods",
        "insights",
        "other",
        "suggestions",
        "Petticrew",
        "Roberts",
        "Crossan",
        "Apaydin",
        "Siddaway",
        "Gough",
        "databases",
        "discussion",
        "keywords",
        "3.1",
        "EBSCO Business Source Premier database",
        "social science citation index",
        "language peer-reviewed journals",
        "first source",
        "singular/plural forms",
        "different spellings",
        "narrow terms",
        "classification terms",
        "complete list",
        "broad coverage",
        "impact factor",
        "reasonable proxy",
        "important publications",
        "reference section",
        "search strings",
        "relevant articles",
        "scholarly literature",
        "electronic databases",
        "terminology",
        "synonyms",
        "contents",
        "priority",
        "balance",
        "sensitivity",
        "specificity",
        "unintentional",
        "omission",
        "SSCI",
        "management",
        "part",
        "Web",
        "Knowledge",
        "breadth",
        "papers"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 10.760407,
      "content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6,\nArticle No. 203,",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 3040236,
      "metadata_storage_name": "Liang-Lin2021_Article_MachineLearningForDigitalTry-o.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L0xpYW5nLUxpbjIwMjFfQXJ0aWNsZV9NYWNoaW5lTGVhcm5pbmdGb3JEaWdpdGFsVHJ5LW8ucGRm0",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Administrator",
      "metadata_title": "01-CVM0189.pdf",
      "metadata_creation_date": "2021-04-14T08:29:06Z",
      "keyphrases": [
        "variable 3D human body shapes",
        "Computational Visual Media",
        "Ming C. Lin1",
        "The Author(s",
        "direct visual impression",
        "M. C. Lin",
        "2D image learning",
        "substantial technological gap",
        "human body estimation",
        "open research issues",
        "three major challenges",
        "human shapes",
        "open issues",
        "accurate estimation",
        "extensive research",
        "open challenges",
        "2 Open problems",
        "Machine learning",
        "doi.org",
        "Review Article",
        "Junbang Liang1",
        "economic benefits",
        "practical constraints",
        "accurate sizing",
        "art approaches",
        "future directions",
        "rapid pace",
        "recent years",
        "retail store",
        "attractive alternative",
        "user experience",
        "fashion shopping",
        "physical try",
        "image- editing",
        "College Park",
        "J. Liang",
        "ultimate goal",
        "real worlds",
        "physical worlds",
        "consumer devices",
        "faithful recovery",
        "sewing patterns",
        "traditional methods",
        "constrained problems",
        "learning-based approaches",
        "machine-learning algorithms",
        "tedious data",
        "challenging problems",
        "other areas",
        "several reasons",
        "material modeling",
        "accurate modeling",
        "faithful modeling",
        "garment material",
        "garment pieces",
        "recent advances",
        "template demonstrations",
        "realistic demonstration",
        "notable impact",
        "unseen images",
        "garment modeling",
        "digital try",
        "Abstract Digital",
        "tremendous potential",
        "problem description",
        "sizing systems",
        "Vol.",
        "June",
        "progress",
        "commerce",
        "people",
        "lives",
        "development",
        "realism",
        "complete",
        "state",
        "Keywords",
        "1 Introduction",
        "Consumers",
        "situation",
        "clothes",
        "needs",
        "attention",
        "technology",
        "1 University",
        "Maryland",
        "USA",
        "mail",
        "liangjb",
        "umd",
        "Manuscript",
        "fast",
        "customer",
        "garments",
        "lossless",
        "transformation",
        "virtual",
        "paper",
        "sizes",
        "ease",
        "design",
        "manipulation",
        "end-users",
        "training",
        "optimization",
        "processing",
        "importance",
        "present",
        "improvements",
        "solutions",
        "significant",
        "section",
        "shoppers",
        "lack",
        "consistency",
        "direct 3D body scanning",
        "full fabric digital model",
        "accurate human shape estimation",
        "parametric human model",
        "commodity mobile devices",
        "uncovered body parts",
        "2D/3D body skeletons",
        "successful digital try",
        "Human body reconstruction",
        "human body geometry",
        "entire human body",
        "notable technology gap",
        "voxel- based representation",
        "3D humanoid mesh",
        "standard body shapes",
        "human-body reconstruction problem",
        "3 Human shape estimation",
        "many other works",
        "machine learning methods",
        "Accurate estimation",
        "digital representation",
        "shape learning",
        "body movements",
        "accurate results",
        "digital worlds",
        "digital surveillance",
        "perceptual gap",
        "3D mesh-based",
        "3D animation",
        "early works",
        "recent works",
        "learning process",
        "Different fabric",
        "actual material",
        "real-world examples",
        "two approaches",
        "2D image-based",
        "photo-realistic rendering",
        "considerable effort",
        "2D images",
        "user-friendly design",
        "realistic animation",
        "other factors",
        "Previous work",
        "cloud computing",
        "animation speed",
        "high-quality, interactive",
        "computer animation",
        "special effects",
        "augmented environments",
        "popular topic",
        "specialized hardware",
        "RGB images",
        "Traditional algorithms",
        "optimization problem",
        "silhouette difference",
        "objective function",
        "target function",
        "response time",
        "network models",
        "real-world datasets",
        "joint positions",
        "various ways",
        "current state",
        "different brands",
        "garment materials",
        "garment database",
        "key consideration",
        "online shopping",
        "regression network",
        "body-joint losses",
        "standardization",
        "persons",
        "proportion",
        "consumers",
        "correspondence",
        "customers",
        "appearance",
        "advantages",
        "drawbacks",
        "large",
        "support",
        "variation",
        "case",
        "system",
        "backend",
        "motion",
        "pose",
        "variety",
        "areas",
        "challenging",
        "interest",
        "excellent",
        "adoption",
        "input",
        "one",
        "size",
        "output",
        "major",
        "tight",
        "advances",
        "parameters",
        "techniques",
        "annotations",
        "most",
        "Ref.",
        "complex data generation pipeline",
        "iterative value correction structure",
        "synthetic data generation pipeline",
        "garment material modeling problem",
        "convenient registration process",
        "larger garment dataset",
        "compelling visual simulation",
        "parameter prediction block",
        "image style transfer",
        "4 Garment material modeling",
        "good pose estimation",
        "single view image",
        "unified human shape",
        "human body parameters",
        "Network structure",
        "recurrent structure",
        "RNN-like structure",
        "regression block",
        "Prediction results",
        "fabric modeling",
        "error correction",
        "visual effects",
        "estimated pose",
        "body pose",
        "human bodies",
        "training data",
        "key contribution",
        "several stages",
        "image feature",
        "input guesses",
        "same time",
        "3D joints",
        "loss computation",
        "global orientation",
        "static multi-view",
        "cyclic form",
        "corrective values",
        "ground-truth parameters",
        "performance gap",
        "real-world images",
        "other variables",
        "skin color",
        "3D backgrounds",
        "subtle elements",
        "higher expense",
        "promising direction",
        "realistic images",
        "important role",
        "Physical recreation",
        "challenging task",
        "physical properties",
        "physical behaviour",
        "semantic primitives",
        "sufficient amount",
        "physical behavior",
        "synthetic result",
        "visual information",
        "camera projection",
        "image inputs",
        "shape parameters",
        "camera calibration",
        "multi-view inputs",
        "shape estimation",
        "multi-stage framework",
        "traditional datasets",
        "recent progress",
        "162 J. Liang",
        "different views",
        "fixed number",
        "universal model",
        "ambiguity",
        "Fig.",
        "step",
        "code",
        "gamma",
        "researchdirections",
        "virtualtryon",
        "humanmultiview",
        "digital",
        "Challenges",
        "permission",
        "2D",
        "features",
        "CNNs",
        "use",
        "gradient",
        "Experiments",
        "art",
        "example",
        "regularization",
        "cases",
        "diversity",
        "addition",
        "hair",
        "GAN",
        "legs",
        "chest",
        "person",
        "Author",
        "Introduction",
        "systems",
        "cloth",
        "type",
        "materials",
        "sewing",
        "researchers",
        "underlying",
        "image signal feature extraction method",
        "temporal sequence learning method",
        "CNN conv5-layer activation visualization",
        "deep neural network model",
        "material parameter sensitivity analysis",
        "spring–mass systems",
        "simple feed-forward operation",
        "varying lighting conditions",
        "high-level visual features",
        "optimal material parameter",
        "traditional optimization methods",
        "image features",
        "machine-learning methods",
        "visual appearance",
        "CNN layer",
        "two implications",
        "physical model",
        "many ways",
        "finite elements",
        "popular model",
        "realistic results",
        "isotropic properties",
        "Poisson ratio",
        "anisotropic model",
        "different behaviors",
        "4.2 Learning-based estimation",
        "fast feedback",
        "art model",
        "solution space",
        "positive coefficient",
        "larger number",
        "based simulation",
        "consequent movement",
        "video recording",
        "other object",
        "real-world scenarios",
        "new insights",
        "challenging problem",
        "natural extension",
        "human body",
        "partial occlusion",
        "differentiable physics",
        "geometric differences",
        "unseen video",
        "real material",
        "material sub-space",
        "material properties",
        "material cloning",
        "material type",
        "material parameters",
        "long time",
        "real time",
        "data samples",
        "motion patterns",
        "garment prototyping",
        "small amount",
        "single piece",
        "material basis",
        "cloth simulations",
        "cloth meshes",
        "LSTM layer",
        "cloth motion",
        "clothes materials",
        "input videos",
        "computer",
        "Young",
        "modulus",
        "choice",
        "weave",
        "predictions",
        "applications",
        "Yang",
        "unc",
        "VideoCloth",
        "respect",
        "deformation",
        "sub-spaces",
        "texture",
        "mapping",
        "framework",
        "clothing",
        "approach",
        "fabric",
        "parts",
        "simulated",
        "skirt",
        "environment",
        "computation",
        "constant diagonal weight matrix",
        "high dimensional simulation problem",
        "finite element method",
        "modern simulation algorithms",
        "differentiablecloth 164 J. Liang",
        "material parameter estimation",
        "stable integration results",
        "other differentiable simulations",
        "Differentiable simulation embedding",
        "implicit Euler method",
        "continuous collision detection",
        "original mesh state",
        "general differentiable collision",
        "task-specific loss function",
        "impact zone optimization",
        "differentiable cloth simulation",
        "high dimensionality",
        "QP problem",
        "constraint matrix",
        "impact zones",
        "M̂a",
        "original paper",
        "general approach",
        "physics simulations",
        "complete � Liang",
        "implicit differentiation",
        "Collision handling",
        "mass matrix",
        "typical usage",
        "Similar processes",
        "pixel space",
        "3D space",
        "visual effect",
        "physics work",
        "rigid bodies",
        "particle-grid systems",
        "first work",
        "follow-up work",
        "computational flow",
        "common approach",
        "collision response",
        "linear solver",
        "input parameters",
        "discontinuous states",
        "several techniques",
        "physical solution",
        "linear solution",
        "solution z",
        "physics simulator",
        "dynamic approach",
        "minimum change",
        "distance function",
        "quadratic form",
        "constraint parameters",
        "motion control",
        "handling algorithm",
        "collision constraints",
        "static LCP",
        "neural network",
        "corresponding gradients",
        "gradient values",
        "difference",
        "target",
        "computing",
        "dynamics",
        "discretization",
        "order",
        "discontinuity",
        "zero-measure",
        "backpropagation",
        "QR",
        "decomposition",
        "pipeline",
        "Derivatives",
        "Jacobian",
        "forces",
        "acceleration",
        "equation",
        "readers",
        "details",
        "Collisions",
        "contacts",
        "data",
        "edu",
        "learning",
        "tasks",
        "implementation",
        "instances",
        "Gz",
        "vertex",
        "numbers",
        "variables",
        "3.1",
        "3.2",
        "multiple adjacent collision pairs",
        "interactive apparel design systems",
        "automatic garment model generation",
        "Material parameter estimation results",
        "Realistic apparel model generation",
        "garment generation algorithm",
        "other body shapes",
        "constant wind force",
        "rigid body dynamics",
        "differentiable simulation pipeline",
        "bending stiffness models",
        "baselines Method Runtime",
        "general cloth model",
        "fashion apparel",
        "material model",
        "garment models",
        "realistic 3D",
        "Garment modeling",
        "reaction force",
        "universal pipeline",
        "Simulation error",
        "L/∂z",
        "loss function",
        "linear system",
        "one contact",
        "linear equation",
        "QR decomposition",
        "high dimensional",
        "QP problems",
        "The scene",
        "three parts",
        "stiffness S",
        "stiffness B.",
        "most measurements",
        "linear part",
        "stiffness matrix",
        "unknown parameters",
        "generalized coordinates",
        "Frobenius norms",
        "simulated result",
        "vertex distances",
        "gradient-based method",
        "smaller errors",
        "computing gradients",
        "rapid changes",
        "fashion trends",
        "growing need",
        "different applications",
        "Application requirements",
        "diverse set",
        "different types",
        "image sketches",
        "general garments",
        "optimization formulation",
        "stiffness error",
        "cloth movement",
        "Linear stretching",
        "many challenges",
        "key observation",
        "inputs",
        "goal",
        "dimensionality",
        "rank",
        "4 vertices",
        "12 variables",
        "experiments",
        "ability",
        "piece",
        "gravity",
        "density",
        "Table 1",
        "comparison",
        "Qiao",
        "dt",
        "q̇",
        "response",
        "L-BFGS.",
        "Values",
        "average",
        "iter",
        "Liang",
        "Gf",
        "inclusion",
        "constraints",
        "neighborhood",
        "approximation",
        "topology",
        "customization",
        "extent",
        "Huang",
        "front",
        "Wang",
        "−",
        "5",
        "conditional generative adversarial network",
        "Image-based clothes changing system",
        "skinned multi-person linear model",
        "learning-based parametric generative model",
        "irregular 3D mesh data",
        "M. C. Lin References",
        "3D garment mesh data",
        "latent space learning",
        "relative geometric information",
        "machine learning algorithms",
        "convolutional neural networks",
        "Convolutional pose machines",
        "regular image data",
        "arbitrary human motions",
        "National Science Foundation",
        "4th International Conference",
        "A. Eds. Springer",
        "2D human representations",
        "garment sewing patterns",
        "U–V space",
        "2D displacement image",
        "Visual Media Vol.",
        "One possible approach",
        "The 2D representation",
        "human shape estimation",
        "deep garment representation",
        "garment material prediction",
        "M. J. SMPL",
        "human body shape",
        "one model",
        "3D Vision",
        "estimation accuracy",
        "A. O.",
        "Computer Science",
        "IEEE Conference",
        "One possibility",
        "M. HS-Nets",
        "limited topology",
        "recent work",
        "two problems",
        "Different topology",
        "different values",
        "traditional CNN",
        "different person",
        "virtual reality",
        "excellent potential",
        "open problems",
        "art methods",
        "daily tasks",
        "good start",
        "multiple garments",
        "optimization-based methods",
        "intersection loss",
        "new solution",
        "garment type",
        "substantial breakthroughs",
        "Iribe Professorship",
        "Öztireli",
        "naked truth",
        "Computer Vision",
        "Lecture Notes",
        "Pattern Recognition",
        "ACM Transactions",
        "Article No.",
        "S.-E.",
        "same body",
        "166 J. Liang",
        "unified representation",
        "external constraints",
        "hard constraints",
        "Z. H.",
        "T. J.",
        "F. L.",
        "P. V.",
        "H. T.",
        "al.",
        "retargeting",
        "T-shirts",
        "skirts",
        "difficulties",
        "shapes",
        "cGAN",
        "6 Conclusions",
        "scenario",
        "wind",
        "applicability",
        "area",
        "investigation",
        "directions",
        "Acknowledgements",
        "research",
        "part",
        "Zhang",
        "Computational",
        "Dibra",
        "Jain",
        "Ziegler",
        "R.",
        "Gross",
        "silhouettes",
        "Proceedings",
        "Black",
        "ECCV",
        "Forsyth",
        "D.",
        "Torr",
        "Zisserman",
        "Berlin",
        "Romero",
        "Kiefel",
        "Bogo",
        "Gehler",
        "loop",
        "Loper",
        "Mahmood",
        "N.",
        "Pons-Moll",
        "G.",
        "Graphics",
        "Wei",
        "Ramakrishna",
        "Kanade",
        "Sheikh",
        "Y.",
        "Y. Realtime multi- person 2D pose estimation",
        "Realtime 3D human pose estimation",
        "Neural Information Processing Systems",
        "V. Differentiable cloth simulation",
        "Joint 3D pose",
        "Shape-aware human pose",
        "part affinity fields",
        "human body shapes",
        "De Avila Belbute-Peres",
        "Scalable differentiable physics",
        "differentiable physics engine",
        "single RGB camera",
        "3D human reconstruction",
        "IEEE International Conference",
        "3D people models",
        "Video based reconstruction",
        "IEEE conference",
        "human digitization",
        "Learning-based cloth",
        "human shape",
        "European Conference",
        "33rd Conference",
        "shape reconstruction",
        "Y. Deephuman",
        "Y. L.",
        "single image",
        "Pons- Moll",
        "I. Bodynet",
        "Volumetric inference",
        "high-resolution clothed",
        "accurate scans",
        "multi-view images",
        "Physics-inspired garment",
        "arXiv preprint",
        "inverse problems",
        "B. L.",
        "H.-P.",
        "C. VNect",
        "end recovery",
        "H. PIFu",
        "L. C.",
        "single-view image",
        "material recovery",
        "deep learning",
        "a second",
        "J. FACSIMILE",
        "T. Denserac",
        "M. C.",
        "J. B.",
        "Graphics Vol.",
        "Z. R.",
        "M. J.",
        "J. Z.",
        "W. T.",
        "D. W.",
        "K. A.",
        "Cao",
        "Simon",
        "S.",
        "Mehta",
        "Sridhar",
        "Sotnychenko",
        "Rhodin",
        "Shafiei",
        "Seidel",
        "Xu",
        "Casas",
        "Theobalt",
        "Alldieck",
        "Magnor",
        "Kanazawa",
        "Jacobs",
        "Malik",
        "Varol",
        "Ceylan",
        "Russell",
        "Yumer",
        "E.",
        "Laptev",
        "Zheng",
        "Dai",
        "Q.",
        "Liu",
        "Saito",
        "Natsume",
        "Morishima",
        "Pixel-aligned",
        "function",
        "Zhu",
        "Tung",
        "Smith",
        "X.",
        "Mavroidis",
        "Fast",
        "less",
        "Bhatnagar",
        "Kolotouros",
        "Pavlakos",
        "Daniilidis",
        "modelfitting",
        "Lin",
        "Pan",
        "Amert",
        "Berg",
        "Koltun",
        "control",
        "F.",
        "Allen",
        "Tenenbaum",
        "Kolter",
        "Advances",
        "Degrave",
        "Hermans",
        "Dambre",
        "Wyffels",
        "robotics",
        "Frontiers",
        "Freeman",
        "Wu",
        "real-time differentiable physical simulator",
        "3D garment generation",
        "shared shape space",
        "multimodal garment design",
        "physical simulation",
        "Computer-Aided Design",
        "W. ChainQueen",
        "International Conference",
        "Y. M.",
        "T. M.",
        "F. DiffTaichi",
        "K. X.",
        "X. Y.",
        "X. F.",
        "H. Automatic",
        "two images",
        "Virtual Reality",
        "T. Y.",
        "soft robotics",
        "N. J.",
        "Rus",
        "Matusik",
        "Automation",
        "Hu",
        "Anderson",
        "Sun",
        "Carr",
        "Ragan-Kelley",
        "Durand",
        "programming",
        "Zeng",
        "Bruniaux",
        "P.",
        "Tao",
        "Yao",
        "V.",
        "Zhao",
        "Visualization",
        "Popović",
        "Mitra"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 9.207208,
      "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data  (2018) 5:33  \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence:   \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n  • K anonymity\n  • L diversity\n  • T closeness\n  • Randomization\n  • Data distribution\n  • Cryptographic techniques\n  • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n  • More number of Mappers and Reducers were used as data volume increased.\n  • Results before and after randomization were significantly different.\n  • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n  • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018   Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Prote ction",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1156098,
      "metadata_storage_name": "s40537-018-0141-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTgtMDE0MS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "P. Ram Mohan Rao ",
      "metadata_title": "Privacy preservation techniques in big data analytics: a survey",
      "metadata_creation_date": "2018-09-20T05:58:23Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "A. P. Siva Kumar3",
        "P. Ram Mohan Rao",
        "third party data analyst",
        "modernistic privacy preservation technique",
        "Privacy preservation Open Access",
        "Creative Commons license",
        "Ram Mohan Rao",
        "S. Murali Krishna2",
        "diverse applica- tions",
        "person specific private",
        "value added services",
        "web site logs",
        "creat iveco mmons",
        "original author(s",
        "J Big Data",
        "Privacy preservation techniques",
        "privacy preserving techniques",
        "The data holder",
        "various privacy threats",
        "user activity data",
        "big data analytics",
        "The Author",
        "privacy violations",
        "privacy concerns",
        "various organizations",
        "various sources",
        "author information",
        "domain areas",
        "able availability",
        "network connectivity",
        "scale data",
        "sensitive data",
        "zip code",
        "shopping cart",
        "public domain",
        "deeper insights",
        "hidden patterns",
        "important decisions",
        "ecommerce sites",
        "buying habits",
        "Face book",
        "inference attacks",
        "Incredible amounts",
        "supply chain",
        "sion streaming",
        "social media",
        "smart phones",
        "voluminous data",
        "decision making",
        "dation systems",
        "data lake",
        "unstructured data",
        "unrestricted use",
        "appropriate credit",
        "MLR Institute",
        "Full list",
        "research challenges",
        "data utility",
        "movie recommendation",
        "digital technology",
        "exponential growth",
        "prominent applications",
        "doi.org",
        "computer technology",
        "Flip kart",
        "SURVEY PAPER",
        "Introduction",
        "volume",
        "variety",
        "computers",
        "storage",
        "gender",
        "disease",
        "caste",
        "religion",
        "businesses",
        "customers",
        "prediction",
        "casting",
        "Amazon",
        "products",
        "friends",
        "places",
        "interest",
        "number",
        "merits",
        "Abstract",
        "hospitals",
        "banks",
        "retail",
        "virtue",
        "humans",
        "machines",
        "Tons",
        "serious",
        "models",
        "limitations",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "rammohan04",
        "1 Department",
        "Science",
        "Engineering",
        "Hyderabad",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "trade",
        "Multidimensional Sensitivity Based Anonymization",
        "Many Privacy preserving techniques",
        "many smart phones users",
        "social networking application",
        "social media sites",
        "hospital holding patient",
        "12Ram Mohan Rao",
        "regular business model",
        "serious privacy breach",
        "Privacy preservation laws",
        "Privacy preservation methods",
        "privacy preservation techniques",
        "smart phone users",
        "maximum data utility",
        "external data sources",
        "private informa- tion",
        "key privacy threats",
        "Data analytics activity",
        "serious privacy threat",
        "person specific data",
        "Cryptographic techniques",
        "Many organizations",
        "Many countries",
        "many apps",
        "users data",
        "medica- tion",
        "specific problem",
        "regular basis",
        "privacy statement",
        "data Privacy",
        "privacy attacks",
        "data holder",
        "mobile apps",
        "ecommerce site",
        "data leakage",
        "Personal embracement",
        "opinion data",
        "new friends",
        "census data",
        "electoral results",
        "medical shop",
        "family member",
        "major reason",
        "Previous research",
        "K anonymity",
        "L diversity",
        "T closeness",
        "Data distribution",
        "private information",
        "individual privacy",
        "sentiment analysis",
        "statistical analysis",
        "access control",
        "various offers",
        "one community",
        "ability",
        "websites",
        "responsibility",
        "contacts",
        "files",
        "camera",
        "conditions",
        "need",
        "Surveillance",
        "Disclosure",
        "crimination",
        "abuse",
        "habits",
        "recommendations",
        "people",
        "transactions",
        "Zip",
        "sensitive",
        "disorder",
        "bias",
        "inequality",
        "instance",
        "government",
        "example",
        "medicines",
        "reminder",
        "Lack",
        "awareness",
        "list",
        "Randomization",
        "MDSBA",
        "Sno Zip Age Salary Disease",
        "Table 3 L diversity privacy preservation technique",
        "Sno Zip Age Disease",
        "equivalence classes attribute disclosure",
        "minimal gener- alization",
        "low income group",
        "sensi- tive attribute",
        "ground knowledge attack",
        "three equivalence classes",
        "huge data loss",
        "40 20k Skin allergy",
        "T closeness measure",
        "2* 6k Cardiac problem",
        "7k Cardiac problem",
        "22k Cardiac problem",
        "K anonymity algorithm",
        "K indistinguishable records",
        "Personalized privacy",
        "age attributes",
        "sensitive attribute",
        "40 Skin allergy",
        "Identity disclosure",
        "three persons",
        "40 Cardiac problem",
        "k value",
        "57677 zip codes",
        "larity attack",
        "de identification",
        "two attacks",
        "particular person",
        "two attributes",
        "vacy threat",
        "important aspect",
        "overall distribution",
        "entire records",
        "close- ness",
        "data analytics",
        "anonymized data",
        "patient data",
        "homogeneity attack",
        "40 24k Cancer",
        "576** zip",
        "3 indistinguishable",
        "Table 1",
        "40 Cancer",
        "Anonymization",
        "process",
        "attempt",
        "algorithms",
        "Incognito",
        "drian",
        "result",
        "generalization",
        "John",
        "Achieving",
        "suppression",
        "values",
        "salaries",
        "others",
        "improvement",
        "distance",
        "distributions",
        "threshold",
        "Table 4 T closeness privacy preservation technique",
        "Hadoop Distributed File System",
        "2* 9k Skin allergy",
        "same attrib- utes",
        "pre processing time",
        "third party analyst",
        "40 42k Cardiac problem",
        "Map Reduce Job",
        "Person specific information",
        "Data distribution technique",
        "age groups",
        "privacy breach",
        "time complexity",
        "one party",
        "Randomization technique",
        "low income",
        "large datasets",
        "employee database",
        "data- base",
        "More number",
        "adversary attack",
        "40 8k Flu",
        "many sites",
        "two ways",
        "aggregate functions",
        "retail store",
        "various branches",
        "Clustering algorithms",
        "different sites",
        "different organizations",
        "other parties",
        "participating sites",
        "crime investigations",
        "police officials",
        "particular criminal",
        "other records",
        "10k records",
        "5k records",
        "proper distribution",
        "probability distribution",
        "Horizontal distribution",
        "Vertical distribution",
        "attribute disclosure",
        "one site",
        "data collection",
        "data volume",
        "aggregate data",
        "respect",
        "ficult",
        "noise",
        "surveys",
        "sis",
        "knowledge",
        "anonymization",
        "head",
        "experiment",
        "employees",
        "order",
        "observations",
        "Mappers",
        "Reducers",
        "Results",
        "outliers",
        "cost",
        "Fig.",
        "operations",
        "sales",
        "analytics",
        "computations",
        "Classification",
        "situations",
        "custodian",
        "details",
        "sion",
        "attributes",
        "conventional Hadoop Distrib- uted Files System",
        "Apache Pig scripting language",
        "large scale data sets",
        "Apache MAP REDUCE",
        "large data sets",
        "tional Anonymization techniques",
        "data collection time",
        "two data items",
        "personal embarrass- ment",
        "four quasi identifiers",
        "conventional encryption techniques",
        "Differential privacy techniques",
        "conventional methods",
        "cryptographic techniques",
        "Data Anonymization",
        "sales data",
        "Data utility",
        "data records",
        "different parties",
        "police investigation",
        "health department",
        "privacy preservation",
        "aggregate computations",
        "single location",
        "single organization",
        "multiparty computation",
        "reduced loss",
        "informa- tion",
        "different nodes",
        "different bags",
        "sensitive information",
        "aggregate information",
        "information loss",
        "vertical distribution",
        "investigating officer",
        "similar technique",
        "generali- zation",
        "class values",
        "vulnerability",
        "datasets",
        "employer",
        "bank",
        "insights",
        "character",
        "overlaps",
        "inputs",
        "function",
        "case",
        "Generalization",
        "issues",
        "scalability",
        "version",
        "improved",
        "predefined",
        "blocks",
        "64 MB",
        "128 MB",
        "filters",
        "bottom",
        "Cryptographic techniques Data distribution Randomization",
        "novel privacy preservation model",
        "Various privacy preservation techniques",
        "soft computing techniques",
        "script- ing language",
        "systematic literature review",
        "large scale heterogeneous",
        "serious privacy threats",
        "identity dis- closure",
        "vacy preservation law",
        "large scale data",
        "stream- ing data",
        "data mining algorithms",
        "Apache Pig script",
        "Data Lake concept",
        "Map Reduce job",
        "various vulnerabilities",
        "Anonymization techniques",
        "robust techniques",
        "attribute preservation",
        "New techniques",
        "privacy problems",
        "privacy hazards",
        "privacy breaches",
        "identity disclosure",
        "law enforcement",
        "data privacy",
        "different groups",
        "development effort",
        "code efficiency",
        "concrete solution",
        "clustering problems",
        "Machine learning",
        "appropriate solution",
        "personal embarrassment",
        "European Union",
        "technological solutions",
        "smart phone",
        "raw format",
        "data sets",
        "Data ingestion",
        "external sources",
        "specific information",
        "personal information",
        "diverse sources",
        "strong need",
        "Table 5 Comparison",
        "native form",
        "time people",
        "bag",
        "method",
        "implementation",
        "rest",
        "Analysis",
        "features",
        "type",
        "complexity",
        "Suitability",
        "Damage",
        "Accuracy",
        "results",
        "discussions",
        "part",
        "nisms",
        "More",
        "challenges",
        "Scalable",
        "transformation",
        "protection",
        "discrimination",
        "lance",
        "Conclusion",
        "Conventional",
        "classification",
        "governments",
        "countries",
        "selves",
        "Lot",
        "messages",
        "chats",
        "leakage",
        "repository",
        "Novel privacy preservation model",
        "rigorous train- ing",
        "Advanced computing techniques",
        "privacy preserving algorithms",
        "massive parallel computing",
        "social media pages",
        "closed circuit television",
        "Sri Venkateswara College",
        "deep learning techniques",
        "existing data sets",
        "SQOOP relational data",
        "social media data",
        "A Data lake",
        "Data Lake Sqoop",
        "Ph.D. work",
        "data processing techniques",
        "machine learning",
        "Hadoop map",
        "sensitive attributes",
        "memory processing",
        "fast processing",
        "proposed model",
        "website logs",
        "customers opinion",
        "different sources",
        "HIVE tables",
        "Abbreviations CCTV",
        "literature survey",
        "necessary amendments",
        "final manuscript",
        "Computer Science",
        "Andhra Pradesh",
        "necessary corrections",
        "Competing interests",
        "many repositories",
        "Springer Nature",
        "jurisdictional claims",
        "institutional affiliations",
        "Ducange Pietro",
        "Pecori Riccardo",
        "Mezzina Paolo",
        "marketing strategies",
        "Soft Comput",
        "Chauhan Arun",
        "Kummamuru Krishna",
        "Toshniwal Durga",
        "transactional data",
        "Load data",
        "Apache Spark",
        "Author details",
        "Apache Flume",
        "intelligent algorithm",
        "Authors’ contributions",
        "JNTU Anantapur",
        "4 JNTU",
        "layers",
        "training",
        "HDFS",
        "job",
        "tokenization",
        "PRMR",
        "paper",
        "SMK",
        "APSK",
        "Technology",
        "Tirupati",
        "Anantapuramu",
        "Acknowledgements",
        "guides",
        "RDBMS",
        "�cal",
        "Availability",
        "materials",
        "one",
        "Funding",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "References",
        "glimpse",
        "framework",
        "Prediction",
        "visit",
        "tweets",
        "23rd ACM SIGMOD-SIGACT-SIGART symp. principles",
        "Privacy-preserving social media data publishing",
        "Diane L. Disclosure-limited data dissemination",
        "eighth ACM SIGKDD international conference",
        "IEEE Trans Knowl Data Eng",
        "Ramakrishnan R. Mondrian multidimensional k-anonymity",
        "J Am Stat Assoc",
        "Int J Comput Vision",
        "IEEE Trans Ind Inf.",
        "Future Gen Comput Syst",
        "practical privacy-preserving data aggregation",
        "22nd international confer- ence",
        "privacy-preserving data mining models",
        "ACM SIGMOD international conference",
        "Ramakrishnan R. Incognito",
        "J Off Stat",
        "Int J Uncertain",
        "Knowl Inf Syst.",
        "scalable multidimensional anonymization",
        "IEEE international conference",
        "self personal data",
        "big metering data",
        "efficient full-domain k-anonymity",
        "data mining solution",
        "Disclosure limitation methods",
        "big data privacy",
        "Personalized privacy preservation",
        "k-anonymity privacy protection",
        "privacy-preserving query",
        "Lambert Diane",
        "SRI International",
        "practical applications",
        "Fuzziness Knowl",
        "Williams R.",
        "Jiang R",
        "Lu R",
        "tabular data",
        "data access",
        "data engineering",
        "optimal k-anonymity",
        "Yang D",
        "Bingqing Q",
        "Cudre-Mauroux P",
        "Liu Y",
        "PDA) scheme",
        "smart grid",
        "Duncan GT",
        "statistical agencies",
        "Spiller K",
        "Hettig M",
        "Kiss E",
        "Kassel J-F",
        "Weber S",
        "Harbach M",
        "android apps",
        "Smith M",
        "usable privacy",
        "Bayardo RJ",
        "Agrawal A.",
        "optimal k-anonymization",
        "Iyengar S.",
        "privacy constraints",
        "knowledge discovery",
        "New York",
        "LeFevre K",
        "DeWitt DJ",
        "Latanya Sweeney",
        "Technical report",
        "Sweeney Latanya",
        "database systems",
        "Machanavajjhala A",
        "Xiao X",
        "Yufei T.",
        "Rubner Y",
        "Tomasi T",
        "Guibas LJ.",
        "earth mover",
        "image retrieval",
        "Aggarwal CC",
        "Philip SY",
        "general survey",
        "Choo KK",
        "high performance",
        "Wang K",
        "Yu PS",
        "Chakraborty S",
        "Fung BCM",
        "down specialization",
        "Zhang X",
        "A MapReduce",
        "disclosure risk",
        "Bottom-up generalization",
        "recommendation",
        "ISSN",
        "Print",
        "Electronic",
        "Confidentiality",
        "theory",
        "Measures",
        "harm",
        "users",
        "thoughts",
        "Self-Tracking",
        "Cham",
        "lan",
        "threats",
        "editor",
        "Symposium",
        "security",
        "SOUPS",
        "Newcastle",
        "UK",
        "July",
        "Proceedings",
        "ICDE",
        "Piscataway",
        "management",
        "Samarati",
        "Pierangela",
        "enforcement",
        "Proc.",
        "PODS",
        "L-diversity",
        "metric",
        "Springer",
        "Fourth",
        "ICDM",
        "Top",
        "approach",
        "scalable two-phase top-down specialization approach",
        "IEEE Trans Parallel Distrib Syst",
        "Third international conference",
        "analytics access control",
        "Secur ity/Abstr act",
        "Priva cy-Prote ction",
        "big data anonymity",
        "green computing",
        "Al-Zobbi M",
        "Shahrestani S",
        "Ruan C",
        "29. Schneider C.",
        "government regulations",
        "techn ologi",
        "Stren gthen",
        "data anonymization",
        "MapReduce privacy",
        "IBM Blogs",
        "preservation",
        "cloud",
        "CGC",
        "Trustcom/BigDataSE/ICESS",
        "bigge",
        "chall",
        "TCS",
        "conte",
        "Cyber"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 9.070947,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1434817,
      "metadata_storage_name": "s40708-020-00109-x.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDcwOC0wMjAtMDAxMDkteC5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Ferdousi Sabera Rawnaque ",
      "metadata_title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "metadata_creation_date": "2020-09-18T02:02:41Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Khondaker Abdullah Al Mamun",
        "high time resolution advantages",
        "Physiological response measuring techniques",
        "consumer emotion recognition-based experiments",
        "other third party material",
        "neural record- ing techniques",
        "video advertisement-based Neuromarketing experiments",
        "other machine learning algorithms",
        "Creative Commons licence",
        "Support Vector Machine",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "empirical research findings",
        "magnetic resonance imaging",
        "heart rate monitoring",
        "traditional filtering methods",
        "independent component analysis",
        "Linear Discriminant Analysis",
        "highest average accuracy",
        "market- ing research",
        "consumer response prediction",
        "Artificial Neural Network",
        "prefrontal alpha band",
        "skin conductance recording",
        "Brain computer interface",
        "brain–computer interface",
        "prevalent marketing stimuli",
        "first systematic review",
        "Neural recording",
        "unspoken response",
        "Neuromarket- ing",
        "consumer goods",
        "neural signal",
        "consumer preferences",
        "Brain Inf.",
        "brain recordings",
        "doi.org",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "commercial area",
        "last 5 years",
        "valid databases",
        "promotion forms",
        "asymmetry theory",
        "many researchers",
        "low cost",
        "eye tracking",
        "facial mapping",
        "artifact removal",
        "future researchers",
        "vital information",
        "novel contributions",
        "The Author",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "BCI) technology",
        "interdisciplinary bridge",
        "ultimate sale",
        "targeted audiences",
        "expanding economy",
        "effective marketing",
        "57 relevant literatures",
        "intended use",
        "permitted use",
        "good product",
        "new businesses",
        "Neuromarketing field",
        "Technological advancements",
        "opportunities",
        "Abstract",
        "academic",
        "interest",
        "interpreting",
        "tool",
        "consumers",
        "article",
        "purpose",
        "authors",
        "total",
        "basic",
        "trend",
        "nals",
        "electroencephalogram",
        "EEG",
        "functional",
        "fMRI",
        "parallel",
        "classification",
        "ANN",
        "SVM",
        "LDA",
        "Keywords",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "1 Introduction",
        "application",
        "invasive",
        "neuroscience",
        "perception",
        "changing",
        "non-invasive brain signal record- ing techniques",
        "1 Advanced Intelligent Multidisciplinary Systems Lab",
        "transcranial mag- netic stimulator",
        "Functional mag- netic resonance",
        "Open Access Brain Informatics",
        "market- ing stimuli",
        "image processing techniques",
        "nal recording techniques",
        "functional near-infrared spectroscopy",
        "com- petitive market",
        "United International University",
        "positron emission tomography",
        "machine learning algorithms",
        "skin conductance measurements",
        "neuronal activ- ity",
        "Different stimuli trigger",
        "marketing research techniques",
        "ventional market research",
        "traditional survey methods",
        "neural recording devices",
        "various marketing stimuli",
        "consumers’ purchase decisions",
        "Neuromarketing World Forum",
        "Advanced Research",
        "neuronal signals",
        "human brain",
        "traditional marketing",
        "Neuro- marketing",
        "academic research",
        "qualitative assessment",
        "posteriori analysis",
        "Full list",
        "author information",
        "group discussion",
        "personal interviews",
        "consumer feedback",
        "time requirement",
        "high cost",
        "unreliable information",
        "inaccurate results",
        "possible meanings",
        "new door",
        "buying behavior",
        "tal states",
        "physiological signals",
        "heart rate",
        "physiological responses",
        "spectral analysis",
        "accurate depiction",
        "Early years",
        "contro- versy",
        "high promises",
        "consumer mind",
        "buy buttons",
        "consumer behavior",
        "Business Association",
        "annual event",
        "Neuromarketing research",
        "field trials",
        "unspoken cognitive",
        "emotional responses",
        "Neuromarketing Science",
        "consumer response",
        "associated response",
        "quantitative",
        "products",
        "surveys",
        "Correspondence",
        "frawnaque",
        "umassd",
        "Institute",
        "Dhaka",
        "Bangladesh",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "19Rawnaque",
        "observations",
        "approaches",
        "limitations",
        "contrast",
        "customer",
        "electroencephalography",
        "magnetoencephalography",
        "MEG",
        "TMS",
        "fNIRS",
        "examples",
        "withdrawal",
        "change",
        "brainwaves",
        "researchers",
        "help",
        "excitement",
        "engagement",
        "stress",
        "insight",
        "audience",
        "preferences",
        "likes",
        "academician",
        "marketers",
        "due",
        "lack",
        "groundwork",
        "claim",
        "scrutiny",
        "scope",
        "NMSBA",
        "gap",
        "dialogue",
        "other physiological signal recording device",
        "best brain signal recording tool",
        "address inclusion–exclusion criteria",
        "relevant academic journal articles",
        "neural recording tools",
        "industry–academia collaboration",
        "brief intro- duction",
        "unbiased evidence collection",
        "clear con- clusions",
        "Numerous literature reviews",
        "other literature review",
        "mar- keting stimuli",
        "date research scopes",
        "systematic literature review",
        "new research field",
        "signal processing",
        "inclusion criteria",
        "Academic research",
        "relevant inference",
        "new perspective",
        "brain signals",
        "cal tools",
        "systematic review",
        "research question",
        "brain regions",
        "150 consumer neuroscience",
        "big brands",
        "efficient way",
        "lytical accuracy",
        "building blocks",
        "theoretical aspect",
        "business ethics",
        "current methods",
        "tech- niques",
        "art findings",
        "synthesized result",
        "impartial conclusion",
        "exhaustive search",
        "experimental findings",
        "neu- ral",
        "biometric data",
        "Book chapters",
        "marketing stimuli",
        "empirical experiments",
        "engineering part",
        "pretational methodologies",
        "important results",
        "lowing questions",
        "Neuromarketing experiments",
        "platform",
        "panies",
        "globe",
        "Google",
        "Microsoft",
        "Unilever",
        "insights",
        "tailored",
        "breakthrough",
        "acceptance",
        "world",
        "capacities",
        "management",
        "psychology",
        "focus",
        "regard",
        "premises",
        "types",
        "analysis",
        "techniques",
        "comprehensive",
        "knowledge",
        "methodology",
        "state",
        "synthesis",
        "recommendation",
        "body",
        "objective",
        "obligation",
        "investigation",
        "existing",
        "literatures",
        "evance",
        "studies",
        "analytical",
        "BCI",
        "language",
        "English",
        "brain signal recording techniques",
        "Neural response recording techniques",
        "Brain signal processing",
        "relevant existing literatures",
        "Machine learning applications",
        "autonomous nervous system",
        "Neuromarket- ing field",
        "different brain regions",
        "galvanic skin response",
        "Wiley Online Library",
        "Neuromarketing Product Chew",
        "neural data",
        "3  Systematic review",
        "database source",
        "book chapters",
        "six databases",
        "Taylor Francis",
        "initial article",
        "critical analy",
        "action coding",
        "successful means",
        "withdrawal actions",
        "engineering perspective",
        "statistical analy",
        "lyze activities",
        "Price Çakar",
        "Grigaliūnaitė",
        "Soria Morillo",
        "marketing strategies",
        "Neuromarketing questions",
        "search item",
        "Science Direct",
        "IEEE Xplore",
        "Table 1 Number",
        "database Results",
        "Emerald insight",
        "Neuromarketing studies",
        "following dimensions",
        "Marketing stimuli",
        "Table 2 Studies",
        "57 research articles",
        "931 articles",
        "internet",
        "Neuro-marketing",
        "publications",
        "aggregation",
        "Sage",
        "mulation",
        "title",
        "abstract",
        "keywords",
        "sis",
        "terms",
        "trends",
        "advancements",
        "Activation",
        "facial",
        "consumer",
        "arousal",
        "attention",
        "study",
        "Name",
        "tools",
        "paper",
        "form",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Touchette",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "al.",
        "Gong",
        "Pilelienė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Promotion",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "neural recording systems",
        "signal capturing technologies",
        "heart rate Cherubino",
        "EEG Soria Morillo",
        "brain region",
        "brain functions",
        "brain anatomy",
        "brain areas",
        "brain activity",
        "EMG Missagila",
        "Eye tracking",
        "Galvanic skin",
        "atic review",
        "human behavior",
        "cal functions",
        "true essence",
        "anatomical functionality",
        "high accuracy",
        "external stimuli",
        "fNIRS Çakir",
        "Neuromarketing Cherubino",
        "fMRI Venkatraman",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "subjects",
        "matter",
        "interconnection",
        "actions",
        "physiological",
        "hidden",
        "ences",
        "section",
        "targeted",
        "signals",
        "associated",
        "number",
        "3D (three dimensional) image",
        "machine learning techniques",
        "tral nervous system",
        "predictive modeling framework",
        "3D visualiza- tion",
        "other marketing models",
        "consumer behavior researchers",
        "E-commerce prod- ucts",
        "gift card limit",
        "virtual 3D bracelet",
        "30 existing image",
        "3D shapes",
        "consumer demand",
        "Consumer response",
        "consumer choice",
        "different strategies",
        "market- ing",
        "quantitative assessment",
        "rological data",
        "thinking procedures",
        "last 5  years",
        "two forms",
        "physical object",
        "important role",
        "user esthetics",
        "new area",
        "mathematical model",
        "lis superformula",
        "school bags",
        "first-time buyers",
        "partici- pants",
        "commerce website",
        "negative emotion",
        "necessary buttons",
        "sorting options",
        "Retail businesses",
        "large amount",
        "large number",
        "mock shop",
        "right-handed participants",
        "test participants",
        "different designs",
        "shoe designs",
        "E-commerce products",
        "retail products",
        "test subjects",
        "normal vision",
        "brain response",
        "user experience",
        "42 product images",
        "product search",
        "wrong product",
        "possible preference",
        "60 bracelet",
        "medications",
        "history",
        "neuropathology",
        "Nemorin",
        "price",
        "promotions",
        "service",
        "beverage",
        "decision",
        "objects",
        "recognition",
        "bracelets",
        "advantage",
        "apparels",
        "accessory",
        "items",
        "shirts",
        "sweaters",
        "shoes",
        "wrist",
        "watches",
        "view",
        "maximum",
        "money",
        "thousands",
        "blueprints",
        "manufacturing",
        "failures",
        "3.1",
        "corporate social respon- sibilities",
        "frontal asymmetry theory",
        "female undergraduate students",
        "capital white letter",
        "luxury) prod- ucts",
        "high emotional value",
        "higher emotional value",
        "shoe selection time",
        "high-resolution computer screen",
        "60 basic brand items",
        "wine tasting experiment",
        "regular brand products",
        "luxury brand products",
        "brain response-based prediction",
        "consumer brain response",
        "60 luxury items",
        "gift value",
        "brand images",
        "brand names",
        "brand logos",
        "national brand",
        "social atmosphere",
        "apparel products",
        "branded products",
        "shoe experiment",
        "EEG signals",
        "Likert scale",
        "self-report-based methods",
        "sales data",
        "12.1% profit growth",
        "36.4% profit growth",
        "young adults",
        "random order",
        "alone atmosphere",
        "brain responses",
        "consumer attitude",
        "Tahoma font",
        "black background",
        "other hand",
        "implicit response",
        "different types",
        "important factor",
        "interesting concept",
        "tional companies",
        "responsible companies",
        "decision-making time",
        "passive role",
        "keting strategies",
        "lower degree",
        "con- trol",
        "pivotal role",
        "purchase decision",
        "two wines",
        "lesser price",
        "rank",
        "simulation",
        "choice",
        "Davidson",
        "comparison",
        "brands",
        "blocks",
        "popular",
        "experiments",
        "branding",
        "user",
        "first-time",
        "promotional",
        "relation",
        "author",
        "company",
        "socially",
        "influence",
        "discounts",
        "gifts",
        "gift-giving",
        "ambiguity",
        "40",
        "mon experimental design procedure",
        "merchandise product advertisement clips",
        "wine tasting session",
        "older adults’ reaction",
        "mated review systems",
        "different aged population",
        "facial biometric sensors",
        "TV commer- cials",
        "six smartphone commercials",
        "super bowl commercials",
        "promotion-based Neuromarketing experiments",
        "The Neuromarketing experiments",
        "sustainable product designs",
        "seven TV commercials",
        "unfavorable TV commercials",
        "cognitive neurophysiological indices",
        "consecutive advertise- ments",
        "experimental setting",
        "conventional product",
        "14 TV commercials",
        "100 TV commercials",
        "different brands",
        "ral indices",
        "Nestle advertisement",
        "television commercials",
        "consecutive advertisements",
        "TV advertisements",
        "equal price",
        "short movies",
        "key focus",
        "higher rate",
        "ful promotions",
        "memory rate",
        "time distance",
        "neutral stimuli",
        "white screen",
        "green scenario",
        "electrical activity",
        "advanced algorithms",
        "emotional changes",
        "cerebral activity",
        "strong images",
        "pulse analysis",
        "print media",
        "animated commercial",
        "verbal narrative",
        "ing commercial",
        "visual narrative",
        "graduate students",
        "class rank",
        "emotional states",
        "visual attentions",
        "lit- eratures",
        "standard time",
        "video advertisements",
        "neural response",
        "consumer engagement",
        "audience brain",
        "web page",
        "user preference",
        "audiences’ taste",
        "user attention",
        "favorable advertisements",
        "static advertisements",
        "50 commercials",
        "communication",
        "Neuromarketers",
        "air",
        "extract",
        "happiness",
        "surprise",
        "10 s",
        "participants",
        "observation",
        "Krugman",
        "success",
        "neuroimaging",
        "gies",
        "TVC",
        "30 s",
        "other",
        "videos",
        "social adver- tisements",
        "social advertisement stimuli",
        "social groups",
        "documentary film",
        "gaming video",
        "true response",
        "different purposes",
        "gender-related advertisements",
        "appli- cation",
        "cigarette commercials",
        "smoking cessa",
        "atten- tion",
        "substantial role",
        "advertisement industry",
        "gender-targeted marketing",
        "tion frames",
        "drama",
        "Neuromarketing",
        "The",
        "ads",
        "messages",
        "smokers",
        "celebrity",
        "endorsement",
        "Missaglia",
        "research"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 9.010597,
      "content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchical c",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1012456,
      "metadata_storage_name": "s40493-015-0015-3.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxNS0zLnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-19T12:42:36Z",
      "keyphrases": [
        "Creative Commons Attribution License",
        "mobile ad hoc networks",
        "Günther Pernul",
        "Universitätsstraße",
        "descriptive scenario-based analysis",
        "general prob- lem",
        "one novel idea",
        "Open Access article",
        "single building blocks",
        "RESEARCH Open Access",
        "online reputation systems",
        "new reputation systems",
        "hierarchical component taxonomy",
        "most reputation systems",
        "common reputation models",
        "Johannes Sänger",
        "hierarchical taxonomy",
        "common systems",
        "peer networks",
        "social networks",
        "component repository",
        "common models",
        "Christian Richthammer",
        "various disciplines",
        "application areas",
        "promising approaches",
        "implementation level",
        "obvious utility",
        "practical point",
        "last decade",
        "wide range",
        "considerable research",
        "data accuracy",
        "several environments",
        "statistical approaches",
        "personal preferences",
        "software engineering",
        "unrestricted use",
        "effective use",
        "computation engines",
        "computation phase",
        "graph-based models",
        "natural framework",
        "design process",
        "design knowledge",
        "Design approaches",
        "computation methods",
        "Reusable components",
        "Trust Management",
        "Trust pattern",
        "reputation-based trust",
        "context information",
        "original work",
        "Journal",
        "DOI",
        "Correspondence",
        "saenger",
        "wiwi",
        "regensburg",
        "University",
        "Germany",
        "Abstract",
        "problem",
        "scratch",
        "concepts",
        "achievements",
        "others",
        "shuffle",
        "reuse",
        "respect",
        "order",
        "conceptual",
        "view",
        "properties",
        "literature",
        "aspects",
        "Keywords",
        "Reusability",
        "Introduction",
        "metrics",
        "commerce",
        "eBay",
        "buyers",
        "relevance",
        "quality",
        "arithmetic",
        "multiple",
        "factors",
        "propagation",
        "solutions",
        "authors",
        "proposals",
        "development",
        "attention",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "mailto",
        "Page",
        "community",
        "efits",
        "specialists",
        "reliability",
        "goal",
        "classification",
        "functions",
        "many applied computation techniques",
        "design science research paradigm",
        "design pattern-like solution",
        "rational choice mechanism",
        "general problem context",
        "multifaceted terms trust",
        "computational trust models",
        "current models",
        "reputation models",
        "building block",
        "web services",
        "research gap",
        "objec- tives",
        "same time",
        "increasing distribution",
        "decision making",
        "network environments",
        "common understanding",
        "various fields",
        "multidimension- ality",
        "abstract concept",
        "behavioral dimension",
        "interpersonal phenomenon",
        "particular level",
        "subjective probability",
        "Sänger",
        "Multiple authors",
        "plex definitions",
        "reputation systems",
        "conceptual level",
        "following section",
        "important artifacts",
        "bility trust",
        "particular action",
        "future work",
        "several properties",
        "means",
        "rest",
        "paper",
        "guidelines",
        "Hevner",
        "overview",
        "motivation",
        "approach",
        "contribution",
        "name",
        "plans",
        "success",
        "Internet",
        "connectivity",
        "notion",
        "regard",
        "objectives",
        "topic",
        "decades",
        "uniform",
        "Reasons",
        "circumstance",
        "credibility",
        "confidence",
        "emotional",
        "nature",
        "sociologists",
        "psychologists",
        "Economists",
        "online",
        "Gambetta",
        "distrust",
        "agent",
        "group",
        "capacity",
        "security",
        "risk",
        "variety",
        "Table",
        "basis",
        "two common ways",
        "multiple discrete levels",
        "hard security mechanism",
        "One negative experience",
        "Propagative One property",
        "one aggregated picture",
        "many trust models",
        "multiple trust chains",
        "several trust models",
        "several models",
        "propagative nature",
        "aggregated value",
        "different chains",
        "recent years",
        "new experiences",
        "time-based aging",
        "greater importance",
        "old experiences",
        "delicious meal",
        "rela- tionships",
        "final decision",
        "greater influence",
        "distance-based aging",
        "subjective nature",
        "high level",
        "binary manner",
        "maximum value",
        "human nature",
        "hard evidence",
        "one agent",
        "other agents",
        "untrustworthy agents",
        "Dynamic Trust",
        "Context-dependent Trust",
        "trust value",
        "overall trust",
        "trust information",
        "trust statements",
        "lower trust",
        "Reflexive Trust",
        "specific context",
        "particular restaurant",
        "specific aspects",
        "long time",
        "particular agent",
        "distant nodes",
        "reviewer Rachel",
        "book review",
        "trust transitivity",
        "Policy-based trust",
        "same context",
        "values",
        "Table 1",
        "Overview",
        "acteristics",
        "example",
        "Alice",
        "Bob",
        "doctor",
        "cook",
        "customer",
        "food",
        "service",
        "combination",
        "amount",
        "use",
        "propagativity",
        "turn",
        "Claire",
        "Christianson",
        "Harbison",
        "transitive",
        "reverse",
        "case",
        "Composable",
        "Composition",
        "zon",
        "opinion",
        "numbers",
        "continuous",
        "variable",
        "interval",
        "reinforcing",
        "trustworthiness",
        "consideration",
        "establishment",
        "exchange",
        "credentials",
        "contrast",
        "history",
        "interactions",
        "estimation",
        "three process steps filtering",
        "most reputation-based trust models",
        "three fundamental phases",
        "repu- tation system",
        "function-based component classification",
        "current reputation models",
        "existing trust models",
        "three steps",
        "existing models",
        "component classes",
        "generic process",
        "two steps",
        "existing approaches",
        "existing systems",
        "soft security",
        "collective measure",
        "global value",
        "personal experience",
        "many approvedmodels",
        "promising thoughts",
        "generic mechanism",
        "compo- nent",
        "accepted principles",
        "one hand",
        "other hand",
        "critical question",
        "communica- tion",
        "feedback generation/collection",
        "feedback distribution",
        "central part",
        "common trust",
        "feedback aggregation",
        "existing concepts",
        "academic community",
        "context setting",
        "single components",
        "reusable components",
        "trust properties",
        "Research gap",
        "worthiness",
        "work",
        "thing",
        "character",
        "referrals",
        "ratings",
        "reviews",
        "members",
        "someone",
        "ideas",
        "benefits",
        "years",
        "Rehak",
        "instance",
        "capabilities",
        "sound",
        "new",
        "repository",
        "artifacts",
        "developers",
        "researchers",
        "novel",
        "way",
        "implemented",
        "communication",
        "ability",
        "table",
        "computationmethods",
        "analysis",
        "identification",
        "logical",
        "Figure",
        "preparation",
        "storage",
        "weighting",
        "trustor",
        "novel hierarchical component taxonomy",
        "common rep- utation systems",
        "three generic process steps",
        "two broad categories",
        "reputa- tion systems",
        "subsequent aggre- gation",
        "current trust models",
        "second process step",
        "single process steps",
        "Common reputation systems",
        "first two steps",
        "several reputation scores",
        "common examples",
        "subsequent computing",
        "utation information",
        "second case",
        "computation process",
        "trust relation",
        "first phase",
        "reputation value",
        "personalization parameters",
        "past behavior",
        "sonal experience",
        "other sources",
        "personal collections",
        "ferent peers",
        "distributed network",
        "different sources",
        "uniform format",
        "specific situation",
        "weight factors",
        "struc- ture",
        "decentralized environments",
        "functional blocks",
        "different surveys",
        "computation engine",
        "last phase",
        "first question",
        "filtering process",
        "hard selection",
        "soft selection",
        "secondary classes",
        "reputation data",
        "information amount",
        "extra information",
        "available data",
        "Preparation techniques",
        "preparation phase",
        "Figure 2 Classes",
        "public storage",
        "input data",
        "trustee",
        "context",
        "normalization",
        "output",
        "aggregation",
        "need",
        "processing",
        "line",
        "Zhang",
        "difference",
        "algo",
        "rithm",
        "structure",
        "decentralized/hybrid",
        "meaning",
        "transparency",
        "design",
        "section",
        "detail",
        "primary",
        "three broad classes attribute-based, statistic- based",
        "simple ballot stuffing attacks",
        "payment Sänger",
        "statistical filter technique",
        "one application area",
        "value imbalance problem",
        "Bayesian reputation systems",
        "cluster analysis approaches",
        "following classes",
        "Attribute-based filtering",
        "Attribute-based filters",
        "simple logic",
        "statistical patterns",
        "Clustering-based filter",
        "filtering step",
        "Other models",
        "filtering phase",
        "single attributes",
        "reference value",
        "huge amounts",
        "initial filtering",
        "weighting phase",
        "last 12 months",
        "positive, neutral",
        "Statistic-based filtering",
        "false rumors",
        "exemplary procedure",
        "cluster filtering",
        "available information",
        "current situation",
        "various data",
        "different factors",
        "different prices",
        "product types",
        "malicious seller",
        "cheap products",
        "many current",
        "reputation calculation",
        "Reputation data",
        "high reputation",
        "negative ratings",
        "positive ratings",
        "unfair ratings",
        "false ratings",
        "Further techniques",
        "majority rule",
        "filtering techniques",
        "weighting techniques",
        "false feedback",
        "other parties",
        "common experience",
        "fake transactions",
        "obvious way",
        "crucial attribute",
        "Context comparability",
        "constraint-factor",
        "lightweight",
        "Time",
        "Sporas",
        "party",
        "robustness",
        "spread",
        "Whitby",
        "dishonest",
        "advisor",
        "rater",
        "iCLUB",
        "clusters",
        "evaluations",
        "bootstrapping",
        "Dellarocas",
        "characteristics",
        "reason",
        "discounting",
        "methods",
        "non",
        "expensive",
        "situations",
        "1.",
        "several trust/reputa- tion values",
        "biometric identity trust model",
        "human decision makers",
        "adaptive forgetting scheme",
        "distinct information sources",
        "common aggregation techniques",
        "one important factor",
        "different application areas",
        "simple aggregation techniques",
        "low feedback reputation",
        "reputation values",
        "forgetting factor",
        "low impact",
        "Simple arithmetic",
        "critical factor",
        "first-hand information",
        "Different actors",
        "different perceptions",
        "aggregation process",
        "aggregated reputation",
        "good reputation",
        "reputation scoring",
        "Reputation systems",
        "file-sharing networks",
        "successful transaction",
        "customer satisfaction",
        "network structures",
        "transitivity) rate",
        "honest feedback",
        "recursive algorithm",
        "stronger punishment",
        "bad behavior",
        "good behavior",
        "dynamic nature",
        "Old feedback",
        "Jøsang",
        "Personal preferences",
        "various end-users",
        "direct experience",
        "weight-factor(s",
        "third step",
        "iterative manner",
        "single steps",
        "first class",
        "Criteria comparability",
        "different communities",
        "Rating value",
        "trust chain",
        "Other distinctions",
        "propagation degree",
        "web graph",
        "example measure",
        "time-based weighting",
        "new referrals",
        "agents’ reliability",
        "approaches",
        "evaluation",
        "costs",
        "level",
        "anonymity",
        "number",
        "peers",
        "differences",
        "recommendation",
        "second",
        "distance",
        "reputation-factor",
        "nodes",
        "honesty",
        "concept",
        "consequence",
        "bases",
        "calculation",
        "Google",
        "PageRank",
        "position",
        "website",
        "Epinions",
        "reviewers",
        "effects",
        "Sun",
        "adaptation",
        "importance",
        "significance",
        "newcomers",
        "tuple",
        "input",
        "phase",
        "cases",
        "list",
        "algorithms",
        "fuzzy",
        "2.",
        "4.",
        "5.",
        "beta PDF function parameter tuple",
        "human decision making process",
        "beta probability density function",
        "solid mathematical basis",
        "More complex solutions",
        "The beta PDF",
        "Beta Reputation system",
        "exact reputation score",
        "priori reputation score",
        "online social networks",
        "Other aggregation techniques",
        "Bayesian probabilistic models",
        "Hidden Markov Model",
        "graph-based flow model",
        "prominent trust models",
        "beta model",
        "Bayesian probability",
        "probability distributions",
        "probabilistic approach",
        "Markov Models",
        "Applied techniques",
        "graph-based approach",
        "belief model",
        "model truth",
        "Reputation values",
        "overall reputation",
        "basic way",
        "descending order",
        "message boards",
        "citation counts",
        "academic literature",
        "understandable algorithm",
        "last years",
        "statistical approach",
        "binary events",
        "Dempster-Shafer theory",
        "subjective logic",
        "machine learning",
        "dynamic behavior",
        "classical logic",
        "linguistic approach",
        "prominent example",
        "graph theory",
        "incoming edges",
        "outgoing edges",
        "several factors",
        "Popular algorithms",
        "Eigentrust Algorithm",
        "many models",
        "fuzzy logic",
        "trust management",
        "transitive trust",
        "trust chains",
        "fuzzy models",
        "new ratings",
        "Other examples",
        "ranking algorithms",
        "impact factor",
        "different measures",
        "one edge",
        "two nodes",
        "summation",
        "average",
        "proxy",
        "systems",
        "Slashdot",
        "implementation",
        "Amazon",
        "positive",
        "composition",
        "result",
        "weakness",
        "uncertainty",
        "DST",
        "Yu",
        "Singh",
        "generalization",
        "Malik",
        "falsity",
        "degree",
        "agent/resource",
        "REGRET",
        "information",
        "people",
        "field",
        "centrality",
        "Eigenvector",
        "betweenness",
        "α",
        "β",
        "reputa- tion system",
        "single logical components",
        "aggregation phases",
        "incremental nature",
        "one component",
        "primary class",
        "aggregation techniques",
        "hybrid model",
        "new classes",
        "novel models",
        "hierarchical c",
        "taxonomy",
        "heuristic",
        "knowledge",
        "extension"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 8.577549,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-s",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 548509,
      "metadata_storage_name": "s13640-020-00545-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMjAtMDA1NDUtei5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "metadata_creation_date": "2021-01-12T23:22:39Z",
      "keyphrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "doi.org",
        "orcid.org",
        "commerce platforms",
        "Correspondence",
        "shaoshuaib",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "BERT",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "creativecommons",
        "licenses",
        "crossmark",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "BERT model",
        "Related work",
        "present stage",
        "good results",
        "extraction capacity",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social information Preprocessing",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "Background processing",
        "subsequent processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "characteristic dimension",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "Data label",
        "tf matrix",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "dividend",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "BERT-Base",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "Text classification BERT fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "Text classification fine-tuning",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "official recommended values",
        "feature matching scheme",
        "text information token",
        "full connection layer",
        "additional 9500 text data",
        "natural language processing",
        "machine learning scheme",
        "word segmentation process",
        "social information data",
        "5.2 Classification scheme",
        "text length",
        "text description",
        "input BERT",
        "first token",
        "TF-IDF model",
        "classification problem",
        "deep learning",
        "learning rate",
        "TF-IDF-based model",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "commodities",
        "social e-commerce market",
        "standard description text",
        "social e-commerce environment",
        "knowledge distillation technology",
        "standard product names",
        "large-scale data marking",
        "social e-commerce classification",
        "model recognition rate",
        "product information",
        "test data",
        "text-based classification",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "high correlation",
        "work",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "semi"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 7.504171,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relatives",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2986829,
      "metadata_storage_name": "s40493-015-0019-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxOS16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Partheeban Chandrasekaran",
      "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "metadata_creation_date": "2015-09-04T09:59:41Z",
      "keyphrases": [
        "new classi- fication scheme",
        "peer file-sharing applications",
        "social networking websites",
        "comprehensive test tool",
        "Attribution 4.0 International License",
        "original author(s",
        "three trust algorithms",
        "various trust properties",
        "computational trust mechanisms",
        "online community-based systems",
        "Creative Commons license",
        "computational trust models",
        "social trust models",
        "three reputation systems",
        "different trust models",
        "RESEARCH Open Access",
        "trust assessment workflow",
        "existing reputation systems",
        "existing models",
        "Many models",
        "different stages",
        "Trust Management",
        "trust relationship",
        "Multi-agent systems",
        "Computer Engineering",
        "Introduction Motivation",
        "simple averaging",
        "based scores",
        "Advogato website",
        "application-dependent metrics",
        "transformation stage",
        "unrestricted use",
        "appropriate credit",
        "first contribution",
        "second contribution",
        "evaluation schemes",
        "Trust testbed",
        "Esfandiari Journal",
        "latest model",
        "single model",
        "generic testbed",
        "increasing need",
        "past transactions",
        "graph transformations",
        "Carleton University",
        "larger number",
        "Partheeban Chandrasekaran",
        "slandering attacks",
        "two agents",
        "Babak Esfandiari",
        "DOI",
        "experiments",
        "analysis",
        "Correspondence",
        "Department",
        "1125 Colonel",
        "Drive",
        "Ottawa",
        "Ontario",
        "Canada",
        "Abstract",
        "tesbed",
        "flexibility",
        "design",
        "prototype",
        "EigenTrust",
        "PeerTrust",
        "Appleseed",
        "vulnerabilites",
        "compliance",
        "example",
        "discrepancies",
        "trade-offs",
        "resistance",
        "self-promotion",
        "Keywords",
        "growth",
        "users",
        "history",
        "interactions",
        "calculations",
        "ratings",
        "eBay",
        "researcher",
        "tools",
        "evaluations",
        "set",
        "vulnerabilities",
        "types",
        "Overview",
        "solution",
        "contributions",
        "paper",
        "family",
        "process",
        "succession",
        "vertices",
        "meaning",
        "edges",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "crossmark",
        "org",
        "mailto",
        "Page",
        "presence",
        "existence",
        "variety",
        "certificate authority-based PKI trust models",
        "Trust management systems aid agents",
        "reputation-based trust management systems",
        "three different trust algorithms",
        "public key infras",
        "Social trust models",
        "basic trust properties",
        "social trust algorithms",
        "existing trust algorithms",
        "Beta Reputation System",
        "three systems",
        "reputation systems",
        "chaining algorithms",
        "mutual trust",
        "trust assertions",
        "trust score",
        "existing testbeds",
        "subtle interplay",
        "higher sensitivity",
        "lower sensitivity",
        "literature review",
        "implementation details",
        "actual mechanisms",
        "earlier direct",
        "indirect interactions",
        "e-commerce website",
        "third party",
        "Various inputs",
        "file-sharing network",
        "blogging website",
        "particular level",
        "specific context",
        "simple attacks",
        "self-promoting attacks",
        "Problem description",
        "research problem",
        "particular agent",
        "one attack",
        "friend chain",
        "past experiences",
        "testbed prototype",
        "aggregated trustworthiness",
        "evaluation results",
        "range",
        "differences",
        "way",
        "slandering",
        "Organization",
        "section",
        "Background",
        "state",
        "discussion",
        "Conclusions",
        "limitations",
        "tructures",
        "certificates",
        "The",
        "means",
        "order",
        "instance",
        "sellers",
        "trustworthy",
        "product",
        "drawbacks",
        "Chandrasekaran",
        "root",
        "recommendations",
        "turn",
        "recommenders",
        "Nature",
        "TRAVOS",
        "BRS",
        "satisfaction",
        "transaction",
        "P2P",
        "Aberer",
        "Despotovic",
        "plaints",
        "Advogato",
        "goal",
        "spam",
        "community",
        "TidalTrust",
        "beta probability density function",
        "late indirect trust scores",
        "other agents’ past experiences",
        "Spreading Activation model",
        "local trust scores",
        "global trust algorithms",
        "global trust score",
        "local trust algorithms",
        "three algorithms",
        "general trust",
        "Eigen- Trust",
        "Trust decisions",
        "similarity score",
        "file authenticity",
        "recommenda- tions",
        "different inputs",
        "thresholding techniques",
        "edge weights",
        "maximum “flow",
        "direct experience",
        "party recommendations",
        "different families",
        "next sections",
        "detailed descriptions",
        "respective sections",
        "satisfaction ratings",
        "trust graph",
        "direct interactions",
        "truster agent",
        "Credence",
        "votes",
        "files",
        "trustee",
        "gossiping",
        "information",
        "perspective",
        "survey",
        "Marsh",
        "Abdul-Rahman",
        "case",
        "percentage",
        "top",
        "value",
        "energy",
        "tor",
        "short",
        "third",
        "one",
        "scope",
        "testbed",
        "details",
        "output",
        "readers",
        "high local trust values",
        "low local trust values",
        "local direct trust score",
        "normalized local trust score",
        "global trust score computations",
        "current trust scores",
        "trans- action ratings",
        "local trust vector",
        "global trust scores",
        "fellow malicious agents",
        "gence threshold Tc",
        "global value",
        "trust relationships",
        "trust propagation",
        "trust graphs",
        "flow-based algorithm",
        "trust seed",
        "path length",
        "termination condition",
        "weighted graph",
        "Tij trij",
        "cij elements",
        "factor decay",
        "decay factor",
        "honest agents",
        "agent u",
        "agent j",
        "satisfaction rating",
        "transaction rating",
        "raters",
        "Eq.",
        "transactions",
        "system",
        "method",
        "input",
        "sij",
        "sum",
        "truster",
        "friends",
        "cijcjk",
        "matrix",
        "tik",
        "opinion",
        "number",
        "hops",
        "i.",
        "rank",
        "nodes",
        "trustworthiness",
        "sink",
        "amount",
        "places",
        "loops",
        "Ai",
        "step",
        "∑",
        "∈",
        "open-source message- driven simulation engine",
        "two different targets",
        "two testbed implementations",
        "document recommendation systems",
        "beta probability distribution",
        "local trust algorithm",
        "reputation estimation errors",
        "two testbed models",
        "general-purpose trust models",
        "previous reputation value",
        "art painting sales",
        "general trust systems",
        "The Agent Reputation",
        "beta distribution",
        "two roles",
        "initial value",
        "real systems",
        "trust assessment",
        "brief survey",
        "important role",
        "case studies",
        "direct feedbacks",
        "current reputation",
        "single fixpoint",
        "important stage",
        "one trustworthiness",
        "dom variables",
        "various attacks",
        "particular era",
        "market values",
        "specific era",
        "other eras",
        "reputation scores",
        "trust algorithms",
        "next section",
        "rating function",
        "desirable properties",
        "negative experiences",
        "mutual ratings",
        "target pairing",
        "other agents",
        "Guha Guha",
        "seed",
        "methods",
        "look",
        "andMacau",
        "TREET",
        "investigation",
        "graph",
        "documents",
        "many",
        "specialization",
        "subclass",
        "Hazard",
        "Singh",
        "authors",
        "rater",
        "favor",
        "payoff",
        "belief",
        "likelihood",
        "future",
        "definitions",
        "Monotonicity",
        "computed",
        "b.",
        "Unambiguity",
        "convergence",
        "time",
        "Accuracy",
        "total",
        "update",
        "positive",
        "comparison",
        "resilience",
        "performance",
        "domain",
        "client",
        "paintings",
        "appraisals",
        "expert",
        "dedicated distributed infras- tructure",
        "online product review websites",
        "trust score rep- resentation",
        "graphical user interface",
        "first person point",
        "P2P file-sharing networks",
        "honest sales ratio",
        "trust calculation schemes",
        "decentralized trust algorithms",
        "highest bank balance",
        "Value Imbalance attack",
        "general marketplace scenario",
        "various reputation systems",
        "Reputation Lag attack",
        "honest profit ratio",
        "multiple buyer accounts",
        "Multiple seller accounts",
        "The ART testbed",
        "distributed version",
        "utility value",
        "Proliferation attack",
        "cheater sales",
        "other systems",
        "marketplace domain",
        "Reputation Experimentation",
        "trust mechanism",
        "trust management",
        "The Trust",
        "future interactions",
        "possible messages",
        "time interval",
        "The engine",
        "new clients",
        "correct ability",
        "problem domain",
        "Evaluation Testbed",
        "varying prices",
        "inexpensive items",
        "market competition",
        "selling price",
        "future transactions",
        "following metrics",
        "evaluation metrics",
        "simulation engine",
        "simulation days",
        "trustworthy agents",
        "right agents",
        "participating agents",
        "different agents",
        "centralized version",
        "sale price",
        "random number",
        "collusion attacks",
        "winning agent",
        "1,000 different products",
        "departing agent",
        "protocol",
        "Themessages",
        "track",
        "results",
        "database",
        "GUI",
        "runtime",
        "platform",
        "service",
        "cooperation",
        "additional",
        "model",
        "buyers",
        "influence",
        "cost",
        "purchase",
        "probability",
        "initialization",
        "offers",
        "feedback",
        "experience",
        "White-Washing",
        "Self-Promoting",
        "limitation",
        "restriction",
        "others",
        "knowledge",
        "100",
        "many trust management systems",
        "obtain feedback history graph",
        "discrete event simulator",
        "generic evaluation metrics",
        "particular trust algorithm",
        "A Reputation Graph",
        "trust assessment process",
        "Existing trust models",
        "new systems",
        "decision-making process",
        "evaluation framework",
        "labelled graph",
        "agent A",
        "Summary Trust",
        "social trust",
        "trust properties",
        "nth-hand trust",
        "application domain",
        "rational decisions",
        "abstraction layer",
        "n feedbacks",
        "ith satisfaction",
        "different ranges",
        "labelled arcs",
        "other things",
        "next step",
        "transitive closure",
        "target agent",
        "agent B",
        "indirect feedback",
        "feedback histories",
        "following stages",
        "past behavior",
        "R N",
        "common set",
        "final stage",
        "abstract model",
        "feedback value",
        "output requirements",
        "R.",
        "tool",
        "manymodels",
        "attacks",
        "andmodel",
        "developers",
        "foundation",
        "expectation",
        "individual",
        "observations",
        "estimation",
        "combination",
        "rest",
        "graphs",
        "group",
        "actions",
        "list",
        "FHG",
        "downloader",
        "uploader",
        "values",
        "timestamps",
        "directed",
        "second",
        "E.",
        "∅",
        "single reputa- tion score",
        "absolute global reputation score",
        "new classification scheme",
        "continuous reputation values",
        "global reputation scores",
        "existing classification criteria",
        "stage transi- tions",
        "top k agents",
        "local reputation scores",
        "obtain trust graph",
        "Global algorithms",
        "existing literature",
        "one scores",
        "Reputation algorithms",
        "particular task",
        "two groups",
        "clar- ity",
        "Two methods",
        "trust models",
        "above sections",
        "Apple- seed",
        "other hand",
        "same codomain",
        "other words",
        "reputation graph",
        "various algorithms",
        "different algorithms",
        "edge labels",
        "local algorithms",
        "other method",
        "intermediate graph",
        "reflexive property",
        "incoming arcs",
        "among agents",
        "ranking algorithms",
        "One method",
        "trusting agent",
        "EigenTrust outputs",
        "one stage",
        "looping",
        "degree",
        "Figs",
        "weights",
        "Fig.",
        "Examples",
        "decision",
        "TG",
        "ourmodel",
        "stages",
        "workflow",
        "Classifying",
        "RG",
        "nisms",
        "threshold",
        "transitions",
        "addition",
        "Table",
        "semantics",
        "relative",
        "Trust Algorithm Transitions Input",
        "satisfaction local absoluteratings Ranking",
        "continuous feedback values",
        "trust models Stage",
        "graph transformation function",
        "Stage transitions",
        "global absoluteratings",
        "local absolutescores",
        "Trust algorithms",
        "functional view",
        "evaluation mechanisms",
        "global relativeratings",
        "N/A relatives",
        "Reputation Scores",
        "experimenter",
        "inputs",
        "outputs",
        "classification",
        "AppleSeed",
        "3 complaints",
        "3 certificates"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 7.0367537,
      "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\n\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\n\nn\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C)−H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN (AD − CB)2\n\n(A+ C)(B+ D)(A+ B)(C + D)\n\n(5)χ2(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf\n+ is the number of documents in the positive class that contain feature f, DFf\n\n− \nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and mf, mc\n\nf are \nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc −mf\n\n)2\n\n(8)mc =\n\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1− qf\n)\n\n(\n\n1− pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf\n+ and DFf\n\n− are the raw counts of documents that contain f in the positive and \nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs. 12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5 GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table 2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nTu\nrk\n\nis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\no",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1280780,
      "metadata_storage_name": "s13673-018-0135-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTgtMDEzNS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tuba Parlar ",
      "metadata_title": "QER: a new feature selection method for sentiment analysis",
      "metadata_creation_date": "2018-04-18T08:33:56Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "naïve Bayes multinomial classifier",
        "Selma Ayşe Özel2",
        "natural language processing task",
        "Text classification Open Access",
        "document frequency difference methods",
        "new feature selection method",
        "Creative Commons license",
        "other feature selection methods",
        "query expansion ranking",
        "query expansion term",
        "optimal orthogonal centroid",
        "four classi- fiers",
        "support vector machines",
        "third class label",
        "decision making process",
        "two-class classification problem",
        "original author(s",
        "Internet World Stats",
        "review docu- ments",
        "Hum. Cent. Comput",
        "statistical methods",
        "learning methods",
        "senti- ments",
        "decision trees",
        "classification accuracy",
        "sentiment classification",
        "author information",
        "Tuba Parlar1",
        "Fei Song3",
        "important piece",
        "social media",
        "top 20 countries",
        "highest numbers",
        "Internet users",
        "exploding growth",
        "main reasons",
        "low ranks",
        "multiple kinds",
        "product reviews",
        "Machine learning",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Parlar",
        "Inf. Sci.",
        "Kemal University",
        "Full list",
        "ferent approaches",
        "data preprocessing",
        "other people",
        "The Author",
        "classification performance",
        "opinionated information",
        "information retrieval",
        "Chi square",
        "information gain",
        "sentiment analysis",
        "ent languages",
        "noisy features",
        "doi.org",
        "orcid.org",
        "irrelevant features",
        "major source",
        "online documents",
        "many researchers",
        "different domains",
        "Turkish reviews",
        "English languages",
        "QER",
        "Introduction",
        "most",
        "opinions",
        "Turkey",
        "area",
        "SA",
        "general",
        "internetworldstats",
        "Abstract",
        "sentiments",
        "order",
        "non-informative",
        "study",
        "field",
        "ling",
        "movie",
        "performances",
        "classifiers",
        "results",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "tparlar",
        "mku",
        "1 Department",
        "Mathematics",
        "Mustafa",
        "Antakya",
        "Hatay",
        "end",
        "crossmark",
        "crossref",
        "Page",
        "19Parlar",
        "studies",
        "query expansion term weighting methods",
        "other common feature selection methods",
        "ten different review documents datasets",
        "maximum entropy mod- elling",
        "naïve Bayes multinomial",
        "traditional topic-based text classification",
        "good feature selection",
        "information retrieval systems",
        "Natural Language Processing",
        "machine learning methods",
        "text classification task",
        "document frequency difference",
        "machine learning approaches",
        "four text classifiers",
        "kitchen appliances reviews",
        "different document domains",
        "Related work SA",
        "feature space",
        "feature sizes",
        "training datasets",
        "different reviews",
        "classification accu",
        "particular review",
        "related attributes",
        "different languages",
        "LCD screen",
        "sentiment-expressing words",
        "experimental settings",
        "important topic",
        "Artificial Intelligence",
        "opinion mining",
        "useful applications",
        "negative ratings",
        "Hum. Cent",
        "popular implementation",
        "movie reviews",
        "book reviews",
        "search performance",
        "performance measures",
        "testing results",
        "Numerous studies",
        "SA problems",
        "overall sentiments",
        "non-informative features",
        "great pictures",
        "NB",
        "SVM",
        "DT",
        "MEM",
        "differences",
        "example",
        "camera",
        "people",
        "breath",
        "observation",
        "account",
        "paper",
        "dimensionality",
        "aim",
        "texts",
        "Turkish",
        "English",
        "effectiveness",
        "DFD",
        "OCFS",
        "goal",
        "racy",
        "respect",
        "rest",
        "Experiments",
        "Conclusion",
        "evalua",
        "emotions",
        "entities",
        "products",
        "services",
        "organizations",
        "als",
        "issues",
        "events",
        "kind",
        "many",
        "popularity",
        "user",
        "companies",
        "businesses",
        "Comput",
        "models",
        "cally",
        "Pang",
        "researchers",
        "effects",
        "Yang",
        "Yu",
        "C4.5",
        "Tabu heuristic search-enhanced Markov blanket model",
        "karşılaştırmalısın",
        "information gain feature selection method",
        "document frequency feature selection method",
        "minimum redundancy maximum relevancy",
        "Weighted Log Likelihood Ratio",
        "Weka [33] data mining tool",
        "DFD feature selection method",
        "reduced feature sizes",
        "Machine learning algorithms",
        "product review dataset",
        "Chinese online reviews",
        "movie review dataset",
        "The Turkish language",
        "feature sets",
        "Boynukalın",
        "n words",
        "lish reviews",
        "product datasets",
        "execution time",
        "large number",
        "comparative experiments",
        "significant improvement",
        "Altaic branch",
        "Ural-Altaic family",
        "agglutinative language",
        "longer sentence",
        "ture space",
        "emotional analysis",
        "initial version",
        "DT classifiers",
        "statistical analysis",
        "IG methods",
        "word relation",
        "sentiment features",
        "related features",
        "unigram features",
        "single word",
        "one classifier",
        "SVM classifiers",
        "two types",
        "high accuracy",
        "95% average accuracy",
        "English language",
        "Complementary NB",
        "MEM classifier",
        "English datasets",
        "SA studies",
        "Nicholls",
        "difference",
        "Agarwal",
        "mRMR",
        "NBM",
        "Abbasi",
        "EWGA",
        "Xia",
        "design",
        "Bai",
        "vocabulary",
        "Mladenovic",
        "mapping",
        "Zheng",
        "languages",
        "Republic",
        "Finnish",
        "garian",
        "instance",
        "something",
        "teristics",
        "WLLR",
        "combinations",
        "n-grams",
        "sequences",
        "Akba",
        "CHI2",
        "85.",
        "92",
        "optimal orthogonal cen- troid",
        "new query expansion ranking",
        "traditional text classification tasks",
        "several feature selection methods",
        "common feature selection methods",
        "high information gain scores",
        "follow- ing reasons",
        "decision tree classifier",
        "Turkish review datasets",
        "English review datasets",
        "good classification performance",
        "large vocabulary sizes",
        "many classification problems",
        "J48 classification methods",
        "total feature sizes",
        "Chi square score",
        "Such methods",
        "higher score",
        "low score",
        "feature f",
        "four classifiers",
        "tain measures",
        "same time",
        "effec- tiveness",
        "distinct values",
        "same way",
        "contingency table",
        "total number",
        "related class",
        "class c",
        "other class",
        "many documents",
        "valuable features",
        "previous study",
        "different classes",
        "P(aj",
        "m number",
        "probability distribution",
        "entropy values",
        "attribute value",
        "experiments",
        "SMO",
        "LR",
        "specific",
        "efficiency",
        "range",
        "500 increments",
        "improvement",
        "content",
        "power",
        "tainty",
        "Hum",
        "Cent",
        "definitions",
        "reviews",
        "positive",
        "egories",
        "keyword",
        "formulas",
        "dependence",
        "3000",
        "The Chi square statistics",
        "Query expansion ranking method",
        "probabilistic weighting model",
        "similar document frequencies",
        "high dimensional vectors",
        "Document frequency difference",
        "query expansion methods",
        "query expansion technique",
        "Optimal orthogonal centroid",
        "orthogonal centroid algorithm",
        "many sentiment-expressing features",
        "one specific class",
        "document vectors",
        "CHI2 method",
        "bigger difference",
        "document collection",
        "One problem",
        "One limitation",
        "new terms",
        "original query",
        "expanded query",
        "big impact",
        "H(C",
        "+ D",
        "big issue",
        "sentiment word",
        "optimized form",
        "Docu- ments",
        "age vector",
        "DFD method",
        "high scores",
        "centroid m",
        "OCFS method",
        "negative class",
        "rare features",
        "individual review",
        "xi N",
        "positive class",
        "highest scores",
        "Nc N",
        "relevant documents",
        "equal number",
        "classes",
        "dataset",
        "text",
        "|A",
        "aj",
        "CB",
        "classification",
        "Song",
        "DFf",
        "weights",
        "importance",
        "subset",
        "sum",
        "distances",
        "subspace",
        "mc",
        "mf",
        "values",
        "centroids",
        "IR",
        "Scoref",
        "Harman",
        "∑",
        "χ",
        "tional topic-based text classification",
        "QER feature selection method",
        "information retrieval strategy",
        "statistical language processing",
        "informa- tion gain",
        "feature selection methods",
        "efficient feature selector",
        "information retrieval system",
        "score computation method",
        "k new terms",
        "classification process",
        "score formula",
        "ranking score",
        "k terms",
        "high scored",
        "Sparck Jones",
        "raw counts",
        "negative classes",
        "small constants",
        "similar constants",
        "data smoothing",
        "fea- ture",
        "multiple classes",
        "two classes",
        "greedy approach",
        "related classes",
        "high values",
        "distance/similarity measures",
        "document frequencies",
        "Q. Equation",
        "term f",
        "negative documents",
        "related documents",
        "valuable terms",
        "top-k scores",
        "ability scores",
        "compute scores",
        "lowest scores",
        "zero probabilities",
        "query Q",
        "probability calculations",
        "sentiment-expressing features",
        "positive documents",
        "precision",
        "man",
        "set",
        "pf",
        "qf",
        "Robertson",
        "Eq.",
        "ratio",
        "Eqs",
        "log2",
        "DF",
        "N−",
        "numbers",
        "numerators",
        "denominators",
        "CHI",
        "collection",
        "vectors",
        "review",
        "introduction",
        "1−",
        "2.5 GHz Intel Core i7 processor",
        "The English movie review dataset",
        "The Turkish product review dataset",
        "The Turkish movie reviews",
        "English product review dataset",
        "English product reviews",
        "multi-fold cross validations",
        "fivefold cross validation",
        "Turkish product reviews",
        "same dataset sizes",
        "punctua- tion marks",
        "five F measures",
        "large feature space",
        "average F measures",
        "final classification results",
        "English reviews",
        "final P",
        "classification system",
        "five folds",
        "punctuation marks",
        "question marks",
        "addi- tion",
        "five portions",
        "reduc- tion",
        "978 negative reviews",
        "700 negative reviews",
        "1000 negative reviews",
        "feature spaces",
        "four subsets",
        "elec- tronics",
        "kitchen appliances",
        "existing work",
        "similar datasets",
        "composite score",
        "harmonic mean",
        "Experimental settings",
        "MacBook Pro",
        "16 GB 1600 MHz",
        "NLTK [38] library",
        "case normalization",
        "docu- ments",
        "two probabilities",
        "commerce website",
        "Performance evaluation",
        "classified items",
        "LR classifier",
        "document",
        "quencies",
        "beyazperde",
        "1057 positive",
        "hepsiburada",
        "books",
        "DVDs",
        "700 positive",
        "Lee",
        "1000 positive",
        "Blitzer",
        "electronics",
        "recall",
        "Python",
        "words",
        "exclamation",
        "polarity",
        "suffixes",
        "J48",
        "MEM.",
        "couple",
        "thousands",
        "training",
        "remaining",
        "testing",
        "Table",
        "memory",
        "features",
        "NBM SVM J48 LR Movie",
        "NBM SVM J48 LR Features",
        "five folds cross validation",
        "six fea- ture sizes",
        "smaller fea- ture sizes",
        "five feature selection methods",
        "best micro-average F measure",
        "bigger feature sizes",
        "post hoc tests",
        "desirable size n",
        "senti- ment analysis",
        "electronic review dataset",
        "topical text classification",
        "kitchen review datasets",
        "best classification results",
        "electronics review dataset",
        "Table 1 Baseline results",
        "NBM classifier",
        "best performer",
        "top-ranked features",
        "new method",
        "best results",
        "text classifiers",
        "possible improvements",
        "total features",
        "related formulas",
        "ing classifier",
        "same performance",
        "different documents",
        "kitchen reviews",
        "univariate ANOVA",
        "significant differences",
        "three groups",
        "95% confidence level",
        "two groups",
        "new results",
        "baseline systems",
        "low frequencies",
        "same intuition",
        "baselines",
        "Book",
        "scores",
        "settings",
        "120 experiments",
        "pairs",
        "bold-face",
        "Note",
        "expressing",
        "traditional",
        "Cai",
        "2×",
        "C FS C H I2",
        "text classifi- cation",
        "traditional topic-based text",
        "Q ER D",
        "review datasets",
        "QER method",
        "One reason",
        "decision-tree-based solution",
        "smaller set",
        "ss ifi",
        "ze F",
        "evant features",
        "result",
        "classifier",
        "rs",
        "rk",
        "FD",
        "IG"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 6.772005,
      "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2469691,
      "metadata_storage_name": "s41109-020-00330-x.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MTEwOS0wMjAtMDAzMzAteC5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": " Shun Kodate ",
      "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network",
      "metadata_creation_date": "2020-11-12T15:20:34Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Computational social science Open Access",
        "other third party material",
        "eight local network indices",
        "various online transaction platforms",
        "Applied Network Science",
        "Creative Commons licence",
        "random forest classifiers",
        "Appl Netw Sci",
        "eight network indices",
        "present descriptive analysis",
        "credit card fraud",
        "Network analysis",
        "present study",
        "problematic transaction",
        "appropriate credit",
        "credit line",
        "online marketplaces",
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "rapid growth",
        "electronic transactions",
        "communi- cations",
        "dramatic speed",
        "daily lives",
        "UK Parliament",
        "recent era",
        "money laundering",
        "computer intrusion",
        "illegal items",
        "fictive items",
        "typical approach",
        "individual transactions",
        "traditional approach",
        "corresponding buyer",
        "several hundreds",
        "similar number",
        "focal node",
        "twelve features",
        "four types",
        "classification performance",
        "Machine learning",
        "author(s",
        "statutory regulation",
        "copyright holder",
        "iveco mmons",
        "RESEARCH Kodate",
        "Full list",
        "malicious users",
        "fraudulent users",
        "normal users",
        "online consumer",
        "intended use",
        "permitted use",
        "doi.org",
        "orcid.org",
        "Fraud detection",
        "consumer marketplace",
        "egocentric networks",
        "author information",
        "user profiles",
        "fraud activity",
        "Introduction",
        "tandem",
        "cybercrimes",
        "billions",
        "dollars",
        "year",
        "security",
        "society",
        "McAfee",
        "system",
        "Anderson",
        "dimension",
        "ranges",
        "plagiarism",
        "Abstract",
        "Providers",
        "behavior",
        "texts",
        "background",
        "frauds",
        "seller",
        "edge",
        "up",
        "connectivity",
        "neighbors",
        "aim",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "original",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "naokimas",
        "buffalo",
        "4 Department",
        "Mathematics",
        "University",
        "USA",
        "creativecommons",
        "licenses",
        "crossmark",
        "dialog",
        "Page",
        "18Kodate",
        "13 million monthly active users",
        "credit card systems",
        "social security system",
        "various other fields",
        "online review forums",
        "large online consumer",
        "machine learning algorithms",
        "online social networks",
        "mobile phone network",
        "credit card transactions",
        "suspicious connectivity patterns",
        "online auction system",
        "fraud detec- tion",
        "fraud detection techniques",
        "online C2C marketplaces",
        "online auctions",
        "supervised learning",
        "telecommunica- tion",
        "transaction networks",
        "C2C) marketplace",
        "Standard practice",
        "transaction amount",
        "item category",
        "call duration",
        "call type",
        "geographical region",
        "transaction history",
        "advanced fraudsters",
        "Google LLC",
        "particular words",
        "anomalous behavior",
        "alternative way",
        "graph-theoretic quantities",
        "Dreżewski",
        "car- booking",
        "Van Vlasselaer",
        "cryptocurrency transaction",
        "accom- plices",
        "bipartite cores",
        "belief propagation",
        "133 billion yen",
        "1.2 billion USD",
        "quarter year",
        "previous studies",
        "anomalous users",
        "fraudulent samples",
        "health-care data",
        "empirical data",
        "data set",
        "statistical methods",
        "transaction frauds",
        "reputation frauds",
        "statistical classifier",
        "one jargon",
        "Exemplar features",
        "Computational",
        "decades",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "case",
        "day",
        "week",
        "address",
        "number",
        "calls",
        "age",
        "gender",
        "Akoglu",
        "eyes",
        "administrators",
        "authorities",
        "signature",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "example",
        "authority",
        "drug",
        "idea",
        "nodes",
        "goods",
        "scores",
        "expectation",
        "insurance",
        "Šubelj",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "advertising",
        "Ferrara",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Monamo",
        "reputations",
        "authors",
        "eBay",
        "Mercari",
        "Japan",
        "Many prior network-based fraud detection algorithms",
        "online C2C marketplace service",
        "one problematic sell",
        "local clustering coefficient",
        "local infor- mation",
        "two fraudulent users",
        "two normal users",
        "two indices",
        "sell probability",
        "local indices",
        "two users",
        "local features",
        "connected components",
        "Yan- chun",
        "commercial implementations",
        "various items",
        "United States",
        "Japanese market",
        "following types",
        "non-existing items",
        "medicinal supplies",
        "weighted network",
        "egocentric network",
        "eight indices",
        "trans- actions",
        "separate index",
        "three indices",
        "informa- tion",
        "users’ network",
        "fictive item",
        "node vi",
        "node strength",
        "methods Data",
        "normal transactions",
        "lematic transactions",
        "mean number",
        "focal user",
        "Fictive transactions",
        "directed edge",
        "networks",
        "global",
        "communities",
        "betweenness",
        "k-cores",
        "Bhat",
        "Savage",
        "Others",
        "degree",
        "triangles",
        "Materials",
        "July",
        "January",
        "addition",
        "underwear",
        "medicine",
        "weapon",
        "perspective",
        "morality",
        "hygiene",
        "law",
        "crime",
        "Table",
        "buyer",
        "Figure",
        "1a",
        "Fig.",
        "edges",
        "pairs",
        "information",
        "Barrat",
        "direction",
        "SPi",
        "local clustering coefficient Ci ranges",
        "Random forest classifier",
        "unweighted trian- gles",
        "other three indices",
        "one feedforward triangle",
        "one cyclic triangle",
        "local network",
        "one item",
        "triangle congregation",
        "other four",
        "k ini",
        "weighted version",
        "overlapping set",
        "three-node subnetworks",
        "work motifs",
        "natural interpretation",
        "three nodes",
        "unweighted triangles",
        "two triangles",
        "feedforward triangles",
        "cyclic triangles",
        "many users",
        "different directed",
        "particular neighbors",
        "Such neighbors",
        "bidirectional edges",
        "ki(ki",
        "Fig. 2c",
        "degree zero",
        "proportion",
        "kouti",
        "Examples",
        "vj",
        "contribution",
        "sini",
        "buys",
        "souti",
        "sells",
        "abundance",
        "undirected",
        "Newman",
        "increase",
        "community",
        "Radicchi",
        "Palla",
        "hypothesis",
        "extent",
        "concept",
        "mi",
        "Ciki",
        "Note",
        "Frequencies",
        "marketplace",
        "contrast",
        "index",
        "cycle",
        "CYPi",
        "CYi",
        "definition",
        "calculation",
        "weights",
        "Breiman",
        "The precision–recall (PR) curve",
        "random forest clas- sifier",
        "random forest classifier",
        "ensemble learning method",
        "classifica- tion performance",
        "receiver operating characteristic",
        "two performance measures",
        "true positive rate",
        "false positive rate",
        "piecewise linear manner",
        "Directed triangle patterns",
        "same test sample",
        "tive training samples",
        "ROC) curve",
        "ROC curve",
        "two sets",
        "three-node patterns",
        "directed triangles",
        "same order",
        "single-tree positive",
        "Cyclic triangle",
        "tive samples",
        "precision range",
        "test data",
        "positive probability",
        "multiple classifiers",
        "decision-tree classifiers",
        "best split",
        "reciprocal edges",
        "fraudulent user",
        "fraudulent types",
        "descending order",
        "horizontal axis",
        "vertical axis",
        "test samples",
        "training data",
        "fraudulent) samples",
        "terminal node",
        "negative probability",
        "candidate features",
        "Feedforward triangle",
        "decision trees",
        "300 trees",
        "Hastie",
        "scikit-learn",
        "Pedregosa",
        "overfitting",
        "basis",
        "replace",
        "ment",
        "fraction",
        "average",
        "count",
        "Five",
        "numbers",
        "figure",
        "2",
        "100 random forest classifiers",
        "accurate binary classifier",
        "random number generator",
        "ten different permutations",
        "ten permutations",
        "different types",
        "PR curves",
        "grid search",
        "10-fold cross-validation",
        "maximum depth",
        "max_depth parameter",
        "other hyperparameters",
        "default values",
        "parameter optimization",
        "sampling bias",
        "sampling scheme",
        "Descriptive statistics",
        "survival probability",
        "frac- tion",
        "one transaction",
        "clear distinction",
        "large value",
        "The AUC",
        "lent users",
        "mal users",
        "minimum number",
        "seed number",
        "single set",
        "AUC value",
        "permutation importance",
        "nal nodes",
        "scikit-learn version",
        "useful information",
        "degree distribution",
        "unweighted degree",
        "user type",
        "Fig. 3a",
        "Fig. 3c",
        "training set",
        "performance measure",
        "degree ki",
        "normal",
        "area",
        "good",
        "Strobl",
        "Altmann",
        "method",
        "decrease",
        "tree",
        "integers",
        "split",
        "max_features",
        "n_estimators",
        "Results",
        "specified",
        "classification",
        "difference",
        "many",
        "fact",
        "≥",
        "Normal Fictive Underwear Medicine Weapon",
        "unweighted sell probability",
        "different user types",
        "less frequent transactions",
        "one sell transaction",
        "Seed user type",
        "large fraction",
        "smaller value",
        "characteristic behavior",
        "first column",
        "exclusive buyer",
        "fraudulent type",
        "seed users",
        "sell transactions",
        "Fig. 4a",
        "Fig.  5a",
        "c Strength",
        "specific neighbor",
        "average number",
        "Total number",
        "Table 1 Properties",
        "conditional mean",
        "Degree",
        "ki",
        "majority",
        "result",
        "ence",
        "median",
        "FFi",
        "samples",
        "peak",
        "smallest possible weighted sell probability",
        "smallest possible out-degree",
        "corresponding frac- tions",
        "tinct frequency distributions",
        "Survival probability",
        "cycle probability",
        "Figure  5b",
        "dashed lines",
        "Figure 5b",
        "two aspects",
        "Figure  6b",
        "larger value",
        "clear difference",
        "descriptive statistics",
        "ous section",
        "horizontal line",
        "second feature",
        "Fig.  6a",
        "Fig. 8a",
        "relationships",
        "ini",
        "buyers",
        "one",
        "transactions",
        "neighbor",
        "results",
        "analysis",
        "tendency",
        "Classification",
        "12 features",
        "unweighted sell probability SPi",
        "Fig. 8 Cycle probability",
        "Fig. 7 Triangle congregation",
        "b Survival probability",
        "third feature",
        "real number",
        "fourth feature",
        "fifth feature",
        "sixth feature",
        "seventh feature",
        "eighth feature",
        "c Relationship",
        "WSPi"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.938228,
      "content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 707841,
      "metadata_storage_name": "s40467-015-0033-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ2Ny0wMTUtMDAzMy05LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-16T18:35:45Z",
      "keyphrases": [
        "Creative Commons Attribution License",
        "interesting, potentially useful patterns",
        "Ant colony optimization algorithm",
        "modern intelligent optimization methods",
        "current data analysis tools",
        "important practical significance",
        "Open Access article",
        "many successful examples",
        "machine learning methods",
        "many different specifiedmethods",
        "Uncertain inference rule",
        "RESEARCH Open Access",
        "Bayesian belief network",
        "large data sets",
        "financial data analysis",
        "data compartment analysis",
        "many different fields",
        "data mining techniques",
        "data mining algorithms",
        "mated data generation",
        "data mining methods",
        "data mining technology",
        "collection tools",
        "many methods",
        "inference rules",
        "many other",
        "Uncertainty Analysis",
        "neutral network",
        "Bayesian classification",
        "Extraction methods",
        "uncertain system",
        "tomany fields",
        "mining objects",
        "tical methods",
        "effective methods",
        "Data classification",
        "existing data",
        "data description",
        "data dependency",
        "data regression",
        "data aggregate",
        "data prediction",
        "training data",
        "Rules extraction",
        "classification rules",
        "Yun Sun",
        "Yuanguo Zhu",
        "recent years",
        "classification problems",
        "computer networks",
        "scientific research",
        "ness decision",
        "computational process",
        "overall goal",
        "standable structure",
        "biomedical research",
        "main jobs",
        "decision trees",
        "mathematical formula",
        "original work",
        "Nanjing University",
        "business management",
        "valuable information",
        "two applications",
        "science research",
        "unrestricted use",
        "Ling Chen",
        "Journal",
        "DOI",
        "Correspondence",
        "chinalsy",
        "School",
        "Abstract",
        "increasing",
        "attention",
        "paper",
        "effectiveness",
        "superiority",
        "Keywords",
        "Introduction",
        "databases",
        "finance",
        "E-commerce",
        "logistics",
        "result",
        "amount",
        "people",
        "basis",
        "difficulty",
        "depth",
        "deficiency",
        "concepts",
        "laws",
        "modes",
        "knowledge",
        "reality",
        "instance",
        "telecommunication",
        "retail",
        "study",
        "couple",
        "models",
        "functions",
        "characteristics",
        "categories",
        "output",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "Page",
        "variety",
        "approaches",
        "start",
        "addition",
        "mutation ant colony optimization algorithm",
        "original ant colony optimization algorithm",
        "early decision trees algorithm",
        "simulated annealing algorithm",
        "global search algorithm",
        "powerful mathematical tool",
        "uncertain differential equation",
        "mature technology methods",
        "Decision trees method",
        "fuzzy rules learning",
        "many mathematical tools",
        "massive data set",
        "data mining technique",
        "neutral network algorithm",
        "probability distri- bution",
        "product measure axiom",
        "decision tree method",
        "incremental tree structure",
        "intelligent optimization algorithms",
        "different network models",
        "random initial solutions",
        "Machine learning methods",
        "popular classification methods",
        "complex nonlinear relations",
        "classification machine learning",
        "many evolution algorithms",
        "Genetic algorithm",
        "optimization strategies",
        "bionic optimization",
        "numerical methods",
        "conceptual learning",
        "inductive learning",
        "ID3 method",
        "C4.5 method",
        "different types",
        "many cases",
        "spatial data",
        "probability theory",
        "normality axiom",
        "duality axiom",
        "subadditivity axiom",
        "pattern classification",
        "statistical principle",
        "tering problems",
        "core content",
        "simulation model",
        "complex system",
        "pattern recognition",
        "biological evolution",
        "genetic mechanism",
        "important role",
        "Mixed algorithms",
        "other algorithms",
        "slow convergence",
        "local optimums",
        "Hybrid genetic",
        "hybrid particle",
        "real world",
        "human being",
        "indetermi- nacy",
        "domain experts",
        "belief degree",
        "tainty theory",
        "Many researchers",
        "theoretical work",
        "uniqueness theorem",
        "practical problems",
        "number",
        "variants",
        "SLIQ",
        "clustering",
        "regression",
        "behavior",
        "ant-miner",
        "Herrera",
        "weakness",
        "reason",
        "Zhu",
        "ACOA",
        "MACO",
        "performance",
        "understanding",
        "indeterminacy",
        "samples",
        "situation",
        "choice",
        "event",
        "unlikely",
        "frequency",
        "view",
        "Liu",
        "Chen",
        "Applications",
        "lot",
        "existence",
        "stability",
        "Peng",
        "Yao",
        "two typical classification problems",
        "ant colony optimization algorithm",
        "uncertain optimal control model",
        "heuristic optimization approach",
        "ant transfer prob",
        "option pricing models",
        "expected value operator",
        "two extraction methods",
        "M feasible solutions",
        "following optimization problem",
        "uncertain inference rule",
        "pheromone evaporation rate",
        "same pheromone level",
        "uncertain inference controller",
        "optimal solution s",
        "one ant",
        "artificial ant",
        "tain controller",
        "uncertain systems",
        "uncertain sets",
        "membership functions",
        "five parts",
        "universal approximator",
        "reasonable tool",
        "inverted pendulum",
        "uncertainty theory",
        "mutation operation",
        "data mining",
        "uncertainty sets",
        "decision variable",
        "domain D",
        "candidate solution",
        "best solution",
        "pheromone communication",
        "pheromone levels",
        "initial pheromone",
        "pheromone trails",
        "many paths",
        "bad paths",
        "good paths",
        "next section",
        "basic concepts",
        "last section",
        "objective function",
        "constraint function",
        "good ability",
        "data set",
        "food sources",
        "real ants",
        "cial ants",
        "one path",
        "walking path",
        "stocks",
        "inputs",
        "outputs",
        "rule-base",
        "Gao",
        "robustness",
        "results",
        "Preliminary",
        "Dorigo",
        "nest",
        "time",
        "parameters",
        "iterations",
        "procedures",
        "Step",
        "probability",
        "sk",
        "5",
        "⎪⎪⎪",
        "first measure inversion formula",
        "following three axiom",
        "product uncertain measureM",
        "Mamdani inference rules",
        "Takagi-Sugeno inference rules",
        "triangular uncertain set",
        "regular membership function μ",
        "inversion formulas",
        "inference systems",
        "fuzzy inference",
        "nonempty set",
        "optimal solution",
        "current iteration",
        "uncertainty space",
        "real numbers",
        "expected value",
        "two integrals",
        "key points",
        "fuzzy systems",
        "CRI approach",
        "human knowledge",
        "set function",
        "Amembership functionμ",
        "fuzzy sets",
        "Mk{�k",
        "M{ξ � r",
        "M{B ⊂",
        "B.",
        "⊂ B",
        "algebra",
        "L.",
        "triplet",
        "Lk",
        "Definition",
        "collection",
        "Borel",
        "Example",
        "power",
        "ξn",
        "Bi",
        "sup",
        "Bc",
        "equations",
        "Remark",
        "mode",
        "Theorem",
        "x0",
        "fact",
        "dx",
        "2b",
        "antecedents",
        "consequents",
        "process",
        "consequences",
        "σ",
        "∈",
        "∑",
        "∞",
        "⎪",
        "γ",
        "1",
        "∫",
        "independent uncertain sets",
        "conditional uncertain set",
        "m crisp inputs",
        "crisp data",
        "crisp values",
        "Uncertain system",
        "uncertain antecedents",
        "uncertain consequents",
        "two concepts",
        "constant a",
        "theory",
        "following",
        "X1",
        "Xm",
        "im",
        "ck",
        "coefficients",
        "Eq.",
        "ηi",
        "constants",
        "k∑",
        "min",
        "μil",
        "experts",
        "Y1",
        "Y2",
        "Yn",
        "Yj",
        "βj",
        "ξ",
        "ν",
        "η∗",
        "⎪⎪⎪⎪",
        "1 μ",
        "special triangular uncertain set",
        "m input data",
        "dent uncertain sets",
        "Q uncertain sets",
        "R uncertain sets",
        "consequence uncertain sets",
        "uncertain inference rules",
        "k inference rules",
        "one inference rule",
        "uncertain consequence",
        "m inputs",
        "three cases",
        "two points",
        "symmetry property",
        "Problem formulation",
        "extraction model",
        "decision vector",
        "rule base",
        "n rules",
        "m antecedents",
        "variable xi",
        "k.",
        "βn",
        "μim",
        "bj",
        "ij",
        "loss",
        "generality",
        "above",
        "1 output",
        "ing",
        "αi",
        "γi",
        "2βi",
        "Proof",
        "Equation",
        "computation",
        "dy",
        "section",
        "example",
        "corresponding objective function values",
        "following optimization model",
        "candidate decision variable",
        "Q + 1 choices",
        "best feasible solution",
        "mean absolute error",
        "best rule base",
        "new candidate sequence",
        "m values",
        "absolute errors",
        "feasible solutionX0",
        "Uncertain systems",
        "mutated solution",
        "current solution",
        "tain sets",
        "five descriptions",
        "system outputs",
        "origin outputs",
        "C classes",
        "ith subinterval",
        "extraction method",
        "The procedures",
        "fixed parameter",
        "evaporation rate",
        "classification problem",
        "next node",
        "sequence xi1xi2",
        "mutated element",
        "step k",
        "left nodes",
        "Once ants",
        "Q∑",
        "4 antecedents",
        "input",
        "MAE",
        "1 P",
        "P∑",
        "integers",
        "nonsense",
        "subintervals",
        "xij",
        "Extractionmethod",
        "withmutations",
        "1 value",
        "R.",
        "steps",
        "walk",
        "Denote",
        "τi",
        "xik",
        "way",
        "termination",
        "condition",
        "ρg",
        "⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪",
        "inner cycle termination criterion",
        "M candidate solutions",
        "good robust property",
        "uncertain rule base",
        "feasible solu- tion",
        "local search operation",
        "Metropolis acceptance criterion",
        "Step 2 Ant movement",
        "new decision vector",
        "Simulated annealing algorithm",
        "feasible solution X0",
        "portfolio optimization",
        "optimization problem",
        "decision variables",
        "initial solution",
        "neighbor solution",
        "initial value",
        "anneal- ing",
        "controlled cooling",
        "sequence xip",
        "allM ants",
        "previous section",
        "Repeat Step",
        "1 steps",
        "values",
        "Xl.",
        "classification",
        "Kirkpatrick",
        "name",
        "inspiration",
        "metallurgy",
        "technique",
        "heating",
        "material",
        "size",
        "crystals",
        "defects",
        "temperature",
        "point",
        "neighborhood",
        "order",
        "xiq",
        "procedure",
        "original feasible solution",
        "search range",
        "current temperature",
        "F(X̂",
        "Set",
        "step",
        "rithm",
        "tk",
        "τ"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.719706,
      "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2424684,
      "metadata_storage_name": "s40064-015-1515-4.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDA2NC0wMTUtMTUxNS00LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Rik Das",
      "metadata_title": "Multi technique amalgamation for enhanced information identification with content based image data",
      "metadata_creation_date": "2015-11-26T05:08:05Z",
      "keyphrases": [
        "multi tech- nique feature extraction process",
        "content based image data Rik Das1",
        "Creative Commons Attribution 4.0 International License",
        "traditional text based annotation technique",
        "Dr. Camil Bulcke Path",
        "customary text based process",
        "Diverse low level features",
        "content based feature extraction",
        "content based image identification",
        "Creative Commons license",
        "robust feature vectors",
        "Multi technique amalgamation",
        "low cost storage",
        "digital photo-capture devices",
        "four public datasets",
        "test Open Access",
        "P.O. Box",
        "digital image acquisition",
        "huge image repositories",
        "gigantic image datasets",
        "image capturing devices",
        "new research challenges",
        "OT Scene) Dataset",
        "three different techniques",
        "exist- ing techniques",
        "original author(s",
        "Semantically analogous images",
        "image identification technique",
        "enhanced information identification",
        "manual process",
        "image content",
        "Diverse applications",
        "image recognition",
        "image binarization",
        "image transform",
        "Image classification",
        "Image retrieval",
        "Raventós",
        "RESEARCH Das",
        "image information",
        "author information",
        "Corel Dataset",
        "Caltech Dataset",
        "Conventional techniques",
        "art techniques",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Recent years",
        "computer power",
        "accessible internet",
        "Efficient indexing",
        "computer vision",
        "machine learning",
        "Automatic derivation",
        "severe limitations",
        "aforesaid limitations",
        "resourceful foundation",
        "social media",
        "morphological operator",
        "Oliva Torralba",
        "recognition rate",
        "Classification result",
        "average increase",
        "retrieval result",
        "Slant transform",
        "unrestricted use",
        "appropriate credit",
        "Xavier Institute",
        "Social Service",
        "Purulia Road",
        "Full list",
        "effective alternative",
        "meaningful information",
        "Information Technology",
        "Wang Dataset",
        "decision fusion",
        "Background",
        "ubiquity",
        "mass",
        "popularity",
        "Madireddy",
        "Walia",
        "words",
        "mapping",
        "perception",
        "vocabulary",
        "person",
        "nature",
        "Abstract",
        "proliferation",
        "areas",
        "biomedicine",
        "military",
        "commerce",
        "education",
        "means",
        "success",
        "paper",
        "Precision",
        "state",
        "Otsu",
        "threshold",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "SpringerPlus",
        "DOI",
        "Correspondence",
        "1 Department",
        "Ranchi",
        "Jharkhand",
        "India",
        "end",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "26Das",
        "competence",
        "color",
        "content based image recog",
        "binarization based feature extraction",
        "content based image classification",
        "feature extraction Feature extraction",
        "Z score normalization",
        "orthogonal unitary matrices",
        "Appropriate threshold selection",
        "fusion based method",
        "single feature extrac",
        "fractional energy coefficient",
        "fifteen fractional coefficients",
        "high dimensional data",
        "main contribu- tion",
        "multi-technique feature extraction",
        "efficient feature extraction",
        "feature extrac- tion",
        "efficient image binarization",
        "feature vector dimension",
        "image trans- formation",
        "multi technique fusion",
        "image binariza- tion",
        "Image binarization techniques",
        "feature vectors",
        "feature spaces",
        "image data",
        "energy spectrum",
        "subband coefficients",
        "novel techniques",
        "Diverse techniques",
        "image identification",
        "image elements",
        "image patterns",
        "seven image",
        "recognition decision",
        "hybrid architecture",
        "fusion architecture",
        "Statistical validation",
        "trans- forms",
        "morphological operators",
        "archi- tecture",
        "four subsections",
        "earlier works",
        "four topics",
        "basis functions",
        "one representation",
        "two aspects",
        "critical components",
        "compact structure",
        "cient storage",
        "radical reduction",
        "multiple scales",
        "tropic transform",
        "various factors",
        "uneven illumination",
        "research objectives",
        "research results",
        "transformation process",
        "aforesaid properties",
        "basis images",
        "Original images",
        "shape",
        "texture",
        "number",
        "features",
        "morphology",
        "Comparison",
        "retrieval",
        "correlation",
        "connection",
        "contemporary",
        "Change",
        "domain",
        "set",
        "series",
        "Annadurai",
        "Shanmugalakshmi",
        "advantages",
        "waveforms",
        "use",
        "analysis",
        "transmission",
        "Kekre",
        "Thepade",
        "subbands",
        "Prakash",
        "Luo",
        "execution",
        "inadequate",
        "Ramírez- Ortegón",
        "Gradient vector flow fields",
        "multi technique feature extraction",
        "fusion based image identification",
        "superior prediction accuracy",
        "feature level correlations",
        "image signature extraction",
        "feature extraction technique",
        "Popular contour-based descriptors",
        "Enhanced classification results",
        "curvature scale space",
        "feature selection technique",
        "region- based descriptors",
        "3 D color histogram",
        "diverse extraction techniques",
        "Two different categorization",
        "global threshold selection",
        "shape feature extraction",
        "multilevel mean threshold",
        "image binarization techniques",
        "local threshold selection",
        "retrieval process complexity",
        "feature vector",
        "identification rate",
        "Image Content",
        "different techniques",
        "shape descriptors",
        "semantic retrieval",
        "Local descriptors",
        "local decisions",
        "shape information",
        "region-based descriptors",
        "information fusion",
        "early fusion",
        "late fusion",
        "hybrid fusion",
        "intermediate fusion",
        "adverse effect",
        "Contemporary literatures",
        "unfavourable influences",
        "binarized images",
        "standard deviation",
        "gray values",
        "Commercial viability",
        "existing literatures",
        "ary lines",
        "Fourier descriptor",
        "chain codes",
        "complex shapes",
        "Information recognition",
        "Recent studies",
        "four classes",
        "higher dimensions",
        "joint model",
        "genetic algorithm",
        "optimum boundaries",
        "numerical intervals",
        "memory con",
        "filter responses",
        "distinct features",
        "concentrated features",
        "multiple features",
        "Fusion methodologies",
        "rate learner",
        "Gabor filters",
        "Color moments",
        "texture features",
        "contrast",
        "computation",
        "Valizadeh",
        "average",
        "advantage",
        "spread",
        "calculation",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Rojas",
        "Shaikh",
        "Use",
        "systems",
        "Flickner",
        "PicToSeek",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Emphasize",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "area",
        "object",
        "Kim",
        "single",
        "input",
        "sepa",
        "combiner",
        "scalability",
        "comparison",
        "mix",
        "Zhu",
        "Shyu",
        "sumption",
        "ElAlami",
        "back propagation neural net- work",
        "fuzzy set theoretic approach",
        "angular radial transform descriptor",
        "artificial neural network",
        "spatial structure descriptors",
        "spatial orientation tree",
        "lesser computational overhead",
        "neural network architecture",
        "correct semantic retrieval",
        "Higher retrieval results",
        "semantic retrieval results",
        "local edge bins",
        "non directional edge",
        "HSV color space",
        "significant point features",
        "Three different techniques",
        "color layout descriptor",
        "greater feature dimension",
        "content-based image retrieval",
        "tex- ture features",
        "edge histogram descriptor",
        "inter-class feature extraction",
        "Gabor texture descriptor",
        "fusion based classifier",
        "invariant color features",
        "invariant moments",
        "retrieval decisions",
        "retrieval performance",
        "retrieval rate",
        "retrieval purpose",
        "color histogram",
        "GIST descriptor",
        "horizontal edge",
        "Feature vectors",
        "popular feature",
        "32 feature maps",
        "feature values",
        "feature size",
        "color co",
        "color motif",
        "color moment",
        "occurrence matrix",
        "Recognition process",
        "image signatures",
        "Multi view",
        "Wavelet packets",
        "Eigen values",
        "sub repository",
        "main image",
        "right neighbourhood",
        "query image",
        "scan pattern",
        "equal weights",
        "Precision values",
        "individual techniques",
        "Six semantics",
        "sub- image",
        "tion techniques",
        "same size",
        "16 average value",
        "other hand",
        "morphological technique",
        "vertical edge",
        "shape features",
        "EHD) features",
        "edge images",
        "45° edge",
        "135° edge",
        "Hiremath",
        "Pujari",
        "Yue",
        "points",
        "similarity",
        "Banerjee",
        "Jalab",
        "increased",
        "Shen",
        "Wu",
        "authors",
        "Irtaza",
        "kind",
        "training",
        "response",
        "intra-class",
        "Rahimi",
        "Moghaddam",
        "CCM",
        "difference",
        "pixels",
        "DBPSP",
        "ANN",
        "Subrahmanyam",
        "MCMCM",
        "colour",
        "CMs",
        "sub-image",
        "unique",
        "Methods",
        "4 scales",
        "8 orientations",
        "Douze",
        "region",
        "Red Component Green Component Blue Component",
        "popular global threshold selection method",
        "local threshold selec- tion method",
        "image feature point extraction",
        "different gray level pixels",
        "four subsec- tions",
        "three color components",
        "necessary image information",
        "squared Euclidian distance",
        "document image binarzation",
        "Conventional BoW model",
        "alloca- tion",
        "feature extraction",
        "level histogram",
        "thresholding method",
        "parametric method",
        "optimal threshold",
        "words model",
        "information losses",
        "fifth subsection",
        "test images",
        "binari- zation",
        "dant details",
        "two classes",
        "foreground pixels",
        "background pixels",
        "class variance",
        "The separation",
        "Comprehensive investigation",
        "class probabilities",
        "Total variance",
        "two terms",
        "erate bag",
        "SIFT algorithm",
        "SIFT descriptors",
        "cluster members",
        "codebook generation",
        "final step",
        "huge computational",
        "BoW generation",
        "class means",
        "tering process",
        "descriptor dimension",
        "classification",
        "techniques",
        "methods",
        "fusion",
        "architecture",
        "following",
        "description",
        "datasets",
        "Fig.",
        "way",
        "combined",
        "sum",
        "variances",
        "Eq.",
        "Eqs.",
        "q1",
        "effect",
        "contributions",
        "Zhao",
        "size",
        "problem",
        "omissions",
        "stability",
        "clustering",
        "codewords",
        "overhead",
        "∑",
        "σ",
        "N*N feature vector Feature Vector Dimension Reduction",
        "Compute binary image maps",
        "N unitary slant matrix",
        "Blue (B) color component",
        "N × N matrix",
        "higher intensity feature vectors",
        "two dimensional slant transform",
        "N × 1 vector",
        "higher intensity group",
        "three different color",
        "discrete Fourier transform",
        "One dimensional transform",
        "lower intensity group",
        "Slant transform matrices",
        "frequency domain information",
        "local threshold value",
        "spatial domain information",
        "efficient algorithm design",
        "insignificant transform coefficients",
        "drastic reduction",
        "smaller dimension",
        "Energy compaction property",
        "unitary transform",
        "spatial information",
        "matrix multiplication",
        "two groups",
        "orthogonal transform",
        "inverse transform",
        "transform operation",
        "grey values",
        "two codewords",
        "large fraction",
        "faster execution",
        "tion operation",
        "computational overhead",
        "energy conservation",
        "signal energy",
        "significant role",
        "Partial Coefficients",
        "sequential transformations",
        "average energy",
        "test image",
        "image features",
        "monochrome image",
        "image quality",
        "image line",
        "components R",
        "tain operations",
        "real components",
        "pixel values",
        "average coding",
        "Transforms",
        "approach",
        "cluster",
        "mean",
        "codebook",
        "Tx",
        "Method",
        "End",
        "xhi",
        "cer",
        "capacity",
        "example",
        "characteristic",
        "Green",
        "8 bits",
        "1 bit",
        "images",
        "24–2 bits",
        "Pratt",
        "less",
        "column",
        "row",
        "forward",
        "Eqs",
        "The energy compaction property",
        "gray scale opening operation",
        "lower intensity feature vector",
        "higher intensity feature vector",
        "entire feature vector set",
        "feature vector extraction process",
        "shorter dis- tance",
        "feature vector formation",
        "highest classification result",
        "Blue color components",
        "top-hat transform technique",
        "feature vector database",
        "significant image information",
        "image transform End",
        "image similarity measures",
        "Higher similarity",
        "database images",
        "working process",
        "Similar process",
        "classification results",
        "slant transform",
        "complete set",
        "partial coefficient",
        "fractional coefficient",
        "small patch",
        "Human perception",
        "shape context",
        "point correspondences",
        "considerable contribution",
        "closing operations",
        "hat transformation",
        "bright peaks",
        "tophat operator",
        "code- book",
        "noteworthy image",
        "original image",
        "visual words",
        "series form",
        "one cluster",
        "codebook size",
        "peak detector",
        "Red, Green",
        "coefficients",
        "2. Peak",
        "algorithm",
        "query",
        "block",
        "12 elements",
        "dimension",
        "variant",
        "Sridhar",
        "Exit",
        "bag",
        "BoW",
        "methodology",
        "clusters",
        "Determination",
        "distance",
        "Dunham",
        "σ   Red Component Green Component Blue Component",
        "Three different distance measures",
        "forward Neural Network Classifier",
        "support vector machine",
        "nearest neigh- bors",
        "city block distance",
        "MSE) distance metric",
        "Canberra Distance measure",
        "fusion based classification",
        "first hidden layer",
        "input feature vectors",
        "mean squared error",
        "Data standardization technique",
        "back propagation technique",
        "supervised learning procedure",
        "multi layer perceptron",
        "database image T",
        "query image Q",
        "color component",
        "SVM) classifier",
        "classifier types",
        "network prediction",
        "Euclidian distance",
        "final distance",
        "classification decision",
        "Multilayer Perceptron",
        "input layer",
        "input features",
        "input units",
        "classified query",
        "clas- sifier",
        "following sections",
        "tophat transform",
        "morphological operation",
        "xloF.V.",
        "common range",
        "Top-Hat operator",
        "weighted sum",
        "appropriate output",
        "sification performance",
        "training tuple",
        "backward direction",
        "normalization process",
        "classified image",
        "higher values",
        "precision values",
        "stdev xhi",
        "qp background",
        "greater effect",
        "individual distances",
        "class majority",
        "similar images",
        "top 20 images",
        "target value",
        "stdev xlo",
        "VF xlo",
        "foreground",
        "xmean",
        "xhimeanxhi",
        "xlomeanxlo",
        "Qi",
        "attributes",
        "possibilities",
        "Dcityblock",
        "Deuclidian",
        "DMSE",
        "distn",
        "disti",
        "weights",
        "interest",
        "Ranking",
        "feed",
        "MLP",
        "Alsmadi",
        "optimization",
        "modifications",
        "The",
        "µ",
        "feature vector dataset",
        "Support vector machine",
        "original training data",
        "Optimal separating hyperplane",
        "two different classes",
        "bias node",
        "subsequent layer",
        "hidden layer",
        "Weighted output",
        "final layer",
        "output layer",
        "forward property",
        "higher dimension",
        "nonlinear mapping",
        "new dimension",
        "appropriate nonlinear",
        "class labels",
        "summation",
        "utes",
        "nodes",
        "half",
        "inputs",
        "processing"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.418702,
      "content": "\nSentiment analysis and the complex \nnatural language\nMuhammad Taimoor Khan1*, Mehr Durrani2, Armughan Ali2, Irum Inayat3, Shehzad Khalid1 and Kamran \nHabib Khan4\n\nIntroduction\nSentiment analysis (Pang and Lillian 2008) is a type of text classification that deals with \nsubjective statements. It is also known as opinion mining, since it processes opinions in \norder to learn about public perception. Sentiment analysis and opinion mining are the \nsame, and are used interchangeably throughout the document. It uses natural language \nprocessing (NLP) to collect and examine opinion or sentiment words. SA is explained \nas identifying the sentiments of people about a topic and its features (Pang and Lillian \n2008). The reason for the popularity of opinion mining is because people prefer to take \nadvice from others in order to invest sensibly. Determining subjective attitudes in big \nsocial data is a hotspot in the field of data mining and NLP (Hai et al. 2014).\n\nAbstract \n\nThere is huge amount of content produced online by amateur authors, covering a \nlarge variety of topics. Sentiment analysis (SA) extracts and aggregates users’ senti-\nments towards a target entity. Machine learning (ML) techniques are frequently used \nas the natural language data is in abundance and has definite patterns. ML techniques \nadapt to domain specific solution at high accuracy depending upon the feature set \nused. The lexicon-based techniques, using external dictionary, are independent of data \nto prevent overfitting but they miss context too in specialized domains. Corpus-based \nstatistical techniques require large data to stabilize. Complex network based tech-\nniques are highly resourceful, preserving order, proximity, context and relationships. \nRecent applications developed incorporate the platform specific structural information \ni.e. meta-data. New sub-domains are introduced as influence analysis, bias analysis, and \ndata leakage analysis. The nature of data is also evolving where transcribed customer-\nagent phone conversation are also used for sentiment analysis. This paper reviews \nsentiment analysis techniques and highlight the need to address natural language \nprocessing (NLP) specific open challenges. Without resolving the complex NLP chal-\nlenges, ML techniques cannot make considerable advancements. The open issues and \nchallenges in the area are discussed, stressing on the need of standard datasets and \nevaluation methodology. It also emphasized on the need of better language models \nthat could capture context and proximity.\n\nKeywords: Sentiment analysis, Machine learning, Sentiment orientation, Complex \nnetworks\n\nOpen Access\n\n© 2016 Khan et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nREVIEW\n\nKhan et al. Complex Adapt Syst Model  (2016) 4:2 \nDOI 10.1186/s40294-016-0016-9\n\n*Correspondence:   \ntaimoor.muhammad@gmail.\ncom \n1 Bahria University, Shangrilla \nRoad, Sector E-8, Islamabad, \nPakistan\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40294-016-0016-9&domain=pdf\n\n\nPage 2 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nManufacturers are also interested to know which features of their products are more \npopular in public, in order to make profitable business decisions. There is a huge reposi-\ntory of opinion content available at various online sources in the form of blogs, forums, \nsocial media, review websites etc. They are growing, with more opinionated content \npoured in continuously. It is, therefore, beyond the control of manual techniques to \nanalyze millions of reviews and to aggregate them towards a rapid and efficient deci-\nsion. Sentiment analysis techniques perform this task through automated processes with \nminimal or no user support. The online datasets may also contain objective statements, \nwhich do not contribute effectively in sentiment analysis. Such statements are filtered at \npre-processing.\n\nOpinion mining deals with identifying opinion patterns and presenting them in a \nway that is easy to understand. The outcome of sentiment analysis can be in the form \nof binary classification, such as categorizing opinions as recommended or not recom-\nmended. It can be considered as a multi-class classification problem on a given scale of \nlikeness. Cambria et al. (2013) used common-sense knowledge to improve the results of \nsentiment analysis. The results can be presented in the form of a short summary gen-\nerated from the overall analysis. Sentiment analysis has various sub streams including \nemotion analysis, trend analysis, and bias analysis etc. Its applications has outgrown \nfrom business to social, political and geographical domains. Sentiment analysis is \napplied to emails for gender identification through emotion analysis (Mohammad and \nYang 2011). Emotion is applied to fairy tales to draw interesting patterns (Mohammad \n2011). Considering text a complex network of words that are associated to each other \nwith sentiments, graph based analysis techniques are used for NLP tasks.\n\nNatural language processing\n\nOpinion mining requires NLP, to extract semantics of opinion words and sentences. \nHowever, NLP has open challenges that are too complex to be handled accurately till \ndate. Since sentiment analysis makes extensive use of NLP, it has this complex behav-\nior reflected. The assumptions in NLP for text categorization do not work with opinion \nmining, as they are different in nature. Documents having high frequency of matching \nwords may not necessarily possess same sentiment polarity. It is because, a fact in text \ncategorization could be either correct or incorrect, and is well known to all. Unlike facts, \na variety of opinions can be correct about the same product, due to its subjective nature. \nAnother difference is that, opinion mining is sensitive to individual words, where a sin-\ngle word like NOT may change the whole context. The open challenges are negations \nwithout using NOT word, sarcastic and comparative sentences etc. The later section has \na detailed discussion on NLP issues that affect sentiment analysis.\n\nThe subjective content from the online sources have simple, compound or complex \nsentences. Simple sentences possess single opinion about a product, while compound \nsentences have multiple opinions expressed together. Complex sentences have implicit \nmeaning and are hard to evaluate. Regular opinions pertain to a single entity only, while \ncomparative opinions have an object or some of its aspects discussed in comparison to \nanother object. Comparative opinions can either be objective or subjective. An example \nof a subjective sentence having comparison is “The sound effects of game X are much \nbetter than that of game Y” whereas an example of objective sentence with comparison is \n\n\n\nPage 3 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\n“Game X has twice as many control options as that of Game Y”. Opinion mining expects \na variety of sentence types, since people follow different writing styles in order to express \nthemselves in a better way.\n\nSentiment analysis\n\nThe machine learning (ML) based techniques are supervised, semi supervised or unsu-\npervised. The supervised techniques require labeled data, while the semi supervised \ntechniques need manual tuning from domain experts. The unsupervised techniques \nmake use of statistical analysis on large volume of data. ML techniques has a large fea-\nture set using Bag-of-words (BOW). Results are improved by pruning repetitive and \nlow quality features. The opinion words are extracted to identify the polarity of opinion \nexpressed for a feature. The performance of a classifier is measured through its effective-\nness at the cost of efficiency. Effectiveness is calculated as precision/recall and F-meas-\nure, which are measurements of relevance.\n\nSentiment analysis can also be considered as a complex network. It consists of nodes \nand edges joining them. Many complex systems from a variety of domains are repre-\nsented as network including environmental modeling (Niazi et al.  2010), business sys-\ntems (Aoyama 2002), wireless sensors, and ad-hoc networks (Niazi and Hussain 2009). \nNetworks are rich in information, having a range of local and global properties. Text cor-\npora can be used with words as nodes and edges representing the structural or seman-\ntic association between them. The adjacent nodes sharing a link are closely associated \nand directly affect each other through the weight of the link they share. Representing \ntext as complex network, various properties like centrality, degree distribution, com-\nponents, communities, paths etc. can be used to explore the data thoroughly. Through \nmulti-partite graphs, nodes can be distributed among various clusters with inter-cluster \nedges only. It separates different types of entities discussed in comparison. Entities are \nlinked to their respective aspects/features and then to the sentiments associated. The \nsentiments can be linked with the reasons shared in support of those sentiments.\n\nData sources\n\nOpinion mining has diverse subjective data sources that are available online. They cover \na large number of topics and are up-to-date with current issues. Introduction of Web2.0 \nin the last decade has enabled people to post their thoughts and opinions on a range of \ntopics. The data produced online is growing all the time produced by people from differ-\nent backgrounds (Katz et al. 2015). Opinion mining makes use of this data generated by \nmillions of users all over the world. According to Business Week survey in 2009, 70 % of \nthe people consult online reviews and ratings to make a purchase. Comscore/The Kelsey \ngroup in 2007 reported that 97 % of the people who made purchases based on online \nreviews, found them to be honest.\n\nThe user generated subjective content is of value to be assessed and summarized for \nprospective customers. These online data sources are in the form of blogs, reviews and \nsocial media websites. The popularity of blogging is on the rise, where people from dif-\nferent walks of life express their opinions about various entities and events and get com-\nments on them. At times, it leads to a form of discussion among the author and various \nusers commenting on them. A detailed analysis on blogging styles of authors, as they \n\n\n\nPage 4 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nfollow their own unique approaches for expressing their feelings is provided in (Chau \nand Xu 2007). Blogs contain opinions about various products, services, their features, \npackages and promotions. Most of the online studies on opinion extraction use blogs as \ndatasets (Qiang and Rob 2009) to perform detailed analysis.\n\nThere are professional review websites providing customers’ feedbacks, used for sen-\ntiment analysis. E-commerce websites allow customers to comment on their products. \nSocial media is another popular medium of sharing information among like-minded \npeople. Here, a variety of subjects are discussed where people express their opinions, \nbased on their own experience. Social media websites have a very complex struc-\nture for extracting information having user opinions. They allow users to express their \nviews through sharing articles and other media sources as an external link. Twitter, also \nreferred to as microblogging, has the problem of reviews being too short and at times \nmiss the context.\n\nThis review article is organized into the following divisions. Section 2 reviews the Sen-\ntiment analysis techniques and the NLP issues. Section 3 provides a discussion on the \nreview studied and Sect. 4 list the application areas for sentiment analysis. Section 5 has \nconcluded the study to important issues drawn from the study. Section 6 has distribu-\ntion of the work carried out by the authors.\n\nReview\nThe sentiment analysis techniques categorize reviews into positive and negative bins or \nmultiple degrees of it. The social data can be analyzed at three different levels i.e. user \ndata, relationship data and content (Tang et al.  2014). In survey (Guellil and Boukhalfa \n2015) these categories are further elaborated. Recommender systems are extended to \nsupport textual content using knowledge (Tang et al. 2013). In our previous work (Khan \nand Khalid 2015) sentiment analysis is highlighted to address health care problems from \nthe view point of a user. The issues faced in SA also depend on the data sources and \nnature of analysis required. An important aspect of social data analysis is the identifi-\ncation of sentiments and sentiment targets (Tuveri and Angioni 2014; Zhang and Liu \n2014). Opinion mining also consider the additional features of opinion holder and time. \nSentiment analysis techniques can be separated into three groups: supervised, semi-\nsupervised and unsupervised techniques.\n\nThe supervised techniques are the machine learning classifiers. They are more accu-\nrate, however, need to be trained on a relevant domain. The unsupervised statistical \ntechniques do not require training. They are efficient in dynamic environment but at the \ncost of accuracy. Sentiment analysis techniques analyze opinion datasets to generate a \ngeneral perception that people have about a product. The classification of sentiments in \na review document is performed through identifying and separating all the positive and \nnegative opinion words. Considering the strength of these words, along with their polar-\nity, helps in multi-class classification. Machine learning classifiers such as Naive-Bayes, \nk-nearest neighbor and centroid based classifier etc., are successfully used for this pur-\npose. Semantic orientation based techniques used for opinion mining are Lexicon based \nand statistical analysis. Lexicon based technique works with individual words while sta-\ntistical analysis incorporates words co-occurrence using point wise mutual information \n(PMI) and latent semantic analysis (LSA). Semi-supervised techniques start with a small \n\n\n\nPage 5 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nset of opinion words from the given domain, and expand on it. More opinion words are \nexplored by querying the starting seeds. The newly found words are queried again to find \nmore words until no new words are returned. Orientation of the opinion word form the \nbasis for classification. Other attributes used are frequency of occurrence, location and \nco-occurrence with other words. The taxonomy of these approaches is shown in Fig. 1.\n\nSentiment classification\n\nThese are the machine learning classifiers used for sentiment analysis. They can be \napplied to text documents at three levels for analysis. A document level approach, \nwhich studies the whole document as a single entity is appropriate for text categoriza-\ntion. However, document level approach is not viable for sentiment analysis with docu-\nments having multiple opinions. Therefore, sentiment analysis is performed extensively \nat sentence or word level. Word level analysis is also known as sentiment level analysis. \nML techniques suits sentiment analysis as the data is in abundance and there is obvious \npresence of patterns (Schouten and Frasincar 2015). The classifiers are trained on label \ndataset having samples representing all classes. A test dataset is used to evaluate the per-\nformance of the classifiers for the given task. Let the set of documents as {D = d1,…,dn}, \nand set of classes labeled as {C = c1,…,cn}, then the task is to classify document di in D \nwith a label ci in C. This task can be performed using supervised classifiers. The more \nfrequently used classifiers for sentiment analysis are discussed below.\n\nNaïve Bayes\n\nNaive Bayes (NB) classifier is extensively used for text classification. It learns from a \ntraining dataset of annotated feature vectors, with labels as positive and negative (in case \nof binary classification). The probability of a feature vector is calculated with each label \nusing the annotated training dataset. The feature vector is assigned a label that has high-\nest probability for it. If this information is preserved, it can be used to show confidence \nin a label for a feature vector. In further modifications of NB a fuzzy region is defined \nin which feature vectors hold both labels with a certain level of confidence. Text data \nnormally have high dimensional feature vectors. Therefore, the process of calculating \nprobability is repeated for each feature vector, and then all the probabilities contribute \ntowards the final decision. The feature set is represented as F = f1, f2…fm}, where prob-\nability of a document belonging to a class shown as:\n\nFig. 1 Taxonomy of expository literature on sentiment analysis\n\n\n\nPage 6 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nShows the probability of a document dj represented by its vector dj* belonging to a class \nci. It is the product of probabilities for all the features in the feature set. The document \nvector dj\n\n* is assigned to a class ci in order to maximize P\n(\n\nci\n\n∣\n\n∣\n\n∣\nd∗j\n\n)\n\n. The logarithm of prob-\nabilities are summed up to classify an opinion document. It is preferred over product of \nprobabilities to avoid underflow. It addresses the missing value problem as well. Slack \nvariables add smoothing effect against noisy data. Weights can also be assigned to fea-\ntures which define their contribution towards the classification. It is a biased approach, \nwhere prominent features are given high weights to play a major role in choose a senti-\nment label.\n\nNaive Bayes works on the assumption that all the sentences of a review document are \nopinion sentences. It also assumes that features of a document are independent of each \nother. Despite of this unrealistic assumption, Naïve Bayes is very successful and is used \nin various practical applications. The assumption of treating features as independent of \neach other makes Naive Bayes highly efficient (Dai et al. 2007). Although, Naive Bayes \nclassifier is simple, yet it is effective because of its robustness to irrelevant features. It \nperforms well in domains with many equally important features. It is considered to be \nmore reliable for text classification and sentiment analysis. The accuracy of the classifier \nimproves with pre-processing noise. It also used as transfer learning when trained on a \ndataset similar to the target dataset.\n\nNearest neighbor\n\nk-nearest neighbor classifier has been frequently used in literature for text classifica-\ntion. It considers the labels of k nearest neighbors to classify a test document. A special \ncase of the k-NN problem is typically referred to as classimbalance problem identified \nin (Yang and Liu 1999). Classes with more training data have higher influence to predict \nsame label for the new document. There are fewer chances of acquiring a class label if \nthat class has fewer training examples. (Li et al. 2003) catered this problem by using vari-\nable value of k for each class. Thus, the class having more training data will have higher \nvalue of k as compared to the one having few samples. This solution is helpful in online \nclassification, where there is time constraint on trying different values of k.\n\nA study on performance of k-NN using pre-processed dataset is conducted in (Shin \net al. 2006) claiming 10 % improvement when noise and outliers are filtered out. An opti-\nmum value is chosen as threshold to separate regular data from noise. Sentiment analy-\nsis is performed with a reduced set of feature vector in (Sreemathy and Balamurugan \n2012) to avoid the curse of dimensionality. Accuracy of the model improves as irrelevant \nfeatures were removed. Features are assigned weights to vary their contribution towards \ndecision making. Weights are extracted from probability of information in documents \nacross different categories. Tree-fast k-NN is introduced as fast kNN model (Soucy and \nMineau 2001). This tree based indexing of retrieval system improves the accuracy of \nk-NN in distance calculation. Its effective against large feature sets. The order of features \nand their thresholds are identified from within the training data. k-NN has promising \n\n(1)P(ci\n∣\n\n∣dj\n∗\n) =\n\np(ci)(\n∏m\n\ni=1 p(fi|ci) )\n\np(d∗j )\n\n\n\nPage 7 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nresults in sentiment analysis; however, it is more susceptible to noise and high dimen-\nsional feature set. Therefore, more of the work in k-NN for text classification has focused \non feature selection and reduction techniques as they are the driving factors of k-NN’s \nperformance.\n\nCentroid based\n\nCentroid based (CB) classifier calculates centroid vector or prototype vector for each \nclass in the training dataset. Centroid vector is the central point of the class and may not \nrepresent an actual training data. The distance of each test document is calculated with \nthe prototype vector of the class and is classified based on similarity with it. Its perfor-\nmance depends on the chosen centroid vectors. It is efficient since time and space com-\nplexities are proportional to the number of classes rather than training documents. To \ndouble the training data reverse of reviews are generated in (Xia et al. 2015) by invert-\ning the sentiment terms and their labels. Using both sets of training data with Mutual \nInformation (MI) the results were improved when only selected reviews were inverted. \nExternal dictionary WordNet is used to generate inverse for sentiment terms, however, \npseudo-antonyms can be generated internally using the corpus.\n\nms, however, pseudo-antonyms can be generated internally using the corpus. A variety \nof approaches have been used for CB classifier. Rocchio algorithm calculates centroid \nto represent feature space of documents (Ana and Arlindo 2007; Tan 2007a, b). Cen-\ntroid is computed through average of positive examples in (Han and Karypis 2000) and \nsum of positive cases i.e. the related training examples (Chuang et al. 2000). Normalized \nsum of positive vectors used in (Lertnattee and Theeramunkong2004), cosine similar-\nity between the test document and the Centroid of a class (Hidayet and Tunga 2012). \nCentroid is used with inverse of class similarity as well improving the accuracy close to \n100 % on the given dataset when characters are chosen as features instead of n-grams.\n\nCentroid evaluation is sensitive to noise in the training dataset which affects the over-\nall performance of the classifier. This shortfall is exposed when Centroid classifier is \napplied to a slightly different domain. The reason for this drawback is that some opinion \nwords are domain dependent. They have different polarity or strength of polarity when \nused in a different domain. Smoothing techniques have being proposed in (Tan 2007a, b;  \nLertnattee and Theeramunkong 2006; Guan 2009) that minimizes the effect of noise \nin the dataset. (Chizi et al. 2009) defined a weighting scheme giving higher weights to \nexplicit opinion words. Characters and special characters for feature selection are used \nin (Ozgur and Gungor 2009). The work in (Shankar and Karypis 2000; Tan et al. 2005) \nis focused on adjusting the value of centroid based with feedback looping, hypothesis \nmargin and weight-adjustment respectively. They try to rectify class Centroid, if it is not \ncalculated accurately. Centroid based classifier performs efficiently as it doesn’t consider \ntraining data each time to decide a test document.\n\nSupport vector machine\n\nSupport vector machine classifier is used for text classification in various studies. It finds \na separation among the data using the annotated training dataset. The margin of sep-\naration between classes, which is known as hyperplane, is used to classify the incom-\ning data. The hyperplane should give maximum separation between the classes. It is \n\n\n\nPage 8 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\napplicable even in the presence of high dimensional feature set representation. It classify \nbased o hyperplane among classes. Like centroid-based, SVM also consider the hyper-\nplane to classify a test document. (Brown et  al. 1997) has compared SVM with artifi-\ncial neural networks for text classification and has found it better. Since it has promising \nresults in text classification, it also performs well for opinion mining. They have also \nclaimed in (Brown et al. 1997) that SVM is better than Naive Bayes and decision trees \nclassification algorithms. However, SVM consumes more resources at the training \nstage. Although, it is efficient with large feature set, Feldman et al.(2011) has shown that \ndimensionality reduction in feature set further improves the performance of SVM. It \nexhibits linear complexity and can scale up to a large dataset.\n\nSVM has a limitation of over-reliance on selection of suitable kernel function. Kernel \nis calculated through Linear, Polynomial, Gaussian or sigmoid methods but they tend to \nbe domain specific. Kernel functions that perform well for one domain may not repeat it \nfor next. Its accuracy is also sensitive to number of training samples close to hyperplane. \nSlack variables are introduced to limit the impact of boundary samples by generalizing \nthe classifier, known as soft margin classification. They also help to avoid over-fitting the \ntraining data.\n\nUnsupervised techniques\n\nThe unsupervised sentiment analysis techniques do not require training data and rather \nrely on semantic orientation. They make use of lexicons to identify the positive or neg-\native semantics of opinion words. The meaning of the word, expressed by its use in a \ncontext is called lexicon. An online or off-line dictionary is consulted for this purpose. \nStatistical analysis techniques are also unsupervised, identifying the orientation of senti-\nment words through statistical evaluations. They require large volume of data for high \naccuracy.\n\nLanguages consists of lexicons that are the words used for a particular sense, and a \ngrammar that connect these lexicons. Part-of-speech rules are used to extract senti-\nment phrases from text document. Search engines are used to identify the orientation \nof sentiment words that are missing in the dictionary. Its polarity is identified through \nthe nearby words brought by search engines. They purely rely on external sources and \ntherefore cannot address the context. Lexicon based techniques perform well for general \ndomains while statistical techniques addresses the context and are useful in specialized \ndomains. The two types of approaches are discussed in detail.\n\nDictionary (Lexicon) based techniques\n\nLexicon based techniques extract opinion lexicons from the document and analyzes \nits orientation without the support of any training data. These techniques process the \nopinion words separately, ignoring the relationship between them. Lexicons refer to the \nsemantic orientation. Lexicons are independent of the source data and therefore it does \nnot fall for over-fitting. But context not addressed either in this approach (Katz et  al. \n2015; Cambria 2013). Search engines are used to find the meaning of unknown opinion \nlexicons. They are searched and the top N results are accepted to identify its orientation. \nThe semantics of lexicons can be categorized as positive or negative with weights rep-\nresenting their strength. This approach struggles with lexicons having domain specific \n\n\n\nPage 9 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\npolarity. For example, good has positive polarity in any type of domain but “heavy \nweight” has positive polarity for bike domain but negative for the domain of electronic \ndevices.\n\nIn its simplest form, sentiment words are split into positive and negative as binary \ndistribution. A more sophisticated approach has fuzzy lexicons, introducing a grey area \nbetween the two categories. These fuzzy lexicons exist in both the classes with a score \nassociated to it, representing the strength of each label. Various manual and semi-auto-\nmatic techniques can be used for building lexicons. Princeton University’s WordNet is \na popular lexicon source available for sentiment analysis. Dictionaries like WordNet, \nextracts synonyms and antonyms for the provided opinion words. Manual cleansing is \nemployed to rectify the lists generated for the unknown sentiment words. These opinion \nwords are used to classify a review as positive or negative.\n\nFixed syntactic patterns are also used for expressing opinions which are composed of \npart-of-speech (POS) tags. The basic idea of this technique is to identify the patterns in \nwhich words co-occur with each other and to exploit those patterns for understanding \nits semantic orientation. One example of such pattern is an adverb followed by an adjec-\ntive. A more sophisticated approach was proposed by (Mohammad and Yang 2011), \nwhich used a WordNet distance based method to determine the sentiment orientation. \nThe distance d(t1, t2) between terms t1 and t2 is the length of the shortest path that con-\nnects them in WordNet, as shown in Eq. 2. The semantic orientation (SO) of an adjective \nterm t is determined by its relative distance from two reference (or seed) terms good and \nbad. The polarity of opinion term t is resolved through eq.\n\nStatistics (Corpus) based techniques\n\nStatistical analysis of large corpus of text can also be used to determine the sentiment \norientation of words. Co-occurrence of words is evaluated without consulting any exter-\nnal support. Two methods are used for this purpose which are point wise mutual infor-\nmation (PMI) and latent semantic analysis (LSA). PMI method for co-occurrence is \ngiven as:\n\nwhere w1 and w2 refers to two words in a given sentence. The main concept behind PMI \nbased techniques is that the semantic orientation of a word has a tendency of being \nclosely related to that of its neighbors. Equation 3 gives the probability of words w1 and \nw2 to co-exist, based on the measure of degree of statistical dependence between the \ntwo. This approach is, however, implemented differently in LSA based techniques. In \nLSA, matrix factorization technique is used with singular value decomposition to dem-\nonstrate the statistical co-occurrence of words. More formally, this process can be speci-\nfied as:\n\n(2)SO(t) =\nd(t, bad)− d(t, good)\n\nd(bad, good)\n\n(3)p(w1,w2) =\np(w1,w2)\n\np(w1) p(w2)\n\n(4)LSA(w) = LSA(w, {+paradigms})− LSA(w, {−paradigms})\n\n\n\nPage 10 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nwhere a word w is passed to LSA with positive and negative paradigms. LSA based tech-\nniques develop a matrix having rows as words and columns as sentences or paragraphs. \nEach cell possesses a weight corresponding to the relation of the word in row with the \nsentence or paragraph in columns. This matrix is decomposed into three matrices using \nsingular value decomposition (SVD).\n\nComplex challenges\n\nOpinion mining is a relatively new area of research and there are open challenges that \nneed to be answered. Some of the challenges are common to opinion mining in general \nwhile others are related to their own sources and context depending upon the domain of \nthe dataset. These issues affect the performance of machine learning techniques, but it \nhas little control on them. Figure 2 gives NLP challenges faced in sentiment analysis, dis-\ntributing them into their logical groups. The groupings are based on the parsing level, at \nwhich these issues occur. The following sub section has detailed discussion on the NLP \nissues.\n\nDocument level\n\nDocument level NLP challenges are the ones that are faced at the document or review \nlevel. They deal in general with the review document or the reviewer style. It is common \nto find reviews that have the information about an object, given in an informal manner. \nCapitalization is over or under used. Spelling mistakes are ignored or words being short-\nened. It makes the analysis very difficult for the automatic techniques to identify features \nand associate them. The unknown words (shortened/miss spelled) are matched with \nsimilar words to identify the aspect or opinion words. Slang specific to a certain region \nare also occasionally used in reviews and discussions. Reviews having sarcastic expres-\nsions are the hardest to deal with. Even though they have the opinion words explicitly \nmen",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1582435,
      "metadata_storage_name": "s40294-016-0016-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDI5NC0wMTYtMDAxNi05LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Muhammad Taimoor Khan",
      "metadata_title": "Sentiment analysis and the complex natural language",
      "metadata_creation_date": "2016-02-02T06:54:52Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "customer- agent phone conversation",
        "Complex networks Open Access",
        "Complex Adapt Syst Model",
        "Creative Commons license",
        "users’ senti- ments",
        "domain specific solution",
        "profitable business decisions",
        "various online sources",
        "Complex network based",
        "specific structural information",
        "original author(s",
        "natural language processing",
        "specific open challenges",
        "natural language data",
        "data leakage analysis",
        "Muhammad Taimoor Khan1",
        "sentiment analysis techniques",
        "open issues",
        "author information",
        "language models",
        "influence analysis",
        "bias analysis",
        "ML) techniques",
        "ML techniques",
        "lexicon-based techniques",
        "statistical techniques",
        "sentiment words",
        "Sentiment orientation",
        "Mehr Durrani",
        "Armughan Ali",
        "Irum Inayat",
        "Shehzad Khalid1",
        "Habib Khan4",
        "text classification",
        "subjective statements",
        "subjective attitudes",
        "social data",
        "huge amount",
        "amateur authors",
        "large variety",
        "target entity",
        "Machine learning",
        "definite patterns",
        "high accuracy",
        "feature set",
        "external dictionary",
        "specialized domains",
        "large data",
        "Recent applications",
        "New sub-domains",
        "considerable advancements",
        "standard datasets",
        "evaluation methodology",
        "unrestricted use",
        "appropriate credit",
        "1 Bahria University",
        "Shangrilla Road",
        "Sector E",
        "Full list",
        "social media",
        "data mining",
        "opinionated content",
        "public perception",
        "opinion mining",
        "opinion content",
        "REVIEW Khan",
        "Kamran",
        "Introduction",
        "Pang",
        "Lillian",
        "type",
        "opinions",
        "order",
        "document",
        "NLP",
        "sentiments",
        "people",
        "topic",
        "features",
        "reason",
        "popularity",
        "advice",
        "others",
        "big",
        "hotspot",
        "field",
        "Hai",
        "Abstract",
        "abundance",
        "overfitting",
        "context",
        "Corpus-based",
        "proximity",
        "relationships",
        "platform",
        "meta-data",
        "nature",
        "paper",
        "need",
        "area",
        "better",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "DOI",
        "Correspondence",
        "Islamabad",
        "Pakistan",
        "end",
        "crossmark",
        "org",
        "Page",
        "19Khan",
        "Manufacturers",
        "products",
        "tory",
        "blogs",
        "forums",
        "websites",
        "graph based analysis techniques",
        "various sub streams",
        "different writing styles",
        "multi-class classification problem",
        "ML) based techniques",
        "Natural language processing",
        "many control options",
        "same sentiment polarity",
        "Sentiment analysis techniques",
        "binary classification",
        "complex network",
        "manual techniques",
        "supervised techniques",
        "overall analysis",
        "trend analysis",
        "statistical analysis",
        "complex sentences",
        "automated processes",
        "user support",
        "online datasets",
        "objective statements",
        "Such statements",
        "common-sense knowledge",
        "geographical domains",
        "gender identification",
        "fairy tales",
        "interesting patterns",
        "open challenges",
        "high frequency",
        "same product",
        "later section",
        "detailed discussion",
        "subjective content",
        "online sources",
        "single entity",
        "subjective sentence",
        "sound effects",
        "game X",
        "game Y",
        "objective sentence",
        "sentence types",
        "machine learning",
        "manual tuning",
        "domain experts",
        "large volume",
        "Opinion mining",
        "opinion patterns",
        "single opinion",
        "emotion analysis",
        "text categorization",
        "comparative sentences",
        "multiple opinions",
        "Regular opinions",
        "comparative opinions",
        "extensive use",
        "subjective nature",
        "gle word",
        "individual words",
        "NLP tasks",
        "NLP issues",
        "opinion words",
        "Simple sentences",
        "millions",
        "reviews",
        "rapid",
        "minimal",
        "way",
        "outcome",
        "form",
        "scale",
        "likeness",
        "Cambria",
        "results",
        "applications",
        "business",
        "political",
        "emails",
        "Mohammad",
        "Yang",
        "semantics",
        "date",
        "ior",
        "assumptions",
        "Documents",
        "matching",
        "fact",
        "variety",
        "difference",
        "negations",
        "NOT",
        "sarcastic",
        "compound",
        "implicit",
        "meaning",
        "aspects",
        "comparison",
        "example",
        "Bag",
        "BOW",
        "diverse subjective data sources",
        "seman- tic association",
        "Business Week survey",
        "Many complex systems",
        "complex struc- ture",
        "professional review websites",
        "other media sources",
        "social media websites",
        "The Kelsey group",
        "low quality features",
        "online data sources",
        "E-commerce websites",
        "online studies",
        "effective- ness",
        "Sentiment analysis",
        "environmental modeling",
        "wireless sensors",
        "global properties",
        "degree distribution",
        "multi-partite graphs",
        "different types",
        "large number",
        "current issues",
        "last decade",
        "ferent walks",
        "detailed analysis",
        "unique approaches",
        "popular medium",
        "various properties",
        "various clusters",
        "opinion extraction",
        "online reviews",
        "prospective customers",
        "customers’ feedbacks",
        "ad-hoc networks",
        "blogging styles",
        "external link",
        "various products",
        "adjacent nodes",
        "various entities",
        "user opinions",
        "Results",
        "repetitive",
        "polarity",
        "performance",
        "classifier",
        "cost",
        "efficiency",
        "Effectiveness",
        "precision/recall",
        "measurements",
        "relevance",
        "edges",
        "domains",
        "Niazi",
        "Aoyama",
        "Hussain",
        "information",
        "range",
        "local",
        "Text",
        "pora",
        "structural",
        "weight",
        "centrality",
        "ponents",
        "communities",
        "paths",
        "inter-cluster",
        "reasons",
        "support",
        "topics",
        "Web2.0",
        "thoughts",
        "backgrounds",
        "Katz",
        "users",
        "world",
        "ratings",
        "purchase",
        "Comscore",
        "value",
        "rise",
        "life",
        "events",
        "times",
        "discussion",
        "author",
        "feelings",
        "Chau",
        "Xu",
        "services",
        "packages",
        "promotions",
        "datasets",
        "Qiang",
        "Rob",
        "minded",
        "subjects",
        "experience",
        "articles",
        "Twitter",
        "point wise mutual information",
        "Semantic orientation based techniques",
        "centroid based classifier",
        "health care problems",
        "latent semantic analysis",
        "Lexicon based technique",
        "machine learning classifiers",
        "document level approach",
        "timent analysis techniques",
        "three different levels",
        "unsupervised statistical techniques",
        "text categoriza- tion",
        "Word level analysis",
        "sentiment level analysis",
        "social data analysis",
        "negative opinion words",
        "unsupervised techniques",
        "view point",
        "three levels",
        "sentiment analysis",
        "Semi-supervised techniques",
        "negative bins",
        "sentiment targets",
        "three groups",
        "opinion holder",
        "opinion datasets",
        "following divisions",
        "application areas",
        "multiple degrees",
        "relationship data",
        "Recommender systems",
        "data sources",
        "important aspect",
        "identifi- cation",
        "additional features",
        "dynamic environment",
        "general perception",
        "polar- ity",
        "k-nearest neighbor",
        "starting seeds",
        "Other attributes",
        "text documents",
        "docu- ments",
        "label dataset",
        "test dataset",
        "Sentiment classification",
        "review document",
        "new words",
        "other words",
        "important issues",
        "review article",
        "user data",
        "textual content",
        "previous work",
        "relevant domain",
        "multi-class classification",
        "Section 2",
        "Sect.",
        "Section 5",
        "study",
        "Section 6",
        "authors",
        "positive",
        "Tang",
        "survey",
        "Guellil",
        "Boukhalfa",
        "categories",
        "knowledge",
        "Khan",
        "Khalid",
        "SA",
        "Tuveri",
        "Angioni",
        "Zhang",
        "Liu",
        "rate",
        "training",
        "accuracy",
        "product",
        "strength",
        "Naive-Bayes",
        "pose",
        "occurrence",
        "PMI",
        "small",
        "More",
        "basis",
        "frequency",
        "location",
        "taxonomy",
        "approaches",
        "Fig.",
        "sentence",
        "presence",
        "patterns",
        "Schouten",
        "Frasincar",
        "classes",
        "formance",
        "task",
        "many equally important features",
        "high dimensional feature vectors",
        "various practical applications",
        "Naïve Bayes",
        "annotated feature vectors",
        "text classifica- tion",
        "k-nearest neighbor classifier",
        "fewer training examples",
        "senti- ment label",
        "k nearest neighbors",
        "missing value problem",
        "Naive Bayes classifier",
        "document vector dj",
        "high weights",
        "fewer chances",
        "Text data",
        "document dj",
        "training data",
        "fuzzy region",
        "final decision",
        "prob- abilities",
        "Slack variables",
        "smoothing effect",
        "noisy data",
        "fea- tures",
        "biased approach",
        "major role",
        "transfer learning",
        "classimbalance problem",
        "able value",
        "time constraint",
        "different values",
        "document di",
        "opinion document",
        "test document",
        "new document",
        "prominent features",
        "irrelevant features",
        "target dataset",
        "online classification",
        "processed dataset",
        "NB) classifier",
        "same label",
        "supervised classifiers",
        "expository literature",
        "opinion sentences",
        "pre-processing noise",
        "special case",
        "k-NN problem",
        "higher influence",
        "unrealistic assumption",
        "class ci",
        "class label",
        "k.",
        "C.",
        "labels",
        "probability",
        "confidence",
        "modifications",
        "level",
        "probabilities",
        "Taxonomy",
        "The",
        "logarithm",
        "underflow",
        "contribution",
        "Dai",
        "robustness",
        "samples",
        "solution",
        "Shin",
        "10 % improvement",
        "outliers",
        "opti",
        "External dictionary WordNet",
        "fast kNN model",
        "explicit opinion words",
        "related training examples",
        "sional feature set",
        "actual training data",
        "training data reverse",
        "large feature sets",
        "reduced set",
        "positive examples",
        "regular data",
        "feature vector",
        "feature selection",
        "decision making",
        "different categories",
        "retrieval system",
        "reduction techniques",
        "driving factors",
        "prototype vector",
        "central point",
        "Rocchio algorithm",
        "Cen- troid",
        "positive cases",
        "positive vectors",
        "different domain",
        "Smoothing techniques",
        "weighting scheme",
        "feedback looping",
        "hypothesis margin",
        "training dataset",
        "feature space",
        "sentiment terms",
        "training documents",
        "Centroid based",
        "centroid vector",
        "Centroid evaluation",
        "mum value",
        "distance calculation",
        "Mutual Information",
        "different polarity",
        "higher weights",
        "special characters",
        "Tree-fast k-NN",
        "Centroid classifier",
        "CB classifier",
        "class Centroid",
        "class similarity",
        "threshold",
        "noise",
        "Sreemathy",
        "Balamurugan",
        "curse",
        "dimensionality",
        "Accuracy",
        "Soucy",
        "Mineau",
        "indexing",
        "dj",
        "work",
        "plexities",
        "number",
        "Xia",
        "inverse",
        "pseudo-antonyms",
        "corpus",
        "Arlindo",
        "average",
        "Karypis",
        "sum",
        "Chuang",
        "Lertnattee",
        "Theeramunkong2004",
        "cosine",
        "Hidayet",
        "Tunga",
        "n-grams",
        "shortfall",
        "drawback",
        "Guan",
        "effect",
        "Chizi",
        "Ozgur",
        "Gungor",
        "Shankar",
        "∏",
        "high dimensional feature set representation",
        "decision trees classification algorithms",
        "Support vector machine classifier",
        "unsupervised sentiment analysis techniques",
        "large feature set",
        "cial neural networks",
        "Statistical analysis techniques",
        "suitable kernel function",
        "top N results",
        "incom- ing data",
        "soft margin classification",
        "Lexicon based techniques",
        "senti- ment words",
        "Centroid based classifier",
        "unknown opinion lexicons",
        "Unsupervised techniques",
        "ment phrases",
        "large dataset",
        "statistical evaluations",
        "various studies",
        "sep- aration",
        "Naive Bayes",
        "training stage",
        "dimensionality reduction",
        "sigmoid methods",
        "Kernel functions",
        "training samples",
        "boundary samples",
        "particular sense",
        "speech rules",
        "Search engines",
        "nearby words",
        "external sources",
        "two types",
        "source data",
        "text document",
        "maximum separation",
        "linear complexity",
        "one domain",
        "ative semantics",
        "general domains",
        "semantic orientation",
        "line dictionary",
        "hyperplane",
        "SVM",
        "Brown",
        "resources",
        "Feldman",
        "limitation",
        "over-reliance",
        "selection",
        "Polynomial",
        "Gaussian",
        "impact",
        "use",
        "online",
        "purpose",
        "Languages",
        "grammar",
        "detail",
        "relationship",
        "negative",
        "weights",
        "WordNet distance based method",
        "popular lexicon source",
        "singular value decomposition",
        "machine learning techniques",
        "Fixed syntactic patterns",
        "LSA based techniques",
        "matrix factorization technique",
        "unknown sentiment words",
        "Complex challenges",
        "relative distance",
        "PMI method",
        "Statistical analysis",
        "electronic devices",
        "simplest form",
        "grey area",
        "two categories",
        "Various manual",
        "Princeton University",
        "Manual cleansing",
        "POS) tags",
        "basic idea",
        "shortest path",
        "adjective term",
        "two reference",
        "nal support",
        "Two methods",
        "main concept",
        "statistical dependence",
        "three matrices",
        "new area",
        "little control",
        "sentiment orientation",
        "NLP challenges",
        "opinion term",
        "sophisticated approach",
        "fuzzy lexicons",
        "One example",
        "eq. Statistics",
        "large corpus",
        "statistical co-occurrence",
        "two words",
        "domain specific",
        "bike domain",
        "negative paradigms",
        "positive polarity",
        "good",
        "score",
        "label",
        "Dictionaries",
        "synonyms",
        "antonyms",
        "lists",
        "review",
        "speech",
        "understanding",
        "adverb",
        "t2",
        "length",
        "seed",
        "text",
        "mation",
        "w1",
        "w2",
        "tendency",
        "neighbors",
        "Equation",
        "measure",
        "degree",
        "process",
        "rows",
        "columns",
        "paragraphs",
        "cell",
        "relation",
        "SVD",
        "research",
        "general",
        "sources",
        "dataset",
        "issues",
        "Figure 2",
        "Document level NLP challenges",
        "following sub section",
        "parsing level",
        "review level",
        "logical groups",
        "reviewer style",
        "informal manner",
        "Spelling mistakes",
        "automatic techniques",
        "unknown words",
        "similar words",
        "groupings",
        "object",
        "Capitalization",
        "analysis",
        "aspect",
        "Slang",
        "region",
        "discussions"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.0393877,
      "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1373874,
      "metadata_storage_name": "s13673-019-0177-6.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTktMDE3Ny02LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Chunyong Yin ",
      "metadata_title": "Mobile marketing recommendation method based on user location feedback",
      "metadata_creation_date": "2019-04-05T10:16:31Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "current mobile marketing recommendation system",
        "Sequential behavior Open Access",
        "Mobile marketing recommendation method",
        "users’ timing preference characteristics",
        "Location-based mobile marketing recommendation",
        "potential mobile user preferences",
        "Creative Commons license",
        "convolutional neural network model",
        "users’ location feedback behavior",
        "user location feedback data",
        "mobile recommendation system",
        "traditional recommendation models",
        "user preference information",
        "creat iveco mmons",
        "users’ location-based behaviors",
        "mobile information overload",
        "user-product binary matrix",
        "original author(s",
        "different geographical locations",
        "shopping location information",
        "famous e-commerce platforms",
        "physical store products",
        "different time windows",
        "users’ location information",
        "Hum. Cent. Comput",
        "mobile network",
        "information recommendation",
        "different preferences",
        "mobile users",
        "convolutional model",
        "author information",
        "irrelevant information",
        "binary relationships",
        "different dimensions",
        "behavioral data",
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "recent years",
        "e-commerce industry",
        "one hand",
        "various products",
        "other hand",
        "large amount",
        "next consumer",
        "recommended methods",
        "third factor",
        "limited check",
        "timeli- ness",
        "hot spots",
        "accuracy rate",
        "recall rate",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Yin",
        "Inf. Sci.",
        "Communication Engineering",
        "Full list",
        "individual needs",
        "doi.org",
        "orcid.org",
        "experimental results",
        "Changsha University",
        "Introduction",
        "popularization",
        "Internet",
        "Alibaba",
        "zon",
        "home",
        "sellers",
        "goods",
        "costs",
        "wealth",
        "consumption",
        "case",
        "advantage",
        "terms",
        "Abstract",
        "attribute",
        "role",
        "paper",
        "LBCNN",
        "extractor",
        "classifier",
        "Keywords",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "jinwang",
        "csust",
        "2 School",
        "Computer",
        "Science",
        "Technology",
        "China",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "17Yin",
        "means",
        "problem",
        "implied feature-based cognitive feature collaborative filter- ing",
        "location-aware based generation probability model",
        "collaborative filtering methods",
        "biological neural networks",
        "Convolutional neural networks",
        "advanced abstract features",
        "loca- tion information",
        "accurate personalized recommendations",
        "small-scale training data",
        "detailed comparative analysis",
        "user informa- tion",
        "mobile marketing recommendation",
        "original input features",
        "business location information",
        "user preference model",
        "mobile recommendation research",
        "feature size",
        "deep learning algorithms",
        "mobile users’ preferences",
        "ing models",
        "deep model",
        "DL) model",
        "shallow model",
        "multimedia recommendations",
        "mobile environments",
        "accurate prediction",
        "local features",
        "overall features",
        "two-dimensional features",
        "user preferences",
        "location recommendation",
        "convolutional layer",
        "spatial information",
        "product information",
        "user information",
        "hot topic",
        "service resources",
        "different assumptions",
        "spatial activities",
        "location movement",
        "negative samples",
        "semantic content",
        "data size",
        "mation analysis",
        "long-term interest",
        "time stamps",
        "great achievements",
        "speech tasks",
        "novel field",
        "interventional optimization",
        "artificial intelligence",
        "great breakthroughs",
        "similar effects",
        "Weight sharing",
        "great advantages",
        "user needs",
        "positional relevance",
        "location relevance",
        "first layer",
        "previous layer",
        "convolution kernel",
        "pooling layer",
        "context information",
        "ventional methods",
        "user models",
        "many researches",
        "two layers",
        "network structure",
        "shortcomings",
        "Zhu",
        "retrograde",
        "approach",
        "Yin",
        "LA-LDA",
        "scoring",
        "items",
        "studies",
        "distance",
        "merchant",
        "area",
        "proximity",
        "importance",
        "gap",
        "core",
        "Lian",
        "impact",
        "author",
        "project",
        "Lee",
        "visual",
        "study",
        "aspects",
        "machine",
        "Experiments",
        "characteristics",
        "CNN",
        "difficulty",
        "number",
        "weights",
        "neuron",
        "Hum",
        "Cent",
        "Comput",
        "images",
        "location-based",
        "Location-based mobile marketing recommendation model",
        "Content‑based recommendation method",
        "convolutional neural network models",
        "Representative content-based recommendation systems",
        "general products recommendation system",
        "Traditional recommendation method",
        "traditional recommendation algorithm",
        "content-based recommendation algorithm",
        "similar Top-N products",
        "timing preference characteristics",
        "interest feature vector",
        "Content-based information filtering",
        "Experimental analysis” section",
        "item feature vector",
        "project feature vector",
        "other existing methods",
        "cosine similarity algorithm",
        "Related work” section",
        "user feature vector",
        "users’ timing preferences",
        "User description file",
        "feature models",
        "Content-based methods",
        "content filtering",
        "content characteristics",
        "other methods",
        "user-product information",
        "collaborative filtering",
        "feedback information",
        "hybrid methods",
        "historical content",
        "training sample",
        "next moment",
        "time window",
        "overall preferences",
        "Top-K sample",
        "four sections",
        "Necessary definitions",
        "specific implementation",
        "future progress",
        "current chapter",
        "three parts",
        "time series",
        "text documents",
        "accurate comparisons",
        "different texts",
        "lowing aspects",
        "Hum. Cent",
        "project profile",
        "user u",
        "similarity calculation",
        "sample features",
        "cast results",
        "effective application",
        "calculation formula",
        "two-class problem",
        "purchase behavior",
        "category",
        "order",
        "merchandise",
        "length",
        "test",
        "Remain",
        "sion",
        "strengths",
        "weaknesses",
        "plans",
        "establishment",
        "differences",
        "research",
        "threshold",
        "a.",
        "topics",
        "projects",
        "purchasing",
        "operation",
        "addition",
        "modulus",
        "Lops",
        "Gemmis",
        "Semeraro",
        "content filtering based recommendation method",
        "m * n matrix R",
        "traditional data mining method",
        "user historical behavior data",
        "traditional collaborative filtering method",
        "likely N objects",
        "user behavior data",
        "various draw- backs",
        "nearest neigh- bor",
        "conven- tional method",
        "association rule algorithm",
        "higher evaluation algorithms",
        "hybrid recommendation algorithm",
        "similar user sets",
        "mutual user relationships",
        "content-based recommendation method",
        "Hybrid recommendation method",
        "other similar users",
        "different recommendation algorithms",
        "The matrix  Umn",
        "content-based filtering",
        "other method",
        "hybrid method",
        "matrix decomposition",
        "different relationships",
        "One method",
        "new content",
        "evaluation standard",
        "association rules",
        "recommendation problem",
        "standard recommendation",
        "two algorithms",
        "different ways",
        "target user",
        "user neighbor",
        "cold-start issues",
        "same type",
        "same partition",
        "expert research",
        "three steps",
        "recommendation results",
        "ommended items",
        "normal methods",
        "supervised learning",
        "online stores",
        "informa- tion",
        "new users",
        "two types",
        "two specific",
        "compensa- tion",
        "different values",
        "common features",
        "rating sparsity",
        "erence information",
        "new projects",
        "mendations",
        "availability",
        "context",
        "party",
        "requirements",
        "merits",
        "quality",
        "memory",
        "following",
        "collection",
        "feedback",
        "range",
        "U11",
        "U1n",
        "U2n",
        "Um1",
        "classification",
        "survey",
        "Amazon",
        "personalized",
        "customer",
        "ratings",
        "techniques",
        "performance",
        "ally",
        "degree",
        "improvement",
        "complexity",
        "different timing behavior characteristics",
        "good recom- mendations",
        "relevant professional research",
        "T-time consumer behavior",
        "behavioral counting feature",
        "duplication counting feature",
        "feature statistics window",
        "historical behavior data",
        "timing behavior data",
        "user position feedback",
        "similar behavioral items",
        "collaborative filtering algorithm",
        "next purchase behavior",
        "feature statistics method",
        "user behavior record",
        "time series behavior",
        "timing recommendation model",
        "user interface layer",
        "U, V data",
        "high quality rules",
        "last behavior",
        "behavior count",
        "Different actions",
        "different times",
        "U data",
        "next section",
        "current window",
        "feature group",
        "product purchase",
        "recommendation process",
        "recommendation systems",
        "ready-made data",
        "data sets",
        "many years",
        "core idea",
        "rule algorithms",
        "total project",
        "disjoint sets",
        "association rule",
        "two criteria",
        "transac- tions",
        "key- word",
        "description layer",
        "main function",
        "strong dependence",
        "most systems",
        "time information",
        "time label",
        "Markov chain",
        "full use",
        "prediction problem",
        "time T",
        "three groups",
        "processing manner",
        "cumulative measure",
        "keyword layer",
        "user layer",
        "unknown items",
        "traditional algorithm",
        "preference features",
        "corresponding features",
        "location visit",
        "total number",
        "∩V",
        "V.",
        "∪ V",
        "business",
        "effect",
        "quantity",
        "focus",
        "transactions",
        "statement",
        "strength",
        "support",
        "confidence",
        "ratio",
        "Formula",
        "interest",
        "other",
        "TOP-N",
        "presentation",
        "attributes",
        "dependencies",
        "keywords",
        "resource",
        "users",
        "dataset",
        "explanation",
        "Construction",
        "Fig.",
        "example",
        "user1",
        "Table",
        "False",
        "behaviors",
        "Location‑based mobile marketing recommendation model",
        "Counting feature Mean feature Ratio feature",
        "location-based mobile marketing recom",
        "Table 1 Characteristic system diagram",
        "collaborative filtering calculation process",
        "False True User feature",
        "K user preference feature",
        "pooled feature vector",
        "evaluation crite- rion",
        "ior preference feature",
        "duplication count feature",
        "different feature maps",
        "repetitive behavioral data",
        "Specific implementa- tion",
        "pool- ing layer",
        "multi-window convolutional layer",
        "real-world work applications",
        "location visit behavior",
        "N time windows",
        "multi-window convolution layer",
        "user characteristics group",
        "products’ total visit",
        "user position features",
        "counting features",
        "Feature group",
        "True False",
        "feature expression",
        "product location",
        "behavioral count",
        "network structures",
        "input feature",
        "Model framework",
        "different lengths",
        "Product feature",
        "mean-type features",
        "output layer",
        "time axis",
        "input layer",
        "total behavior",
        "current research",
        "low difficulty",
        "mendation quality",
        "following sections",
        "important aspects",
        "next step",
        "evant definitions",
        "timing sensitivity",
        "timing behav",
        "four layers",
        "two-dimensional plane",
        "ture plane",
        "The model",
        "Specific information",
        "average number",
        "user-product behavior",
        "product preferences",
        "relevant definition",
        "K.",
        "Definition 1",
        "Definition 2",
        "visits",
        "activity",
        "popularity",
        "method",
        "usable",
        "achievement",
        "speed",
        "avgui",
        "Ij",
        "True/False",
        "sure",
        "correctness",
        "basis",
        "above",
        "analysis",
        "eigenvector",
        "commodity",
        "matrix",
        "new location-based marketing resources recommendation",
        "location-based mobile marketing resources",
        "M convolution kernels",
        "convolu- tion kernel",
        "gradient descent method",
        "convolution kernel w",
        "C × M matrix",
        "nonlinear activation function",
        "kth implicit class",
        "implicit semantic model",
        "maximum pooling operation",
        "latent factor vector",
        "training data set",
        "latent factor model",
        "traditional CNN parameters",
        "likelihood probability value",
        "i-th feature fi",
        "kth product",
        "convolutional kernel",
        "training method",
        "Dropout method",
        "CNN inputs",
        "i-th category",
        "loss function",
        "i-th sample",
        "Feature map",
        "pooled feature",
        "training results",
        "window length",
        "offset term",
        "maximum features",
        "C categories",
        "M-dimensional vector",
        "k-th offset",
        "real category",
        "two items",
        "same time",
        "same class",
        "pTu qi",
        "hidden classes",
        "Specific implementation",
        "two processes",
        "two parts",
        "top module",
        "historical data",
        "other module",
        "second pro",
        "traditional LFM",
        "L2-norm regularization",
        "smoothing problems",
        "training process",
        "dation process",
        "model parameters",
        "Probability distribution",
        "real number",
        "weight parameter",
        "many users",
        "user interest",
        "∑C",
        "j.",
        "framework",
        "ReLu",
        "Tanh",
        "Definition",
        "network",
        "fk",
        "bk",
        "teristic",
        "Zeiler",
        "fitting",
        "neurons",
        "behavior",
        "high",
        "Pool_feature",
        "log",
        "rui",
        "relationship",
        "outputs",
        "excessive",
        "σ",
        "θ",
        "existing authoritative standard training set",
        "Gradient Boosting Regression Tree Model",
        "mining users’ temporal behavior characteristics",
        "Linear Logistic Regression Classification Model",
        "offline training model phase",
        "location-based marketing resources",
        "convolutional neural network",
        "hyper parameter settings",
        "real-time recommendation stage",
        "LBCNN model structure",
        "Random Forest Model",
        "past behavior data",
        "matrix decomposition method",
        "network model parameters",
        "LFM training data",
        "user bias item",
        "item offset item",
        "regression analysis",
        "linear model",
        "classification models",
        "model training",
        "training section",
        "training features",
        "behavior information",
        "previous data",
        "language model",
        "same model",
        "real-time performance",
        "LFM results",
        "sion coefficient",
        "specification results",
        "next question",
        "two parameters",
        "intrinsic property",
        "positive samples",
        "negative sample",
        "first thing",
        "historical score",
        "implied tag",
        "Experimental analysis",
        "timing preferences",
        "same conditions",
        "Vector Machine",
        "Experimental tool",
        "sklearn kit",
        "L2 regular",
        "regularization term",
        "regularization coefficient",
        "user tag",
        "rating score",
        "User information",
        "kind",
        "thinking",
        "sparseness",
        "analyses",
        "solution",
        "premise",
        "mula",
        "calculation",
        "puk",
        "average",
        "qik",
        "public",
        "experience",
        "latter",
        "overfitting",
        "Description",
        "input",
        "problems",
        "output",
        "L1-norm",
        "part",
        "update",
        "background",
        "people",
        "products",
        "advance",
        "period",
        "advantages",
        "several",
        "LR",
        "Support",
        "SVM",
        "GBDT",
        "8 h",
        "λ",
        "∑",
        "lion users’ various behaviors",
        "mobile recommendation algorithm contest",
        "radial basis kernel function",
        "personalized recommendation model",
        "feature segmentation standard",
        "random feature ratio",
        "real business sce",
        "product category information",
        "data set",
        "Behavior types",
        "behavior record",
        "behavior time",
        "c. RF",
        "learning rate",
        "maximum depth",
        "Alibaba Group",
        "offline type",
        "RBF",
        "gamma",
        "trees",
        "entropy",
        "experiment",
        "1 month",
        "2,876,947 items",
        "clicks",
        "shopping",
        "carts",
        "purchases",
        "hour",
        "online",
        "nario",
        "subset"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.369608,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1344318,
      "metadata_storage_name": "s40537-019-0210-7.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDIxMC03LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Taiwo Kolajo ",
      "metadata_title": "Big data stream analysis: a systematic literature review",
      "metadata_creation_date": "2019-06-04T14:40:29Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "effective resource allocation strategy",
        "existing data mining tools",
        "big data stream tools",
        "data stream computing mode",
        "big data streaming tools",
        "big data streaming ana",
        "Big data stream analysis",
        "big data stream computing",
        "big data streams analysis",
        "big data batch computing",
        "Creative Commons license",
        "big data computing",
        "big data tool",
        "J Big Data",
        "several computational challenges",
        "process- ing requirements",
        "inherent dynamic characteristics",
        "Three major databases",
        "first search string",
        "standard benchmark dataset",
        "creat iveco mmons",
        "load balancing issues",
        "nificant research efforts",
        "technologies Open Access",
        "SURVEY PAPER Kolajo",
        "empirical analysis",
        "real-time analysis",
        "future computing",
        "data sources",
        "parallelization issues",
        "research questions",
        "literature review",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "two types",
        "huge amount",
        "methodical approach",
        "global view",
        "exclusion criteria",
        "preprocessing stage",
        "iterative jobs",
        "scalable frameworks",
        "growing size",
        "unrestricted use",
        "appropriate credit",
        "original author",
        "Information Sciences",
        "Covenant University",
        "Full list",
        "author information",
        "doi.org",
        "orcid.org",
        "information technology",
        "large volume",
        "great velocity",
        "systematic review",
        "initial 2295 papers",
        "Taiwo Kolajo",
        "47 papers",
        "Introduction",
        "Advances",
        "high-velocity",
        "ability",
        "nature",
        "terms",
        "variety",
        "veracity",
        "volatility",
        "value",
        "new",
        "trend",
        "Abstract",
        "fact",
        "number",
        "applications",
        "methods",
        "techniques",
        "rigorous",
        "comparisons",
        "Scopus",
        "ScienceDirect",
        "EBSCO",
        "journals",
        "conferences",
        "entities",
        "IEEE",
        "ACM",
        "SpringerLink",
        "Elsevier",
        "inclusion",
        "study",
        "privacy",
        "attention",
        "key",
        "features",
        "lytics",
        "conclusion",
        "algorithms",
        "complexity",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "changes",
        "Correspondence",
        "fulokoja",
        "1 Department",
        "Computer",
        "Ota",
        "Nigeria",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "Page",
        "30Kolajo",
        "scalable com- puting platforms",
        "big data streaming analytics",
        "big data stream analysis",
        "Big data batch processing",
        "Streaming processing frameworks",
        "stream processing paradigms",
        "Stream processing solutions",
        "data flow graph",
        "real-time application scenarios",
        "real-time, high volume",
        "time data analysis",
        "real-time data stream",
        "batch computing",
        "stream computing",
        "stream processor",
        "high-velocity flow",
        "incoming data",
        "data generating",
        "market data",
        "real-time sources",
        "changing conditions",
        "common understanding",
        "ben- efit",
        "scientific community",
        "key issues",
        "detailed evaluation",
        "massive amount",
        "high- velocity",
        "multiple sources",
        "low latency",
        "new sources",
        "location services",
        "mobile devices",
        "sensor pervasiveness",
        "fundamental assumption",
        "potential value",
        "parallel architectures",
        "decision-making process",
        "static questions",
        "continuous queries",
        "diverse sources",
        "sideration availability",
        "fault tolerance",
        "infinite tuple",
        "actionable results",
        "interconnected streams",
        "related work",
        "research works",
        "Research method",
        "crucial need",
        "real contrasts",
        "ming paradigm",
        "Discussion” section",
        "Result” section",
        "addition",
        "output",
        "low-latency",
        "seconds",
        "demand",
        "reason",
        "huge",
        "organisations",
        "businesses",
        "paper",
        "purpose",
        "overview",
        "findings",
        "implications",
        "practice",
        "update",
        "state",
        "areas",
        "challenges",
        "rest",
        "Background",
        "information",
        "Limitation",
        "Conclusion",
        "ubiquity",
        "Internet",
        "Things",
        "Sensors",
        "freshness",
        "Storm",
        "Kafka",
        "Spark",
        "Table",
        "motion",
        "essence",
        "fly",
        "scalability",
        "assimilation",
        "production",
        "operations",
        "Fig.",
        "Dimension Batch processing Streaming processing Input Data chunks",
        "Hardware Multiple CPUs Typical single limited amount",
        "big data stream computing environments",
        "big data streaming analysis",
        "domain Web mining",
        "streaming analytics system",
        "big data streams",
        "Data flow graph",
        "high data rates",
        "data processing node",
        "memory Storage Store",
        "new incoming data",
        "ble efficient operations",
        "typical example",
        "multiple rounds",
        "Such processing",
        "single pass",
        "new data",
        "missing data",
        "data normalization",
        "distributed system",
        "High fault-tolerance",
        "life-critical systems",
        "data tuples",
        "Data size",
        "new information",
        "new tuple",
        "new trends",
        "model predictions",
        "feature extraction",
        "external feeds",
        "non-trivial portion",
        "traffic monitoring",
        "sensor networks",
        "Key issues",
        "useful knowledge",
        "current happenings",
        "speedy manner",
        "organisa- tions",
        "load balancing",
        "privacy issues",
        "exponential growth",
        "computer resources",
        "research efforts",
        "effective resource",
        "cation strategy",
        "com- plexity",
        "small number",
        "different datasets",
        "component failure",
        "time-sensitive processes",
        "security threats",
        "data Time",
        "latest tuples",
        "analytic applications",
        "sliding window",
        "milliseconds Applications",
        "logical container",
        "main challenges",
        "Fault‑tolerance",
        "integration technique",
        "results",
        "operators",
        "models",
        "access",
        "idea",
        "duplicates",
        "parsing",
        "windows",
        "Table 1",
        "Comparison",
        "updates",
        "advance",
        "passes",
        "figure",
        "need",
        "order",
        "problems",
        "performance",
        "timeliness",
        "consistency",
        "heterogeneity",
        "incompleteness",
        "accuracy",
        "The",
        "way",
        "processors",
        "Moore",
        "law",
        "fore",
        "views",
        "interruption",
        "big data stream computing system",
        "organisational resource management specifi",
        "big data stream dataset",
        "Big data stream analytics",
        "good multiple instances replication",
        "big data analytics literature",
        "big data analytics methods",
        "good system structure",
        "distributing envi- ronment",
        "effective tech- niques",
        "Big data streams",
        "competent data presentation",
        "national Data Cooperation",
        "big data analysis",
        "data streams changes",
        "partial data streams",
        "previous research efforts",
        "big data streaming",
        "big data challenges",
        "systematic literature review",
        "High throughput Decision",
        "big threat",
        "streaming analytics",
        "streaming” analytics",
        "streaming data",
        "natural disaster",
        "continuous processing",
        "local views",
        "accessi- bility",
        "meaningful content",
        "Load balancing",
        "load shedding",
        "peak loads",
        "average load",
        "global centre",
        "entire information",
        "needs protection",
        "main objectives",
        "future observations",
        "processing algorithms",
        "stream-specific requirements",
        "various tools",
        "research focus",
        "business values",
        "high consistency",
        "high accuracy",
        "main challenge",
        "communicating nodes",
        "individual privacy",
        "ent characteristics",
        "Related work",
        "The study",
        "reviews",
        "fraud",
        "tures",
        "platforms",
        "architecture",
        "minimal",
        "latency",
        "stability",
        "Heterogeneity",
        "semantics",
        "granularity",
        "correlate",
        "real-time",
        "diversity",
        "hierarchy",
        "resources",
        "variance",
        "result",
        "respect",
        "sub-graph",
        "replicas",
        "portion",
        "issue",
        "opportunities",
        "IDC",
        "half",
        "volume",
        "velocity",
        "variability",
        "consideration",
        "section",
        "technologies",
        "Authors",
        "definitions",
        "types",
        "technology",
        "cally",
        "four big data streaming tools",
        "big data stream analysis tools",
        "big data stream literature review",
        "authoritative, full-text scientific, technical",
        "data stream analysis reviews",
        "big data stream framework",
        "big data stream processing",
        "big data stream algorithms",
        "big data stream analytics",
        "three standard database indexes",
        "big data stream technologies",
        "big data analytics",
        "machine learning algorithms",
        "full-text data- bases",
        "large data- base",
        "smart intuitive functionality",
        "electronic journal service",
        "big data technologies",
        "academic journal articles",
        "leading information solution",
        "systematic mapping method",
        "following research questions",
        "good search string",
        "Data sources",
        "four categories",
        "peer-reviewed literature",
        "citation database",
        "standard databases",
        "information professionals",
        "bibliographic database",
        "empirical research",
        "same vein",
        "particular focus",
        "anomaly detection",
        "tech- niques",
        "clear explanation",
        "Relevant publications",
        "Science Direct",
        "hensive overview",
        "research output",
        "social sciences",
        "largest abstract",
        "open access",
        "health publications",
        "14 million publications",
        "Life Sciences",
        "Physical Sciences",
        "Health Sciences",
        "wide range",
        "academic researchers",
        "application areas",
        "healthcare profes",
        "evaluation techniques",
        "peer-reviewed journals",
        "3800 journals",
        "status",
        "survey",
        "scope",
        "comprehensive",
        "differences",
        "concept",
        "capabilities",
        "limitations",
        "strengths",
        "benchmarks",
        "population",
        "son",
        "intervention",
        "outcome",
        "keywords",
        "searches",
        "EBSCOhost",
        "rich",
        "abstracts",
        "citations",
        "36,377 titles",
        "11,678 publishers",
        "world",
        "arts",
        "humanities",
        "students",
        "teachers",
        "35,000 books",
        "Engineering",
        "porate",
        "Third Search string refinement result",
        "free online professional network",
        "First search string result",
        "Second search string result",
        "Scopus ScienceDirect EBSCOhost Total",
        "major academic publishers",
        "nine (9) search strings",
        "high impact journals",
        "111 seemingly relevant papers",
        "Inclusion criteria Papers",
        "Table 5 Final Selection",
        "Further refinement",
        "Data retrieval",
        "easy analysis",
        "high-quality e-books",
        "100 million publications",
        "secondary source",
        "rich databases",
        "Boolean ‘OR",
        "2295 arti- cles",
        "three databases",
        "computer science",
        "subject domain",
        "quick overview",
        "Microsoft Excel",
        "three categories",
        "black colour",
        "similar investigations",
        "following categories",
        "primary study",
        "source language",
        "relevant” papers",
        "11 million researchers",
        "year range",
        "total number",
        "peer-reviewed conferences",
        "recent papers",
        "16,711 journals",
        "Table 2",
        "Table 3",
        "Table 4",
        "1989 papers",
        "315 papers",
        "111 papers",
        "45 papers",
        "18 papers",
        "magazine",
        "titles",
        "60,000 audiobooks",
        "iv.",
        "ResearchGate4",
        "scientists",
        "collaborators",
        "authors",
        "subscription",
        "set",
        "reseaarchgate",
        "sources",
        "interest",
        "stage",
        "PDF",
        "introduction",
        "green",
        "red",
        "colours",
        "end",
        "workshops",
        "technical",
        "symposium",
        "case",
        "part",
        "English",
        "contributions",
        "900,000",
        "1500",
        "alternative big data streaming solutions",
        "effective data management decisions",
        "spike load profile platform",
        "Big data stream platforms",
        "Streaming data sources",
        "many NoSQL databases",
        "specific application interfaces",
        "data loading procedure",
        "enterprise technology vendors",
        "open source community",
        "open source solutions",
        "Workload profile",
        "proprietary solutions",
        "stream applications",
        "high-velocity data",
        "data stores",
        "recent technology",
        "Such platforms",
        "platform distribution",
        "Data access",
        "Several tools",
        "sary tools",
        "single flow",
        "growing demand",
        "pro- jection",
        "different structures",
        "different ways",
        "CAP theorem",
        "consistent loads",
        "consistent flows",
        "web-based services",
        "soft- ware",
        "critical functions",
        "Latency requirement",
        "minimal delay",
        "key-value stores",
        "memory solution",
        "licensing issues",
        "limited maturity",
        "developer communities",
        "modification challenges",
        "large datasets",
        "large scale",
        "network partition",
        "service deployment",
        "service cloud",
        "premise approach",
        "easy integration",
        "consistency requirement",
        "careful selection",
        "serialization technologies",
        "execution",
        "functionalities",
        "response",
        "factors",
        "Shape",
        "capturing",
        "storing",
        "rep",
        "instance",
        "room",
        "flexibility",
        "storage",
        "users",
        "Availability",
        "presence",
        "break",
        "scenario",
        "requests",
        "Infrastructure",
        "option",
        "processing",
        "predictable",
        "workloads",
        "spikes",
        "combination",
        "go",
        "Tables",
        "pricing",
        "innovation",
        "development",
        "lack",
        "support",
        "outdating",
        "problem",
        "big data stream analysis Tools",
        "Multinomial latent dirichlet allocation",
        "Elastic streaming processing engine",
        "real-time social media data",
        "Microsoft azure stream analytics",
        "old-based stream clustering approaches",
        "Incremental algorithm threshold setting",
        "Table 6 Open source tools",
        "social media analysis",
        "social media streams",
        "high dimensional data",
        "Markov Random Field",
        "Sentiment brand monitoring",
        "IBM InfoSphere streams",
        "Density-based clustering algorithm",
        "Voltage clustering algorithm",
        "maximum similarity threshold",
        "little research efforts",
        "incremental clustering approaches",
        "Online Spherical K-means",
        "Incremental approaches",
        "Splunk stream",
        "incoming stream",
        "data grouping",
        "Proprietary tools",
        "WSO2 analytics",
        "Microsoft StreamInsight",
        "clustering algorithms",
        "Spark streaming",
        "Complete Clustering",
        "hierarchical clustering",
        "online clustering",
        "Research Question",
        "true costs",
        "Apache storm",
        "Apache Samza",
        "Apache Aurora",
        "Apache Kylin",
        "dynamic nature",
        "desirable number",
        "prior knowledge",
        "scalable graph",
        "limited space",
        "apriori number",
        "Google MillWheel",
        "TIBCO StreamBase",
        "Kyvos insights",
        "Lambda architecture",
        "Much work",
        "fragmentation issues",
        "tive approach",
        "static values",
        "metric learning",
        "partitioning algorithms",
        "technology Article",
        "balanced partitioning",
        "time window",
        "Condensed Clusters",
        "existing clusters",
        "Threshold-based techniques",
        "Table 8 Methods",
        "Table 7",
        "understanding",
        "benefits",
        "BlockMon",
        "NoSQL",
        "Photon",
        "MavEStream",
        "EsperTech",
        "Redis",
        "C-SPARQL",
        "SAMOA",
        "CQELS",
        "ETALIS",
        "XSEQ",
        "k-median",
        "k-medoid",
        "expectation-maximization",
        "tendency",
        "decisions",
        "DenStream",
        "OpticStream",
        "Exclusive",
        "outliers",
        "HDDStream",
        "PreDeCon-Stream",
        "PKS-Stream",
        "memory",
        "face",
        "CodeBlue",
        "Anodot",
        "Cloudet",
        "Numenta",
        "Artemis",
        "Striim",
        "AtScale",
        "efficiency",
        "SPADE",
        "LSML",
        "KTS",
        "Dynamic prime-number based security verification",
        "User profile vector update algorithm",
        "Incremental MI outlier detection algorithm",
        "Singular spectrum matrix completion",
        "Temporal fuzzy concept analysis",
        "Tag assignment stream clustering",
        "QRS detection algorithm",
        "cloud computing algorithm",
        "Parallel K-means clustering",
        "Locality sensitive hashing",
        "Forward chaining rule",
        "Continuous query processing",
        "Multi-query optimization strategy",
        "Outlier method",
        "Density cognition",
        "Inc I-MLOF",
        "Adaptive windowing",
        "online ensemble",
        "Nearest neighbour",
        "Markov chains",
        "LSH",
        "TASC",
        "StreamMap",
        "CluStream",
        "HPClustering",
        "D-Stream",
        "DCStream",
        "P-Stream",
        "ADStream",
        "CQR",
        "FPSPAN-growth",
        "OMCA",
        "MQOS",
        "automata",
        "VPA",
        "AWOE",
        "K-anonymity",
        "closeness",
        "ECM-sketch",
        "Block-QuickSort-AdjacentJobMatch",
        "Block-QuickSort-OverlapReplicate"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.029126,
      "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like precision i",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 514849,
      "metadata_storage_name": "s13173-013-0117-7.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzE3My0wMTMtMDExNy03LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2013-10-11T02:33:25Z",
      "keyphrases": [
        "São José dos Campos",
        "J Braz Comput Soc",
        "Ciências Matemáticas",
        "The Brazilian Computer Society",
        "São Paulo",
        "São Carlos",
        "Ciência e",
        "Paulo Henrique Pisani",
        "Ana Carolina Lorena",
        "typ- ing rhythm",
        "P. H. Pisani",
        "A. C. Lorena",
        "behavioral biometric technology",
        "effective security mechanism",
        "Behavioral intrusion detection",
        "formal review protocol",
        "intrusion detection systems",
        "user profil- ing",
        "secure authentication methods",
        "systematic review method",
        "Computação",
        "behavioral analysis",
        "communication systems",
        "security problems",
        "sophisticated methods",
        "The process",
        "Systematic reviews",
        "keystroke dynamics",
        "Abstract Computing",
        "data exposure",
        "ric technologies",
        "possible technologies",
        "future researches",
        "rigorous procedure",
        "ficial intelligence",
        "performance measures",
        "mark datasets",
        "digital identities",
        "greater worries",
        "information exposure",
        "several activities",
        "online banking",
        "new momentum",
        "personal information",
        "system initialization",
        "several aspects",
        "regular behavior",
        "great variety",
        "rigorous method",
        "initial authentication",
        "possible way",
        "Instituto de",
        "Universidade Federal",
        "wider dissemination",
        "identity theft",
        "imate user",
        "potential intrusions",
        "SURVEY PAPER",
        "user authentication",
        "DOI",
        "life",
        "issue",
        "work",
        "users",
        "order",
        "area",
        "definition",
        "results",
        "state",
        "art",
        "sifiers",
        "features",
        "Keywords",
        "Biometrics",
        "ICMC",
        "USP",
        "mail",
        "phpisani",
        "Tecnologia",
        "ICT",
        "UNIFESP",
        "aclorena",
        "1 Introduction",
        "increased",
        "net",
        "commerce",
        "term",
        "crime",
        "someone",
        "scenario",
        "case",
        "example",
        "intruders",
        "session",
        "ways",
        "concept",
        "fact",
        "models",
        "profile",
        "identification",
        "deviations",
        "18",
        "24",
        "10",
        "S N User profile Store profile",
        "regular typing rhythm",
        "possible devia- tions",
        "keystroke dynam- ics",
        "system audit logs",
        "command line use",
        "user behavior pattern",
        "systematic bibliographic reviews",
        "Behavioural intrusion detection",
        "two major steps",
        "intrusion detection system",
        "possible incidents",
        "behaviour pattern",
        "anomaly detection",
        "user profiling",
        "same user",
        "systematic procedure",
        "behavioural IDS",
        "other researchers",
        "other areas",
        "software engineering",
        "artificial intelligence",
        "remaining sections",
        "basic concepts",
        "security policies",
        "acceptable use",
        "security practices",
        "mod- els",
        "basic flow",
        "key issue",
        "similar values",
        "keyboard input",
        "user behaviour",
        "User Training",
        "review method",
        "normal behaviour",
        "valuable information",
        "information security",
        "Training phase",
        "explicit protocols",
        "new concept",
        "Application usage",
        "same time",
        "Preliminary results",
        "An IDS",
        "name",
        "formal",
        "conduction",
        "way",
        "validation",
        "incidence",
        "bias",
        "problem",
        "medicine",
        "number",
        "benefits",
        "computing",
        "paper",
        "tributes",
        "aim",
        "Sect.",
        "process",
        "conclusions",
        "2 Background",
        "events",
        "computer",
        "signals",
        "violations",
        "threats",
        "analysis",
        "type",
        "alarms",
        "Recognition",
        "Fig.",
        "Figure",
        "matching",
        "aspects",
        "questions",
        "group",
        "monitoring",
        "systems audit logs",
        "lower error rates",
        "support vector machines",
        "Two distinctive processes",
        "machine learning algorithms",
        "Systematic literature review",
        "two ways",
        "several algorithms",
        "other aspects",
        "continuous authentication",
        "mail behav",
        "less effort",
        "first process",
        "second process",
        "neural networks",
        "less susceptibility",
        "Software Engineering",
        "major phases",
        "first phase",
        "second phase",
        "secondary study",
        "third phase",
        "final report",
        "check- ing",
        "specific knowledge",
        "important part",
        "primary studies",
        "eval- uation",
        "essential part",
        "other researches",
        "3 Systematic review",
        "Protocol evaluation",
        "systematic reviews",
        "dynamic text",
        "Static text",
        "bibliographic reviews",
        "review protocol",
        "feature classification",
        "classifica- tion",
        "pro- tocol",
        "three phases",
        "cific topic",
        "following steps",
        "research questions",
        "feature extraction",
        "formal way",
        "review need",
        "typing features",
        "specific user",
        "search strategies",
        "rhythms",
        "model",
        "technology",
        "iour",
        "expressions",
        "password",
        "recognition",
        "terms",
        "instance",
        "addition",
        "completion",
        "Medicine",
        "Computing",
        "application",
        "planning",
        "presentation",
        "information",
        "references",
        "items",
        "Identification",
        "goal",
        "existence",
        "investigation",
        "requirements",
        "Commissioning",
        "cases",
        "lack",
        "time",
        "Specification",
        "Development",
        "selection",
        "576 J Braz Comput Soc",
        "greatest possible number",
        "newer review article",
        "systematic review process",
        "traditional reviews",
        "inclusion/exclusion criteria",
        "Data synthesis",
        "quantitative aspects",
        "academic journals",
        "web sites",
        "two researchers",
        "next sections",
        "intrusion detection",
        "Classification algorithms",
        "Performance measures",
        "Benchmarking datasets",
        "comparative experiments",
        "respective sub-questions",
        "typing data",
        "Reference search",
        "search expressions",
        "planning phase",
        "Quality evaluation",
        "research question",
        "diverse aims",
        "Information extraction",
        "dissemination mechanisms",
        "explicit definition",
        "More details",
        "Extracted features",
        "reference databases",
        "conduction phase",
        "previous work",
        "Report evaluation",
        "summary results",
        "rigour",
        "Selection",
        "use",
        "importance",
        "study",
        "support",
        "forms",
        "step",
        "qualitative",
        "meta-analysis",
        "mulation",
        "publishing",
        "conferences",
        "experts",
        "publication",
        "principles",
        "phases",
        "topic",
        "Advantages",
        "Internet",
        "English",
        "authors",
        "part",
        "need",
        "3.2",
        "1.1",
        "logical connective O R.",
        "logic connective AN D",
        "personal- ization systems",
        "authentica- tion systems",
        "behaviour intrusion detection",
        "behavior intrusion detection",
        "ACM Digital Library",
        "biometric intrusion detection",
        "behavioural intrusion detection",
        "behavioral intrusion detection",
        "basic search expression",
        "following exclusion criteria",
        "keystroke biometric(s",
        "recommender systems",
        "1.3 Selection criteria",
        "keypress biometric",
        "behavioral IDS",
        "behavioural biometrics",
        "keystroke pattern",
        "keystroke authentication",
        "behavioral biometrics",
        "many users",
        "typing pattern",
        "uous authentication",
        "same topic",
        "personalized information",
        "ized content",
        "content delivery",
        "recommendation system",
        "ommendations system",
        "information retrieval",
        "N OT",
        "several data-bases",
        "computing area",
        "data- base",
        "Appendix A",
        "next steps",
        "one page",
        "keystroke biometrics",
        "typing dynamics",
        "web search",
        "keystroke analysis",
        "unrelated topics",
        "IEEE Xplore",
        "Science Direct",
        "last part",
        "systematic review",
        "various terms",
        "exclusion terms",
        "performance comparisons",
        "1.2 References search",
        "OR ‘‘personalization",
        "rithms",
        "measures",
        "datasets",
        "classifier",
        "tests",
        "list",
        "papers",
        "completeness",
        "database",
        "conjunction",
        "reason",
        "differences",
        "syntax",
        "org",
        "ieeexplore",
        "sciencedirect",
        "Scopus",
        "inclusion",
        "Publications",
        "requirement",
        "posters",
        "presentations",
        "cts",
        "editorials",
        "texts",
        "magazines",
        "newspaper",
        "578 J Braz Comput Soc",
        "enough informa- tion",
        "use- ful features",
        "high- light references",
        "complete version",
        "unnecessary work",
        "different versions",
        "restricted access",
        "The value",
        "possible scores",
        "research criteria",
        "feature vector",
        "previous researches",
        "researc- hes",
        "exclusion criteria",
        "static text",
        "first step",
        "data- bases",
        "several papers",
        "quality criteria",
        "Basic information",
        "Best performance",
        "duplicate publications",
        "same study",
        "higher values",
        "algorithm parameters",
        "experimental tests",
        "design step",
        "analysis step",
        "conclusion step",
        "test dataset",
        "first publications",
        "conduction step",
        "quality score",
        "nine items",
        "several items",
        "performance tests",
        "criterion",
        "services",
        "accessible",
        "sum",
        "goals",
        "beginning",
        "advantages",
        "approaches",
        "limitations",
        "problems",
        "acm",
        "isiknowledge",
        "scopus",
        "Part",
        "title",
        "year",
        "Type",
        "device",
        "PC",
        "measure",
        "Number",
        "Algorithms",
        "verification",
        "Observations",
        "line",
        "Table",
        "18/February",
        "tool",
        "mendeley",
        "series",
        "duplicates",
        "organization",
        "category",
        "4",
        "2.1",
        "references Database Number",
        "step Step Number",
        "PDF files",
        "fast reading",
        "secondary studies",
        "200 refer- ences",
        "quality assessment",
        "several texts",
        "direct way",
        "pub- lications",
        "descending order",
        "highest scores",
        "higher scores",
        "greater efforts",
        "average score",
        "high number",
        "exclusion criterion",
        "search databases",
        "plete text",
        "full text",
        "duplicate references",
        "remaining references",
        "eligible references",
        "information extraction",
        "Table 2 Number",
        "Table 1",
        "associations",
        "entries",
        "Web",
        "Gaines",
        "centralization",
        "intersection",
        "removal",
        "university",
        "Conse",
        "reviews",
        "surveys",
        "tasks",
        "spreadsheet",
        "Total",
        "elimination",
        "detail",
        "tables",
        "algorithms",
        "performance",
        "abstract",
        "observation",
        "one",
        "ally",
        "purpose",
        "equals",
        "focus",
        "rel",
        "graph",
        "different",
        "4.2.2",
        "2.3",
        "580 J Braz Comput Soc",
        "approximate normal distribution",
        "tional research efforts",
        "highest quality score",
        "publi- cations",
        "scientific community",
        "research sub-questions",
        "performance evaluation",
        "benchmarking datasets",
        "access card",
        "military purposes",
        "simple login",
        "unauthorized access",
        "card ownership",
        "recent years",
        "greatest growth",
        "same institution",
        "biometric technologies",
        "behavioral features",
        "biometric technology",
        "specific devices",
        "biometric data",
        "growth trend",
        "voice recognition",
        "primary method",
        "One way",
        "additional cost",
        "authentication factors",
        "several users",
        "unauthorized user",
        "security context",
        "Publication Year",
        "widespread use",
        "user identity",
        "zero",
        "scores",
        "maximum",
        "aspect",
        "field",
        "atten",
        "Both",
        "5 Results",
        "section",
        "disadvan",
        "credentials",
        "token",
        "biometrics",
        "gerprint",
        "iris",
        "combination",
        "secrecy",
        "weaknesses",
        "authorization",
        "words",
        "birthday",
        "scenarios",
        "expansion",
        "gally",
        "rate",
        "USA",
        "losses",
        "world",
        "insiders",
        "victim",
        "attack",
        "science",
        "methods",
        "determination",
        "iological",
        "hardware",
        "contrast",
        "other",
        "capture",
        "level",
        "transparency",
        "false alarm rates",
        "nature recognition biometrics",
        "specific operations",
        "stroke dynamics",
        "Recognition precision",
        "same environment",
        "nition performance",
        "user identification",
        "legiti- mate",
        "legitimate user",
        "trol systems",
        "authentication",
        "factor",
        "acceptance",
        "presence",
        "keyboards",
        "teristics",
        "pen",
        "intruder",
        "standards",
        "access",
        "European"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.023614,
      "content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n� The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning � Covariate shift �\nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg � X � Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg � X � Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ�Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ�Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ�Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n� min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp �cjjxtr � x\nðiÞ\nte jj22\n\n� �\n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð�cjjxtr � x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð�cjjxtr � x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k � n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð�cjjxtar � xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr � xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n� �� �\n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� y\n\nðiÞ\ntr � f x\n\nðiÞ\ntr\n\n� �� �2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n� �\n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x� CÞ or maxð0;C � xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods sho",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 456223,
      "metadata_storage_name": "s40708-015-0017-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDcwOC0wMTUtMDAxNy16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Zengda Guan",
      "metadata_title": "Local regression transfer learning with applications to users’ psychological characteristics prediction",
      "metadata_creation_date": "2015-08-12T10:56:23Z",
      "keyphrases": [
        "China A. Li Black Dog Institute",
        "weighted risk regression model",
        "Z. Guan Business School",
        "cn T. Zhu Institute",
        "Local regression transfer learning",
        "cn A. Li",
        "Adaptive parameter-setting method",
        "New South Wales",
        "Local transfer learning",
        "clustering reweighting methods",
        "Shandong Jianzhu University",
        "individual psychological characteristics",
        "online user experience",
        "transfer learning methods",
        "personal psychological characteristics",
        "independent test dataset",
        "labelled training dataset",
        "psychological characteristics prediction",
        "Beijing Forestry University",
        "Zengda Guan",
        "Ang Li",
        "Tingshao Zhu",
        "personal preference",
        "online services",
        "web user",
        "The Author",
        "open access",
        "Recent studies",
        "training instance",
        "different genders",
        "ferent districts",
        "� Covariate shift",
        "recent decades",
        "important role",
        "human lives",
        "consumer preference",
        "consistent traits",
        "changeable status",
        "key factors",
        "one type",
        "human behaviour",
        "web behaviour",
        "Many studies",
        "Most studies",
        "many cases",
        "Chinese Academy",
        "Computing Technology",
        "Brain Informatics",
        "Previous studies",
        "little attention",
        "innovative approach",
        "learning models",
        "tional models",
        "generalization capability",
        "demographic variation",
        "identical distribution",
        "users’ personality",
        "behaviours",
        "applications",
        "article",
        "Springerlink",
        "Abstract",
        "differences",
        "problem",
        "bour",
        "importance",
        "situation",
        "labels",
        "experiments",
        "depression",
        "results",
        "Keywords",
        "1 Introduction",
        "people",
        "time",
        "Internet",
        "help",
        "assumption",
        "Jinan",
        "mail",
        "guanzengda",
        "sdjzu",
        "Department",
        "Psychology",
        "Sydney",
        "Australia",
        "blackdog",
        "Sciences",
        "tszhu",
        "DOI",
        "crossmark",
        "org",
        "low",
        "performance",
        "paper",
        "3",
        "14",
        "new local regression transfer learning methods",
        "Most existing covariate shift methods",
        "local transfer learning methods",
        "bility distribution PteðX",
        "local learning views",
        "adaptive k-NN methods",
        "ytrÞ�Pte lðxtr",
        "weighted risk model",
        "PteðxtrÞ PtrðxtrÞ",
        "general solution framework",
        "local neighbour region",
        "fðxð1Þte",
        "training-test k-NN method",
        "pre- diction functions",
        "covariate shift problems",
        "empirical risk form",
        "training data instances",
        "test dataset label",
        "test dataset model",
        "Þg � X � Y",
        "¼ fðxð1Þtr",
        "ytrÞ lðxtr",
        "test dataset population",
        "psychological characteristics labels",
        "learning model",
        "regression form",
        "local region",
        "¼ PteðXÞ",
        "prediction functions",
        "risk function",
        "expected risk",
        "PtrðXÞ",
        "simple covariate",
        "ðxðntrÞ",
        "data points",
        "parameter-setting method",
        "¼ PteðyjxÞ",
        "prediction accuracy",
        "training dataset",
        "entire dataset",
        "input dataset",
        "PtrðyjxÞ",
        "ðxðnteÞ",
        "resampling weight",
        "computation cost",
        "continual values",
        "unknown parameter",
        "exper- iments",
        "experiment results",
        "last section",
        "different distributions",
        "key point",
        "loss function",
        "ntr Xntr",
        "yÞ�Ptr",
        "probability distribution",
        "probability density",
        "previous work",
        "Y.",
        "purpose",
        "researches",
        "procedure",
        "addition",
        "parameters",
        "domains",
        "dictions",
        "rest",
        "Sect.",
        "background",
        "Ztr",
        "Zte",
        "inputs",
        "process",
        "hÞ",
        "datasets",
        "ratio",
        "2",
        "Gaussian kernel basis functions",
        "kernel mean matching method",
        "many machine learning tasks",
        "adaptive K-NN reweighting method",
        "training-test K-NN reweighting method",
        "weighted machine learning model",
        "complete covariate shift process",
        "many density estimation methods",
        "Gaussian kernel functions",
        "2.3.1 K-NN reweighting method",
        "Kullback–Leibler divergence",
        "reproducing-kernel Hilbert space",
        "comparative advan- tage",
        "density estima- tion",
        "nearest-neighbour approximation method",
        "Local machine learning",
        "local classification method",
        "k-nearest test set",
        "many neighbour points",
        "k-nearest-neighbour reweighting methods",
        "less neighbour points",
        "squares importance biases",
        "ratio PteðxtrÞ",
        "kernel methods",
        "learning process",
        "k-NN methods",
        "many kinds",
        "nearest-neighbour methods",
        "local learning",
        "density similarity",
        "transfer learning",
        "neighbour region",
        "histogram methods",
        "clustering methods",
        "PtrðxtrÞ",
        "prediction model",
        "146 Z. Guan",
        "significant effect",
        "one hand",
        "little relationship",
        "target point",
        "unnecessary interferences",
        "similar property",
        "small region",
        "big region",
        "other hand",
        "strong noise",
        "small proportion",
        "given point",
        "PðxÞ",
        "region volume",
        "one volume",
        "two stages",
        "test dataset",
        "first stage",
        "impor- tance",
        "origin form",
        "test points",
        "long-distance points",
        "target data",
        "data point",
        "training distributions",
        "nearest neighbours",
        "training instances",
        "training point",
        "training data",
        "input densities",
        "common situations",
        "total number",
        "errors",
        "researchers",
        "forms",
        "formula",
        "Sugiyama",
        "series",
        "Kanamori",
        "Huang",
        "KMM",
        "Dai",
        "Pardoe",
        "list",
        "boosting",
        "rithms",
        "size",
        "effects",
        "way",
        "nV",
        "ktr",
        "kte",
        "Loog",
        "work",
        "formance",
        "neighbourhood",
        "fact",
        "knowledge",
        "extension",
        "adaptation",
        "Clustering-based",
        "view",
        "ð3Þ",
        "clustering-based reweighting meth- ods",
        "3.2 Training-test K-NN reweighting method",
        "Cross validation tech- nique",
        "Gaussian kernel function ratio",
        "empirical parameter estimation way",
        "Clustering-based reweighting method",
        "nearest-neighbour reweighting method",
        "users’ psychological characteristics",
        "other big values",
        "nearest test set",
        "other test points",
        "covariate shift methods",
        "labelled test data",
        "training data points",
        "training data xtr",
        "k-NN method",
        "training data part",
        "togram method",
        "empirical way",
        "reweighting formula",
        "Gaussian kernels",
        "validation technique",
        "big bias",
        "nearest-neighbour point",
        "WeigðxtrÞ",
        "exponential term",
        "exponential law",
        "appropriate neighbour",
        "computation time",
        "comparable probability",
        "ðjÞ tr",
        "two parts",
        "important issue",
        "smaller value",
        "similar probability",
        "expð�cjjxtar",
        "small value",
        "2nneig Pnneig",
        "clustering algorithm",
        "test neighbours",
        "validation dataset",
        "k value",
        "mine k",
        "k � n",
        "gauðnneig",
        "density distance",
        "population size",
        "ktr Pktr",
        "k.",
        "¼ ktr",
        "Xk",
        "�cjjxtr",
        "bandwidth",
        "effect",
        "denominator",
        "c.",
        "Enas",
        "Choi",
        "threshold",
        "reason",
        "1 jjxtr",
        "kind",
        "histograms",
        "ð4Þ",
        "ð5Þ",
        "147",
        "jClustrðxðiÞtr Þj",
        "The Big Five personality model",
        "accurate density-region division methods",
        "lðyðiÞtr",
        "Big Five personality framework",
        "jClusteðxðiÞtr",
        "adaptive stepwise regression method",
        "WeigðxðiÞtr",
        "ðxðiÞte",
        "local transfer methods",
        "small enough region",
        "approximate computation way",
        "local reweighting methods",
        "2.4 Weighted regression model",
        "weighted learning model",
        "web users’ depression",
        "big influence",
        "five dimensions",
        "hjðxÞ",
        "weighted model",
        "C � xÞ",
        "regression function",
        "human personality",
        "histogram method",
        "PteðxtrÞ",
        "high-dimensional difficulty",
        "high-dimensional situation",
        "previous stage",
        "real value",
        "prediction value",
        "sion splines",
        "constant denoted",
        "hinge function",
        "total steps",
        "optimal performance",
        "cients bj.",
        "logical characteristics",
        "three parts",
        "148 Z. Guan",
        "different districts",
        "unpleasant emotions",
        "intellectual curiosity",
        "CES-T scale",
        "many data",
        "one histogram",
        "same cluster",
        "Clustering method",
        "probability estimation",
        "lowing formula",
        "one cluster",
        "different weight",
        "importance weighting",
        "tr ÞÞ",
        "� CÞ",
        "instances",
        "detail",
        "number",
        "idea",
        "equal",
        "risk",
        "Xntr",
        "bias",
        "MARS",
        "Xm",
        "bjhj",
        "product",
        "two",
        "respec",
        "3 Experiments",
        "psychology",
        "agreeableness",
        "conscientiousness",
        "extraversion",
        "neuroticism",
        "openness",
        "tendency",
        "degree",
        "creativity",
        "preference",
        "novelty",
        "ð6Þ",
        "ð7Þ",
        "famous microblog service providers",
        "male users’ depression degree",
        "many feature dimensions",
        "mogorov–Smirnov test",
        "503 million registered users",
        "high dimension curse",
        "Big Five dimensions",
        "online self-report questionnaire",
        "users’ web behaviours",
        "time series data",
        "additional linguistic features",
        "Guangdong users’ personality",
        "istration time",
        "test set",
        "T test",
        "two-sample test",
        "different users",
        "female users",
        "important factor",
        "training set",
        "3.1 Experiment setup",
        "ing personality",
        "depression scales",
        "digital records",
        "pression behaviours",
        "screen name",
        "facial picture",
        "privacy settings",
        "individual privacy",
        "vate messages",
        "interpersonal behaviours",
        "social interaction",
        "punctuation marks",
        "perceptual processing",
        "stepwisefit method",
        "Matlab toolbox",
        "gender-personality experiment",
        "district-personality experiment",
        "depression experiment",
        "same distribution",
        "Gaussian distribution",
        "unknown distribution",
        "Weibo users",
        "five categories",
        "online behaviours",
        "behavioural data",
        "Sina Weibo",
        "personal image",
        "personal page",
        "sentiment words",
        "cognitive words",
        "Guangdong dataset",
        "female dataset",
        "personality experiments",
        "sion experiments",
        "dynamic features",
        "845 features",
        "20 features",
        "relationship",
        "personality/depression",
        "example",
        "suitability",
        "difference",
        "China",
        "research",
        "consent",
        "prediction",
        "May",
        "August",
        "562 participants",
        "profiles",
        "demographics",
        "expression",
        "self-statement",
        "concern",
        "comments",
        "strangers",
        "outcomes",
        "friends",
        "microblogs",
        "period",
        "apps",
        "June",
        "1000 participants",
        "characters",
        "numerals",
        "pronouns",
        "relevant",
        "Both",
        "22",
        "other local trans- fer learning methods",
        "adaptive k-NN transfer learning methods",
        "test k-NN transfer learning method",
        "other transfer learning methods",
        "famous transfer learning method",
        "regression transfer learning methods",
        "adaptive k-NN reweighting method",
        "Local kNN reweighting methods",
        "transfer learning way",
        "mean square error",
        "Kol- mogorov–Smirnov",
        "weighted risk form",
        "global transfer method",
        "Kolmogorov–Smirnov test",
        "clustering-based reweighting method",
        "single-dimension T test",
        "fixed k value",
        "other k-NN",
        "KMM reweighting method",
        "male users’ personality",
        "global k-NN",
        "regression software",
        "risk models",
        "traditional method",
        "baseline method",
        "training- test",
        "test results",
        "open source",
        "female datasets",
        "situa- tions",
        "trait A",
        "total size",
        "perfor- mances",
        "TTkNN method",
        "5 personality dimensions",
        "MARS method",
        "GkNN method",
        "best results",
        "c value",
        "result comparisons",
        "distribution divergence",
        "comparable performance",
        "Clust denotes",
        "parameter values",
        "C.",
        "4 dimensions",
        "MARS.",
        "neighbours",
        "Matlab/Octave",
        "rtu",
        "lv",
        "jekabsons",
        "tables",
        "figures",
        "nation",
        "AkNN1",
        "AkNN2",
        "MSE",
        "genders",
        "task",
        "2 features",
        "1 features",
        "probability",
        "confidence",
        "situations",
        "others",
        "advantage",
        "impact",
        "Fig.",
        "noise",
        "149",
        "25",
        "test results Condition A C E N O",
        "Local regression transfer learning results",
        "other local transfer learning methods",
        "stable density estimation method",
        "male users’ depression level",
        "k-NN transfer methods",
        "regression transfer methods",
        "clustering transfer methods",
        "other two traits",
        "least k value",
        "five personality traits",
        "other districts",
        "clustering method",
        "five traits",
        "The method",
        "best method",
        "0, outlier point",
        "strong influence",
        "optimization result",
        "three traits",
        "M S",
        "150 Z. Guan",
        "cs.rtu",
        "regular rule",
        "person- ality",
        "mogorov–Smirnov",
        "one trait",
        "3 feature dimensions",
        "5 feature dimensions",
        "different-gender datasets",
        "different-district datasets",
        "best performance",
        "optimal value",
        "Weibo data",
        "gender experiment",
        "most situations",
        "cluster number",
        "clusters increases",
        "prediction performance",
        "5 traits",
        "exception",
        "shake-up",
        "factor",
        "GkNN",
        "constant",
        "diversity",
        "Guangdong",
        "province",
        "model",
        "total",
        "95% confidence",
        "performances",
        "Table 2",
        "local k-NN family transfer learning methods",
        "global k-NN transfer learning method",
        "predic- tion bias",
        "different- distribution datasets",
        "appropriate k value",
        "transfer method",
        "test data",
        "different-distribution feature",
        "subtle nature",
        "density estimation",
        "training",
        "experiment",
        "Table",
        "result",
        "non",
        "Clust",
        "other",
        "3.5 Discussion",
        "conclusion"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.5911174,
      "content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker  J Big Data            (2019) 6:95  \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence:   \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker  J Big Data            (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n\n\nPage 3 of 25Sarker  J Big Data            (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker  J Big Data            (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker  J Big Data            (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker  J Big Data            (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker  J Big Data            (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker  J Big Data            (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1  = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker  J Big Data            (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker  J Big Data            (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsivel",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1660534,
      "metadata_storage_name": "s40537-019-0258-4.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDI1OC00LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Iqbal H. Sarker ",
      "metadata_title": "Context-aware rule learning from smartphone data: survey, challenges and future directions",
      "metadata_creation_date": "2019-10-30T14:24:16Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "intelligent decision making strategies",
        "advanced data analytical techniques",
        "25Sarker  J Big Data",
        "Context‑aware rule learning",
        "Intelligent systems Open Access",
        "machine learning based techniques",
        "Creative Commons license",
        "corresponding context-aware rule learning",
        "Iqbal H. Sarker",
        "User behavior modeling",
        "creat iveco mmons",
        "intelligent context-aware applications",
        "raw contextual data",
        "corresponding search interests",
        "data-driven context-aware systems",
        "phone data analytics",
        "original author(s",
        "corresponding data processing",
        "multifunctional cell phones",
        "users’ surrounding environment",
        "smartphone data",
        "Data science",
        "Predictive analytics",
        "context-aware rules",
        "Context-aware computing",
        "author information",
        "future directions",
        "recent days",
        "daily life",
        "important IoT",
        "next generation",
        "wireless connectivity",
        "work coverage",
        "developed countries",
        "recent statistics",
        "Google Trends",
        "users’ interest",
        "other platforms",
        "Desktop Computer",
        "Laptop Computer",
        "current world",
        "recent developments",
        "behavioral activities",
        "wide attention",
        "high precision",
        "traditional approaches",
        "efficient results",
        "previous work",
        "rule-based automated",
        "IoT services",
        "unrestricted use",
        "appropriate credit",
        "doi.org",
        "1 Swinburne University",
        "Full list",
        "Tablet Computer",
        "last 5 years",
        "timestamp information",
        "particular date",
        "highest point",
        "personal devices",
        "Things) devices",
        "essential part",
        "world population",
        "Mobile Phones",
        "particular term",
        "SURVEY PAPER",
        "peak popularity",
        "challenges",
        "Introduction",
        "smartphones",
        "individuals",
        "Internet",
        "capabilities",
        "number",
        "Fig.",
        "Abstract",
        "ogy",
        "academia",
        "industry",
        "order",
        "set",
        "key",
        "contexts",
        "comparison",
        "effective",
        "article",
        "area",
        "discussion",
        "Clustering",
        "Classification",
        "Association",
        "Personalization",
        "Time-series",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "msarker",
        "Melbourne",
        "Australia",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "Figure",
        "x-axis",
        "range",
        "chart",
        "instance",
        "value",
        "maximum",
        "y-axis",
        "minimum",
        "context-aware mobile notification management system",
        "other broad application areas",
        "corresponding rule-based prediction model",
        "different context-aware test cases",
        "data-driven intelligent context-aware systems",
        "optimal time-based context-aware rules",
        "rule-based intelligent context-aware systems",
        "smartphone data analytic solutions",
        "context-aware smart city services",
        "users’ behavioral activity patterns",
        "intelligent data-driven decisions",
        "THEN” logical structure",
        "corresponding data clustering",
        "corresponding behavioral activities",
        "Users’ interest trends",
        "intelligent eHealth services",
        "aware behavioral rules",
        "relevant context-aware rules",
        "relevant contextual data",
        "contextual data patterns",
        "contextual data clustering",
        "machine learning researchers",
        "machine learning techniques",
        "different surrounding contexts",
        "machine learning rules",
        "relevant contextual information",
        "data mining techniques",
        "advanced data analysis",
        "rule-based classification",
        "raw data",
        "different contexts",
        "mobile notifications",
        "context-aware  software",
        "hidden patterns",
        "data science",
        "time-series data",
        "advanced features",
        "mental learning",
        "personalized services",
        "rity services",
        "deeper analysis",
        "association analysis",
        "detailed analysis",
        "daily basis",
        "great source",
        "useful information",
        "two parts",
        "temporal context",
        "tial context",
        "social contexts",
        "family members",
        "specific exceptions",
        "particular domain",
        "prac- titioners",
        "many reasons",
        "various purposes",
        "particular timestamp",
        "numeric values",
        "world-wide popularity",
        "ture optimization",
        "dynamic updating",
        "The reason",
        "huge amount",
        "unknown correlations",
        "diverse behaviors",
        "quent part",
        "individuals’ usage",
        "phone user",
        "devices",
        "result",
        "antecedent",
        "others",
        "example",
        "Alice",
        "work",
        "evening",
        "general",
        "preferences",
        "addition",
        "cable",
        "study",
        "applications",
        "building",
        "Several",
        "selection",
        "aim",
        "machine learning based context-aware rule learning framework",
        "Suggested machine learning based framework",
        "Context-aware rule based applications",
        "Context-aware rule learning strategies",
        "smartphone data analyt- ics",
        "real world applications",
        "rule discovery techniques",
        "decision making capability",
        "rule-based intelligent systems",
        "Human Computer Interaction",
        "smartphone data analytics",
        "corresponding context-aware rules",
        "contextual smartphone data",
        "Pervasive Computing area",
        "incremental learning",
        "context-aware computing",
        "updation techniques",
        "smrtphone data",
        "uitous Computing",
        "several perspectives",
        "time-series modeling",
        "various layers",
        "various perspectives",
        "different meanings",
        "different purposes",
        "numerous areas",
        "Computer-Supported Collaborative",
        "Ambient Intelligence",
        "recent works",
        "broader collection",
        "social aspects",
        "little work",
        "earlier work",
        "brief discussion",
        "background information",
        "main characteristics",
        "existing research",
        "brief survey",
        "first article",
        "multi-dimensional contexts",
        "sion” section",
        "profit",
        "analysis",
        "discretization",
        "issues",
        "contributions",
        "paper",
        "shortcomings",
        "role",
        "knowledge",
        "remainder",
        "variety",
        "notion",
        "mobile",
        "Ubiquitous",
        "context-awareness",
        "location",
        "people",
        "objects",
        "factors",
        "physical",
        "entity",
        "activities",
        "users",
        "definitions",
        "categories",
        "daily life activities",
        "temporal informa- tion",
        "ubiquitous computing community",
        "ubiquitous computing area",
        "Contextual smartphone data",
        "user diverse activities",
        "rent social situation",
        "data source",
        "smartphone usage",
        "temporal information",
        "defi- nitions",
        "dis- cussion",
        "Several studies",
        "surrounding people",
        "Other definitions",
        "other hand",
        "current situation",
        "nearby people",
        "available processors",
        "network capacity",
        "noise level",
        "lighting level",
        "day situations",
        "cellular phones",
        "computing environment",
        "location information",
        "locational information",
        "environmental information",
        "surrounding information",
        "changing environment",
        "physical environment",
        "other researchers",
        "important aspects",
        "alternative view",
        "con- text",
        "social context",
        "following definition",
        "user location",
        "user input",
        "relevant contexts",
        "spatial context",
        "user environment",
        "pervasive",
        "section",
        "scope",
        "different",
        "perspectives",
        "Schilit",
        "Brown",
        "temperature",
        "Simi",
        "identity",
        "account",
        "Ryan",
        "synonyms",
        "computer",
        "Franklin",
        "Ward",
        "state",
        "Hull",
        "entire",
        "settings",
        "Rodden",
        "resources",
        "connectivity",
        "display",
        "costs",
        "examples",
        "Dey",
        "survey",
        "person",
        "place",
        "interaction",
        "influence",
        "decisions",
        "everything",
        "lives",
        "Mobile",
        "personalized mobile game recommendation system",
        "vidual mobile phone user behavior",
        "mobile notification management systems",
        "relevant contextual informa- tion",
        "related contextual infor- mation",
        "various smart phone applications",
        "individual mobile phone users",
        "personalized context-aware recommendation",
        "user mobile web navigation",
        "smart mobile applications",
        "smart mobile phone",
        "phone usage behavior",
        "standardized communications rules",
        "International Telecommunication Union",
        "sive commercial industry",
        "accurate context-aware predictions",
        "text communication service",
        "contextual behavioral patterns",
        "automatic spam filtering",
        "smartphone notification logs",
        "phone call activities",
        "short text messages",
        "contextual usage patterns",
        "short message service",
        "spam text messages",
        "contextual patterns",
        "unique phone",
        "individual users",
        "User navigation",
        "short messages",
        "game log",
        "voice communication",
        "major activities",
        "spam messages",
        "various types",
        "various kinds",
        "various real",
        "web log",
        "web searching",
        "particular user",
        "meta data",
        "log data",
        "outgoing calls",
        "incoming calls",
        "other dimensions",
        "day situation",
        "social relationship",
        "numerous growth",
        "dramatic increasing",
        "good time",
        "bad time",
        "rapid development",
        "smartphone apps",
        "Such logs",
        "social networks",
        "predictive suggestions",
        "portal structure",
        "computing capabilities",
        "life-logging device",
        "technical sense",
        "context data",
        "tual information",
        "context source",
        "SMS log",
        "real world",
        "life purposes",
        "contact number",
        "ability",
        "caller",
        "callee",
        "exchange",
        "protocols",
        "81 billion",
        "task",
        "Multimedia",
        "Facebook",
        "Gmail",
        "Youtube",
        "Skype",
        "notifications",
        "events",
        "news",
        "reminders",
        "alerts",
        "games",
        "promotional",
        "emails",
        "Twitter",
        "LinkedIN",
        "WhatsApp",
        "Viver",
        "intelligent",
        "entertainment",
        "chat",
        "misc",
        "TV",
        "netting",
        "travel",
        "sport",
        "banking",
        "needs",
        "action",
        "adventure",
        "puzzle",
        "RPG",
        "strategy",
        "ubiquity",
        "opportunity",
        "memories",
        "Context‑aware rule learning strategies",
        "Time interval type Number",
        "temporal context based rules",
        "existing time-series segmentation approaches",
        "time‑series smartphone data",
        "mobile phone data",
        "Google play API",
        "geographical loca- tion",
        "context-aware rule learning",
        "segment details References",
        "personalized log data",
        "user phone calls",
        "smart mobile phones",
        "life log data",
        "mobile user behavior",
        "behavioral data clusters",
        "time-based behavior modeling",
        "A time series",
        "related contextual information",
        "static time segments",
        "existing rule",
        "existing strategies",
        "important context",
        "contextual rule",
        "learning rules",
        "IoT data",
        "data points",
        "temporal, spatial",
        "behavioral rules",
        "exact time",
        "user experience",
        "surrounding environment",
        "core components",
        "life- logs",
        "SMS headers",
        "App use",
        "physical activities",
        "Bluetooth devices",
        "open challenges",
        "diverse activities",
        "effective modeling",
        "open problem",
        "Various types",
        "different applications",
        "remaining hours",
        "late morning",
        "Equal 3 Morning",
        "Equal 4 Morning",
        "Unequal 5 Morning",
        "Equal 4 Night",
        "6:00 a",
        "sensors",
        "content",
        "sapp",
        "WiFi",
        "proximity",
        "opportunities",
        "future",
        "directions",
        "discovery",
        "behaviors",
        "sequence",
        "research",
        "Table",
        "afternoon",
        "Song",
        "Rawassizadeh",
        "Mukherji",
        "Bayir",
        "Paireekreng",
        "day",
        "Jayarajah",
        "Do",
        "Xu",
        "Mehrotra",
        "Zhu",
        "forenoon",
        "Oulasvirta",
        "Next",
        "Yu",
        "midnight",
        "0:00",
        "dynamic segmenta- tion technique",
        "high confidence temporal rules",
        "unequal interval based segmentation",
        "high confidence rules",
        "four different time segments",
        "various mobile applications",
        "dynamic segmen- tation",
        "four time slots",
        "mobile phone users",
        "individual smartphone users",
        "same interval length",
        "two broad categories",
        "users’ behavioral activities",
        "dynamic segmentation techniques",
        "Such static segmentation",
        "various time intervals",
        "unequal time interval",
        "time segments decreases",
        "temporal coverage",
        "different lengths",
        "temporal contexts",
        "arbitrary categories",
        "two types",
        "behavioral characteristics",
        "corresponding segmentation",
        "population behavior",
        "ioral evidence",
        "usage records",
        "seg- ments",
        "single parameter",
        "division result",
        "base period",
        "time period",
        "static generation",
        "meaningful results",
        "variable number",
        "one case",
        "N1 number",
        "N2 number",
        "Naboulsi",
        "Dashdorj",
        "Shin",
        "11:00 a",
        "S5",
        "S8",
        "Farrahi",
        "ii",
        "researchers",
        "periods",
        "1 h",
        "works",
        "approaches",
        "goal",
        "change",
        "patterns",
        "authors",
        "Tmax",
        "24-h",
        "BP",
        "5 min",
        "2:00",
        "5:00",
        "7:00",
        "9:00",
        "important machine learning tech- niques",
        "time- series data Lu",
        "Base technique Description References",
        "time-series log data Halvey",
        "data shape based approaches",
        "squared error-based clustering algorithm",
        "time-series data analysis",
        "time-series data Hartono",
        "time-series data Das",
        "similar K-medoids method",
        "cluster- ing algorithms",
        "mean value calculation",
        "temporal context Khail",
        "different initial points",
        "base time period",
        "dynamic time segments",
        "Various calendar schedules",
        "corresponding time boundaries",
        "user behavior patterns",
        "large time segments",
        "large value",
        "initial partitions",
        "different value",
        "time segmentation",
        "time interval",
        "different contexts",
        "predefined value",
        "clustering algorithms",
        "finest granularity",
        "segments decreases",
        "iable length",
        "particular event",
        "personal calendars",
        "multiple thresholds",
        "sliding window",
        "several applications",
        "specific application",
        "convergence centroids",
        "segments Ozer",
        "segmentation Do",
        "upper boundary",
        "particular segment",
        "genetic algorithm",
        "users’ behavior",
        "users’ preferences",
        "one user",
        "users’ behave",
        "Single parameter",
        "K-means algorithm",
        "predefined number",
        "individual",
        "assumptions",
        "problems",
        "sense",
        "clusters",
        "K.",
        "outliers",
        "30 min",
        "Karatzoglou",
        "small",
        "Phithakkitnukoon",
        "Dekel",
        "Zulkernain",
        "Seo",
        "Multi-thresholds",
        "lower",
        "Zhang",
        "Shokoohi",
        "Keogh",
        "Kandasamy",
        "characteristic",
        "reason",
        "1",
        "2:30",
        "single linkage agglomerative clustering algorithm",
        "bottom-up hierarchical data processing",
        "popular agglomerative clustering",
        "other characteristic mismatches",
        "device clustering method",
        "Existing hierarchical algorithms",
        "agglomerative methods",
        "complete linkage",
        "device methods",
        "other clusters",
        "time segments",
        "behavioral clusters",
        "nearest neighbor",
        "proximity matrix",
        "new cluster",
        "matrix value",
        "outlier problem",
        "presence",
        "medoid",
        "K-means",
        "size",
        "user",
        "practice",
        "simplest",
        "distance",
        "sivel"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.4478297,
      "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 2 of 19\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n�\ni K xi; x\n\n� �þ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2;…; ∂�N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n� �\n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK xi; x j\n\n� �� �\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 3 of 19\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 4 of 19\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 5 of 19\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 6 of 19\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 7 of 19\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 8 of 19\n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi�−R0∅i\n�−b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 9 of 19\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 10 of 19\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2713106,
      "metadata_storage_name": "s13640-018-0373-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMTgtMDM3My04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Boping Zhang",
      "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform",
      "metadata_creation_date": "2018-11-23T11:51:48Z",
      "keyphrases": [
        "accurate feature point alignment model",
        "new online shopping mode",
        "Augmented reality virtual glasses",
        "external propaganda channels",
        "actual wearing effects",
        "Creative Commons Attribution",
        "lates physical information",
        "phys- ical information",
        "head pose estimation",
        "traditional online shopping",
        "virtual glasses model",
        "input device-monocular camera",
        "key research issue",
        "virtual world scene",
        "user online glasses",
        "RESEARCH Open Access",
        "1 Introduction Network virtual",
        "iOS mobile platform",
        "local face features",
        "virtual model",
        "feature points",
        "scale-invariant feature",
        "new way",
        "2018 Open Access",
        "research key",
        "local features",
        "virtual information",
        "Face information",
        "Information Engineer",
        "real world",
        "Virtual try",
        "favorite glasses",
        "different glasses",
        "user senses",
        "face detection",
        "human face",
        "iOS glasses",
        "Boping Zhang",
        "optimal purchase",
        "robust SIFT",
        "rapid achievement",
        "sponding images",
        "vector machine",
        "EURASIP Journal",
        "Video Processing",
        "The Author",
        "iOS platform",
        "AR glasses",
        "camera image",
        "quick try",
        "human senses",
        "AR principle",
        "Computer vision",
        "real time",
        "real-time interaction",
        "Xuchang University",
        "APP system",
        "descent method",
        "computer technology",
        "SVM classifier",
        "Abstract",
        "development",
        "commerce",
        "gap",
        "goods",
        "speed",
        "immersion",
        "SDM",
        "identification",
        "Keywords",
        "merchants",
        "consumers",
        "important",
        "part",
        "field",
        "experiential",
        "position",
        "angle",
        "corre",
        "screen",
        "sound",
        "taste",
        "touch",
        "space",
        "sensory",
        "perience",
        "faces",
        "bopingzhang",
        "yeah",
        "School",
        "Henan",
        "China",
        "article",
        "terms",
        "real-time head mo- tion tracking",
        "Two corresponding isosceles triangles",
        "General face recognition process",
        "original author(s",
        "charge-coupled device) camera",
        "Android mobile platform",
        "Creative Commons license",
        "real envir- onment",
        "foreign glasses sellers",
        "3D modeling approach",
        "2D image overlay",
        "2D image superposition",
        "AR technology design",
        "2D face images",
        "virtual optician system",
        "image processing technology",
        "real-time requirements",
        "2D images",
        "accurate tracking",
        "International License",
        "The superposition",
        "3D glasses",
        "glasses images",
        "network virtual",
        "virtual try",
        "virtual objects",
        "Virtual glasses",
        "unrestricted use",
        "appropriate credit",
        "Research status",
        "Smart Look",
        "wearing effect",
        "research hotspots",
        "research results",
        "four categories",
        "Three points",
        "fine transformation",
        "unrealistic deformation",
        "inter- action",
        "good results",
        "different defects",
        "depth research",
        "large extent",
        "new ideas",
        "false ratios",
        "important indicator",
        "feature extraction",
        "dimension reduc",
        "first step",
        "cific location",
        "Glasses companies",
        "glasses model",
        "AR-based glasses",
        "face features",
        "face area",
        "face needs",
        "matching recognition",
        "doi.org",
        "orcid.org",
        "shopping experience",
        "input sensor",
        "experience senses",
        "input image",
        "computer vision",
        "augmented reality",
        "video stream",
        "system performance",
        "online shopping",
        "based detection",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Zhang",
        "crossmark",
        "crossref",
        "dialog",
        "USA",
        "Camirror",
        "Ipoint",
        "Kisok",
        "Xview",
        "function",
        "Users",
        "domestic",
        "Meijing",
        "Kede",
        "Biyao",
        "present",
        "Huang",
        "eyes",
        "pose",
        "scale",
        "method",
        "realism",
        "Cheng",
        "monocular",
        "CCD",
        "marker",
        "kinds",
        "techniques",
        "advantages",
        "problems",
        "application",
        "precondition",
        "frame",
        "posture",
        "lumination",
        "occlusion",
        "omission",
        "people",
        "Fig.",
        "gion",
        "range",
        "The Gaussian kernel function Kðx",
        "three-layer feedforward neural network",
        "Face recognition process Zhang",
        "high-dimensional face feature vector",
        "Sigmoid kernel functions",
        "local binary patterns",
        "local structural content",
        "Common kernel func",
        "high-dimensional feature space",
        "spatial fre- quency",
        "machine learning method",
        "high-dimensional feature vector",
        "online matching models",
        "low-dimensional subspace information",
        "support vector machine",
        "optimal parameter vector",
        "Feature dimension reduction",
        "different scale spaces",
        "face feature identification",
        "sional face features",
        "face detection methods",
        "3.1 SVM-based face detection",
        "¼ sgn XN train",
        "low-dimensional input space",
        "robust SIFT algorithm",
        "classic SVM algorithm",
        "K xi",
        "Nsv X",
        "detection process",
        "vector dimension",
        "width function",
        "quadratic function",
        "discriminant function",
        "Common features",
        "space complexity",
        "3.2 Face recognition",
        "Matching recognition",
        "Feature extraction",
        "Gabo feature",
        "spatial position",
        "offline learning",
        "optimization parameter",
        "input vector",
        "face database",
        "face alignment",
        "matching strategy",
        "ginal features",
        "network output",
        "detection result",
        "∂�N train",
        "oriented gradient",
        "illumination changes",
        "small displacements",
        "ture changes",
        "brightness invariance",
        "direction selectivity",
        "previous links",
        "final decision",
        "virtual glasses",
        "Markov model",
        "statistical theory",
        "nonlinear mapping",
        "mal hyperplane",
        "largest interval",
        "inner product",
        "Mercer condition",
        "Optimization problem",
        "training sample",
        "face image",
        "video sequence",
        "description ability",
        "output layer",
        "Figure 3 shows",
        "tected image",
        "LBP",
        "HOG",
        "histogram",
        "Gabor",
        "insensitiveness",
        "overall",
        "Page",
        "time",
        "premise",
        "technology",
        "scholars",
        "HMM",
        "AdaBoost",
        "structure",
        "Equation",
        "polynomial",
        "zÞ",
        "Eq.",
        "ÞT",
        "jy",
        "XNtrain",
        "∂i",
        "j � �� � ð3Þ",
        "conditions",
        "rotation",
        "ð1Þ",
        "2σ",
        "∗",
        "Figure 4 shows flow block diagram",
        "stable image fea- tures",
        "other feature extraction functions",
        "higher positioning accuracy",
        "SVM network structure",
        "highest gradient value",
        "ian Laplace function",
        "potential fixed points",
        "variable scale parameter",
        "Establishing Gaussian pyramid",
        "SVM classifier Zhang",
        "Gaussian differential functions",
        "Spatial extreme detection",
        "gradient direction histogram",
        "stitute tower model",
        "two adjacent layers",
        "Key point matching",
        "image matching model",
        "scale space sequence",
        "scale space factor",
        "Gaussian difference pyramid",
        "DoG operator curve",
        "Gaussian difference function",
        "local extreme points",
        "② Position key points",
        "tower top image",
        "local feature",
        "Gaussian function",
        "feature construction",
        "candidate position",
        "feature vector",
        "Gauss-Laplacian curve",
        "local gradients",
        "Gaussian template",
        "Gaussian blur",
        "Laplacian operator",
        "ence space",
        "jacent points",
        "brightness changes",
        "sta- bility",
        "affine transformation",
        "angle change",
        "Basic principle",
        "multiple details",
        "image search",
        "stability degree",
        "main contour",
        "edge features",
        "larger scale",
        "blurred image",
        "human eye",
        "original image",
        "first layer",
        "new image",
        "second layer",
        "minimum dimension",
        "red line",
        "blue line",
        "same group",
        "SIFT algorithm",
        "generation process",
        "main direction",
        "formation process",
        "pixel point",
        "convolution operation",
        "different sizes",
        "scale normalization",
        "The relationship",
        "noise",
        "scales",
        "selection",
        "kind",
        "symbol",
        "target",
        "retina",
        "dimensions",
        "location",
        "down-sampling",
        "pyramids",
        "bottom",
        "number",
        "¼ log2",
        "qð",
        "logarithm",
        "maxima",
        "minima",
        "2G",
        "constant",
        "images",
        "3.2.1",
        "3.2.2",
        "σ",
        "ð6Þ",
        "∇",
        "Fig. 4 SIFT algorithm flow chart",
        "Terravic Re- search Corporation",
        "red intermediate detection point",
        "N+ 3-layer Gaussian pyramid",
        "N+ 2-layer DoG pyramid",
        "Fig. 5 Gaussian pyramid Zhang",
        "match- ing performance",
        "SIFT matching algorithm",
        "face infrared database",
        "feature point field",
        "20 infrared image sequences",
        "initial matching method",
        "N extreme points",
        "Face recognition experiment",
        "scale space value",
        "lower scale spaces",
        "Gaussian difference image",
        "Gradient histogram statistics",
        "7 Gaussian pyramid",
        "key point",
        "Ransac method",
        "group Zhang",
        "adjacent upper",
        "adjacent points",
        "correct matching",
        "gradient value",
        "successful matching",
        "matching process",
        "edge response",
        "independent characteristic",
        "direc- tion",
        "square column",
        "head rotation",
        "light-illuminated pictures",
        "mismatch rate",
        "correct matches",
        "mismatched points",
        "peak value",
        "matching accuracy",
        "gradient direction",
        "auxiliary direction",
        "contributive pixels",
        "Three pairs",
        "other words",
        "Error degree",
        "lower images",
        "total number",
        "26 points",
        "60 pairs",
        "120 images",
        "calculation",
        "surrounding",
        "case",
        "invariance",
        "perspective",
        "set",
        "tors",
        "descriptor",
        "probability",
        "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi",
        "neighborhood",
        "Comparison",
        "Gauss-Laplacian",
        "36 columns",
        "maximum",
        "robustness",
        "glasses",
        "hats",
        "Figure",
        "work",
        "classic",
        "grees",
        "2.5",
        "3.",
        "θ",
        "360°",
        "10°",
        "tracking registra- tion subsystem",
        "DoG space extreme point",
        "nonlinear feature extraction function",
        "many face alignment algorithms",
        "SDM-based face alignment process",
        "face feature point alignment",
        "ture point location",
        "feature vector descriptor",
        "main direction Zhang",
        "original data information",
        "initialization feature point",
        "head rotation angle",
        "n feature points",
        "① SIFT matching performance",
        "face feature points",
        "Harris matching method",
        "ginal face image",
        "face image detection",
        "3.3 Face alignment",
        "function approximation",
        "SIFT feature",
        "head posture",
        "real face",
        "face images",
        "matching re",
        "experimental data",
        "motion direction",
        "key points",
        "contour points",
        "estimation error",
        "average face",
        "number difference",
        "same number",
        "success rate",
        "overall trend",
        "inevitable errors",
        "good effect",
        "step size",
        "optimal position",
        "sonable translation",
        "shape factors",
        "optimal solution",
        "initial value",
        "training process",
        "perimental image",
        "Image normalization",
        "illumination transformation",
        "scaling transformation",
        "first sample",
        "sample size",
        "other variables",
        "Table 1 show",
        "experimental samples",
        "tween average",
        "SDM algorithm",
        "120 samples",
        "Figures",
        "sults",
        "hat",
        "light",
        "shade",
        "darkness",
        "matches",
        "finiteness",
        "positioning",
        "eyebrows",
        "nose",
        "mouth",
        "Zhu",
        "offset",
        "iteration",
        "current",
        "efficiency",
        "interference",
        "center",
        "x∗",
        "coordinates",
        "128 di",
        "mensions",
        "R128n",
        "operation",
        "Δx",
        "xk",
        "bk",
        "paths",
        "3.1",
        "∅",
        "3.3.3 SDM algorithm alignment result",
        "lateral face alignment results",
        "Face feature point location",
        "iteration step size",
        "objective function Eq",
        "principal component analysis",
        "world coordinate system",
        "rigid body invariance",
        "LFW face databases",
        "Surrey Face Model",
        "current spatial position",
        "new spatial position",
        "general 3D face",
        "3.4.1 Feature point labelling",
        "3D face model",
        "linear regression problem",
        "Local feature extraction",
        "3.4 Face pose estimation",
        "local feature points",
        "late 3D coordinates",
        "71 face feature points",
        "68 face feature points",
        "camera parame- ters",
        "3D geometric position",
        "3D rigid object",
        "geometric model parameters",
        "Sample sequence set",
        "3D position",
        "3D pose",
        "alignment accuracy",
        "132 face images",
        "206 face images",
        "803 face images",
        "input feature",
        "Matching results",
        "corresponding point",
        "The pose",
        "2D coordinates",
        "N points",
        "six points",
        "3D object",
        "test sample",
        "argminR0b0 X",
        "two parameters",
        "critical dimensions",
        "pts file",
        "head surface",
        "most cases",
        "optical properties",
        "projection relationship",
        "two movements",
        "② Rotary movement",
        "XYZ axis",
        "six degrees",
        "six numbers",
        "nose tip",
        "extraction process",
        "tion coordinates",
        "test phase",
        "test images",
        "rota- tion",
        "training set",
        "dimension reduction",
        "camera imaging",
        "Camera labeling",
        "camera relative",
        "① Translation movement",
        "hat Zhang",
        "Translation vector",
        "x0",
        "Δxi",
        "way",
        "Rk",
        "impact",
        "amount",
        "Section",
        "detail",
        "IBUG",
        "former",
        "sets",
        "orientation",
        "periments",
        "direction",
        "idea",
        "lows",
        "freedom",
        "corners",
        "SFM",
        "τ",
        "radial distortion parame- ters",
        "Match result analysis table",
        "key points Match ratio",
        "2D facial feature points",
        "False match rate",
        "y direc- tions",
        "least square method",
        "Tracking registration technology",
        "computer-generated virtual objects",
        "4.3 Feature point mapping",
        "arbitrary reference/coordinate system",
        "3.5 Tracking registration system",
        "face alignment process",
        "unknown scaling factor",
        "3D facial point",
        "tersects image plane",
        "2D image plane",
        "rotation matrix R",
        "image coordinate system",
        "2D position",
        "Head rotation",
        "P point",
        "manual labeling",
        "world coordinates",
        "focal length",
        "Variate Number",
        "Total number",
        "Wearing glasses",
        "position O",
        "linear equations",
        "Equa- tion",
        "following form",
        "correct poses",
        "tration techniques",
        "3D point",
        "3D coordinates",
        "same image",
        "pose estimation",
        "optical center",
        "translation vector",
        "camera matrix",
        "S X",
        "projection error",
        "3D model",
        "Table 1",
        "5 ¼ R",
        "5 ¼ S",
        "Yang",
        "Patras",
        "systems",
        "parameters",
        "change",
        "plenty",
        "problem",
        "fx",
        "fy",
        "ray",
        "Eqs",
        "distance",
        "tures",
        "sum",
        "squares",
        "scenes",
        "4.2",
        "664",
        "geometric transformation relation track- ing method",
        "face feature point tracking method",
        "front face alignment",
        "corresponding perspective changes",
        "poor user experience",
        "overlarge head corner",
        "good tacking effect",
        "spective transformation",
        "user head",
        "Face geometry",
        "face superposition",
        "stereoscopic changes",
        "human head",
        "first technique",
        "tech- nique",
        "two methods",
        "movement",
        "3D",
        "estimation",
        "picture"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.0775287,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1688565,
      "metadata_storage_name": "s40537-019-0184-5.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDE4NC01LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tu Nguyen Thi Ngoc ",
      "metadata_title": "Mining aspects of customer’s review on the social network",
      "metadata_creation_date": "2019-02-26T14:27:28Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Naive Bayes Open Access",
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Vietnam Electric Power University",
        "Creative Commons license",
        "Nguyen Thi Ngoc",
        "Viet Anh Nguyen2",
        "commerce web sites",
        "useful informa- tion",
        "real world datasets",
        "creat iveco mmons",
        "235 Hoang Quoc Viet",
        "Bayes classification method",
        "original author(s",
        "5-star overall rating",
        "cus- tomer reviews",
        "social network",
        "recent years",
        "significant role",
        "huge amount",
        "efficient methods",
        "ferent level",
        "important role",
        "conditional probability",
        "bootstrap technique",
        "sentiment words",
        "Experimental results",
        "good performance",
        "other state",
        "art methods",
        "Core term",
        "unrestricted use",
        "appropriate credit",
        "Full list",
        "author information",
        "aspect ratings",
        "aspect weights",
        "aspect extraction",
        "aspect words",
        "aspect consistency",
        "three tasks",
        "positive opinions",
        "customers’ opinion",
        "doi.org",
        "Mining aspects",
        "different aspects",
        "acidity aspects",
        "user sentiments",
        "users’ opinions",
        "product aspects",
        "user review",
        "Introduction",
        "lot",
        "people",
        "things",
        "products",
        "services",
        "quality",
        "challenge",
        "problem",
        "paper",
        "study",
        "attributes",
        "components",
        "concept",
        "positivity",
        "negativity",
        "example",
        "Fig.",
        "coffee",
        "Abstract",
        "solutions",
        "satisfaction",
        "degree",
        "importance",
        "manufacturers",
        "approach",
        "features",
        "frequencies",
        "comparison",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "METHODOLOGY",
        "Correspondence",
        "tunn",
        "dhdl",
        "1 Department",
        "E-Commerce",
        "Hanoi",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "body",
        "taste",
        "aroma",
        "instance",
        "Several complex filter-based approaches",
        "Turkish -style cardamon coffee",
        "Turkish-style cardamon coffee",
        "copper stove-top pot",
        "sweetened condensed milk",
        "21Nguyen Thi Ngoc",
        "Three tasks Extracting",
        "Trung Nguyen coffee",
        "overall rat- ing",
        "Hidden Markov Model",
        "aspect extraction task",
        "conditional probability technique",
        "enough core terms",
        "quency-based approaches",
        "aspect candidates",
        "Aspect Rate",
        "Estimating Aspect",
        "overall rating",
        "previous work",
        "non-aspect concepts",
        "earliest work",
        "relevant words",
        "low cost",
        "frequent nouns",
        "noun phrases",
        "dom Field",
        "manu- ally",
        "main challenge",
        "many reviews",
        "big fan",
        "thorough understanding",
        "general impression",
        "specific rating",
        "implicit aspects",
        "important aspects",
        "low-frequent aspects",
        "possible aspects",
        "wrong aspects",
        "multiple aspects",
        "training data",
        "learning techniques",
        "chocolate-like note",
        "frequency-based methods",
        "ter results",
        "universal set",
        "explicit aspects",
        "acidity",
        "difficulty",
        "noise",
        "rare",
        "concerns",
        "statistics",
        "high",
        "HMM",
        "CRF",
        "product",
        "assumption",
        "number",
        "domain",
        "experts",
        "existing",
        "sentences",
        "new",
        "MYOB",
        "January",
        "flared",
        "Istanbul",
        "stuff",
        "cream",
        "sugar",
        "1 Comment",
        "discussion",
        "section",
        "user",
        "Weight",
        "Dark",
        "flared copper stove",
        "J Big Data",
        "future research directions",
        "based, machine learning",
        "different real-life datasets",
        "data mining algorithm",
        "two important tasks",
        "aspect-based opinion mining",
        "associated overall rating",
        "aspect-based rating inference",
        "many researches work",
        "aspect weighting tasks",
        "Different approach",
        "earliest researches",
        "Related work",
        "top pot",
        "condensed milk",
        "supervised approach",
        "sophisticated state",
        "Problem definition",
        "last decade",
        "increasing attention",
        "sentiment analysis",
        "topic modeling",
        "label assignment",
        "commercial companies",
        "improve- ments",
        "filtering approach",
        "Method” sections",
        "frequency threshold",
        "frequency-based approach",
        "aspect extracting",
        "aspect identification",
        "aspect terms",
        "review content",
        "art approaches",
        "The Fig. 2",
        "similar solution",
        "information distance",
        "frequency-based method",
        "regression methods",
        "interesting methods",
        "words frequency",
        "chocolate",
        "note",
        "Body",
        "Aroma",
        "Acidity",
        "bitter",
        "weights",
        "aspects",
        "reviews",
        "fact",
        "accuracy",
        "Results",
        "details",
        "methodology",
        "experimental",
        "evaluation",
        "Conclusion",
        "area",
        "Researchers",
        "survey",
        "nouns",
        "Hu",
        "Liu",
        "part",
        "speech/POS",
        "occurrence",
        "quencies",
        "frequent",
        "spite",
        "simplicity",
        "business",
        "limita",
        "high-frequency",
        "problems",
        "filters",
        "seed",
        "0.",
        "many practical sentiment analysis applications",
        "abilistic Latent Semantic Analysis",
        "real-life sentiment analysis applications",
        "two main basic models",
        "current topic modeling methods",
        "two parameter vectors",
        "Latent Dirichlet allocation",
        "topic mod- eling",
        "large document collec",
        "topic- modeling approaches",
        "mining textual reviews",
        "Learning aspect labels",
        "traditional topic models",
        "The FLDA method",
        "aspect-specific sentiment words",
        "TripAdvisor data set",
        "latent aspect ratings",
        "many types",
        "lexicon-based methods",
        "rule-based approaches",
        "large collection",
        "latent topics",
        "negative topic",
        "principled method",
        "multi-domain reviews",
        "new method",
        "short reviews",
        "manual effort",
        "Joint Sentiment-Topic",
        "Lee [7] dataset",
        "bipartite graph",
        "small number",
        "Rating model",
        "overall ratings",
        "word distribution",
        "probabilistic inference",
        "significant amount",
        "specific entities",
        "mining aspects",
        "unlabeled data",
        "aspect price",
        "information models",
        "weak supervision",
        "full supervision",
        "AIR model",
        "various parameters",
        "reasonable results",
        "frequent topics",
        "unsupervised approach",
        "review rating",
        "LDA model",
        "frequent aspects",
        "labeled sentences",
        "distance",
        "other",
        "dollars",
        "generalization",
        "practice",
        "limitations",
        "texts",
        "researches",
        "pLSA",
        "authors",
        "adjectives",
        "JST",
        "Both",
        "positive",
        "Pang",
        "distributions",
        "addition",
        "Moghaddam",
        "Ester",
        "fac",
        "item",
        "work",
        "fication",
        "sampling",
        "extraction",
        "reviewers",
        "unbalance",
        "tuning",
        "order",
        "Such",
        "Gini Index based feature selection method",
        "The Gini Index method",
        "large movie review data set",
        "Naïve Bayes",
        "Support Vector Machine",
        "machine learning approaches",
        "multiple layer architecture",
        "different sentiment levels",
        "adjectival modifying relations",
        "knowl- edge representation",
        "aspect-based sentiment analysis",
        "Machine learning methods",
        "product overall ratings",
        "possible K aspects",
        "associated sentiment words",
        "annotated data",
        "supervised learning",
        "associated orientations",
        "Movie Ontology",
        "multiple sentences",
        "possible worlds",
        "wk| k",
        "sentiment score",
        "sentiment lexicon",
        "Sentiment classification",
        "dependency relations",
        "predicate relations",
        "verb-object relations",
        "multiple words",
        "dictionary-based approach",
        "Xiaowen Ding",
        "Minqing Hu",
        "eRank algorithm",
        "Synonym lexicon",
        "relative clause",
        "rela- tions",
        "Non-features nouns",
        "proper nouns",
        "brand names",
        "verbal nouns",
        "personal nouns",
        "Peñalver-Martinez",
        "domain ontology",
        "critical issue",
        "supervised methods",
        "two sets",
        "Decision Tree",
        "Neural Network",
        "Maximum Entropy",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "two parts",
        "core terms",
        "movie reviews",
        "topic words",
        "nouns/noun phrases",
        "higher accuracy",
        "SVM) classifier",
        "input text",
        "product features",
        "product coffee",
        "word dictionary",
        "taste aspect",
        "Opinions",
        "respect",
        "polarity",
        "strength",
        "intensification",
        "negation",
        "document",
        "Yan",
        "subject",
        "list",
        "synonyms",
        "basis",
        "cost",
        "money",
        "dictionaries",
        "training",
        "testing",
        "classifiers",
        "DT",
        "Asha",
        "research",
        "model",
        "prediction",
        "techniques",
        "attribute",
        "component",
        "aj",
        "A(.",
        "operator",
        "aftertaste",
        "mouth",
        "supervised learning method",
        "Naive Bayes method",
        "many other people",
        "Aspect core terms",
        "field experts",
        "probability distribution",
        "multiple labels",
        "Notation Description",
        "non-negative weights",
        "R K",
        "K-dimensional vector",
        "corresponding aspect",
        "Extracting aspect",
        "one aspect",
        "aspect rate",
        "aspect expressions",
        "j-th aspect",
        "same aspect",
        "aspect labels",
        "higher weight",
        "Major notations",
        "K aspect",
        "negative words",
        "aspect aj",
        "vector ri",
        "same review",
        "reviews’ text",
        "positive words",
        "review i",
        "∑K",
        "αi",
        "riK",
        "rij",
        "opinion",
        "assessment",
        "rmin",
        "range",
        "Definition",
        "emphasis",
        "associated",
        "Cj",
        "wjk",
        "Table",
        "goal",
        "task",
        "sentence",
        "1,Q",
        "yi",
        "wk",
        "corpus",
        "Sj",
        "Tj",
        "aij",
        "idea",
        "observations",
        "formula",
        "account",
        "occurrences",
        "frequency",
        "subset",
        "initial aspect core terms",
        "flavor taste aftertaste mouthfeel",
        "conditional probabilistic model",
        "initial aspect labeling",
        "initial core terms",
        "original core terms",
        "Aspect Extraction Algorithm",
        "universal label set",
        "new core terms",
        "corresponding aspect words",
        "new-found aspect words",
        "new aspect word",
        "body acidity acid",
        "bootstrapping algorithm",
        "aspect weight",
        "Aspect ratings",
        "new words",
        "existing methods",
        "Bootstrap technique",
        "bol O",
        "symbol X",
        "high probability",
        "sion-based methods",
        "two parameters",
        "following equation",
        "new set",
        "four aspects",
        "K aspects",
        "two aspects",
        "reviews’ texts",
        "new sentences",
        "four circles",
        "maximum number",
        "incorrect labels",
        "coffee product",
        "richer set",
        "Bayes",
        "Figure",
        "sets",
        "ishing",
        "smell",
        "threshold",
        "one",
        "adverbs",
        "process",
        "procedure",
        "step",
        "iterations",
        "sum",
        "θ",
        "most sentiment analysis work",
        "Naïve Bayes method",
        "Speech tech- nique",
        "two consecutive words",
        "linear regression methods",
        "good grassy note",
        "following two sentences",
        "two noun phrases",
        "aspect rating problem",
        "two patterns",
        "candidate sentiment",
        "other methods",
        "same time",
        "key point",
        "important point",
        "weighted sum",
        "multi-label classification",
        "different contexts",
        "big problem",
        "big room",
        "syntactic patterns",
        "fixed patterns",
        "JJ tags",
        "NN tags",
        "RB tags",
        "VB tags",
        "Laplace transformation",
        "class c",
        "one word",
        "first word",
        "k-th aspect",
        "opposite sentiments",
        "same adjective",
        "POS tags",
        "P(rij",
        "class label",
        "second word",
        "rating label",
        "rating rij",
        "rijαij",
        "third rules",
        "feature fk",
        "word features",
        "probability rij",
        "review",
        "content",
        "requirement",
        "Eq.",
        "labels",
        "known",
        "Part",
        "staff",
        "naj",
        "smoothing",
        "∑",
        "POS labeled rules",
        "RBS JJ",
        "JJ JJ",
        "RBS VB",
        "third word",
        "JJ NN",
        "NNS JJ",
        "fq",
        "∏q",
        "fk",
        "fj",
        "Table 2",
        "RBR",
        "VBD",
        "VBN",
        "VBG",
        "method"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    }
  ]
}

### Query 5 ###
{
  "@odata.context": "https://atc-aisearch.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 14.878597,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope wit",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1702203,
      "metadata_storage_name": "s40537-020-00292-y.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "metadata_creation_date": "2020-02-24T16:27:45Z",
      "keyphrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "big data processing technologies",
        "A Feature Information Gain",
        "other online shopping sites",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "RESEARCH Narayanan",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "doi.org",
        "prediction accuracy",
        "Introduction",
        "Extracting",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "new prod- uct",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "Consumer product success",
        "product pre-launch prediction",
        "future work” section",
        "user rating matrix",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "low dimensionality",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "Related work",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "marketing strategies",
        "embedded process",
        "prediction classifiers",
        "Page",
        "15Narayanan",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "gener",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "MF",
        "Hao",
        "item",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "ity related matrix factorization",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "flip cart customer reviews",
        "ith customer review Ri",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "port Vector Machine",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "product rating scale",
        "categorical, real",
        "Regression Testing Dataset",
        "Dataset pre‑processing",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "data pre",
        "best information",
        "prediction classifier",
        "multivariate data",
        "pre-launch prediction",
        "Product feature",
        "input dataset",
        "Product categories",
        "feature selection",
        "processing Feature",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "particular review",
        "user requirements",
        "feature impurity",
        "market industry",
        "Battery life",
        "Review Content",
        "user features",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "fk",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "launch predic- tion",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "�G value",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "saveAsSequenceFile",
        "path",
        "map",
        "groupBykey",
        "ReduceBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "Support Vector Machine classifiers",
        "t training feature vectors",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "Prediction classifiers",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "+ b",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "mth review",
        "ith review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2 G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "system perfor- mance",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "system response time",
        "Spark python API",
        "several case studies",
        "system design factors",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "redundant customer reviews",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "proposed system",
        "separate servers",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "predic- tion",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "DMRDF model 740",
        "other gini-index",
        "Product price",
        "scalability requirements",
        "other state",
        "Gini-index model",
        "application development",
        "16 GB",
        "gramming",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Months LSA-based DMRDF Gini-index",
        "LSA-based meth- ods",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "Classifier Logistic regression",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "Dataset size",
        "Gini-index approaches",
        "significant features",
        "LSA-based approaches",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "1GB 5GB",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "The DMRDF",
        "large dataset",
        "many features",
        "Performance comparison",
        "future work",
        "PA measures",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "18GB",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "7",
        "other product feature identification",
        "different reliable online websites",
        "prediction model- ling",
        "data processing domains",
        "information fusion approach",
        "statistical prop- erties",
        "memory computation method",
        "time streaming predictions",
        "dataset model performance",
        "important role",
        "DMRDF model",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "Proposed design",
        "unified API",
        "customer comments",
        "learning algorithms",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "machine",
        "wit"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 13.303898,
      "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 593265,
      "metadata_storage_name": "Köchling-Wehner2020_Article_DiscriminatedByAnAlgorithmASys.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L0slQzMlQjZjaGxpbmctV2VobmVyMjAyMF9BcnRpY2xlX0Rpc2NyaW1pbmF0ZWRCeUFuQWxnb3JpdGhtQVN5cy5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Alina Köchling ",
      "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development",
      "metadata_creation_date": "2020-11-19T15:45:16Z",
      "keyphrases": [
        "200 artificial intelligence (AI) specialists",
        "two essential HR functions",
        "Marius Claus Wehner1",
        "Heinrich-Heine-University Düsseldorf",
        "major driving forces",
        "human resource management",
        "Alina Köchling1",
        "routinized workplace decisions",
        "current human resource",
        "crucial future research",
        "HRM � Literature review",
        "Abstract Algorithmic decision-making",
        "Keywords Fairness � Discrimination",
        "human biases",
        "systematic review",
        "HR recruitment",
        "HR development",
        "HR) practices",
        "ORIGINAL RESEARCH",
        "new source",
        "unfair treatment",
        "implicit discrimination",
        "implicit) discrimination",
        "current state",
        "research gaps",
        "36 journal articles",
        "possible pitfalls",
        "important theoretical",
        "practical implications",
        "fruitful avenues",
        "fairness � Ethics",
        "rapid growth",
        "Business Administration",
        "Universitätsstrasse",
        "Business Research",
        "automated decision-making",
        "remote control",
        "Möhlmann",
        "important individual",
        "societal implications",
        "organizational optimization",
        "hidden talented",
        "large number",
        "German companies",
        "competitive advantages",
        "commercial providers",
        "algorithmic platforms",
        "performance measurements",
        "large companies",
        "economic reasons",
        "personal beliefs",
        "doi.org",
        "orcid.org",
        "context",
        "Author",
        "advice",
        "firms",
        "costs",
        "efficiency",
        "objectivity",
        "groups",
        "people",
        "unfairness",
        "knowledge",
        "threats",
        "goal",
        "directions",
        "applications",
        "researchers",
        "practitioners",
        "1 Introduction",
        "information",
        "importance",
        "digitalization",
        "organizations",
        "koechling",
        "hhu",
        "1 Faculty",
        "Economics",
        "40225 Dusseldorf",
        "Germany",
        "crossmark",
        "crossref",
        "standardization",
        "Zalmanson",
        "Algorithms",
        "humans",
        "Chalfin",
        "Lee",
        "Lindebaum",
        "changes",
        "favor",
        "employees",
        "Silverman",
        "Waller",
        "Carey",
        "Smith",
        "Savage",
        "Bales",
        "survey",
        "Deloitte",
        "Several",
        "Google",
        "IBM",
        "SAP",
        "Microsoft",
        "systems",
        "hiring",
        "Walker",
        "turn",
        "Vodafone",
        "Unilever",
        "Ikea",
        "Daugherty",
        "Wilson",
        "Precire",
        "savings",
        "time",
        "risks",
        "productivity",
        "certainty",
        "Suen",
        "McDonald",
        "McColl",
        "Michelotti",
        "Woods",
        "prejudices",
        "15",
        "American e-commerce specialist",
        "algorithmic decision- making",
        "new research avenues",
        "HR development processes",
        "unrepresentative input data",
        "algorithmic decision-making system",
        "complete algorithmic decision-making",
        "training) data",
        "algorithmic outcomes",
        "same attention",
        "same requirements",
        "first glance",
        "human decision-making",
        "unequal treatment",
        "different groups",
        "qualitative differences",
        "individual performance",
        "biased outcomes",
        "biased decisions",
        "current debate",
        "extreme disadvantage",
        "796 Business Research",
        "Previous studies",
        "common procedures",
        "labor shortages",
        "one hand",
        "other side",
        "computer science",
        "neous state",
        "distinct challenges",
        "future research",
        "practical point",
        "known companies",
        "negative consequences",
        "potential downsides",
        "potential dangers",
        "possible threat",
        "prominent example",
        "female applicants",
        "hiring decision",
        "employees’ acceptance",
        "existing knowledge",
        "current literature",
        "hiring algorithms",
        "potential biases",
        "HRM context",
        "consistency",
        "fairness",
        "Langer",
        "Florentine",
        "Raghavan",
        "application",
        "criteria",
        "Lepri",
        "discrimination",
        "Simbeck",
        "gender",
        "ethnicity",
        "Arrow",
        "Kim",
        "Barocas",
        "Selbst",
        "Suresh",
        "Guttag",
        "Chander",
        "issue",
        "transparency",
        "Dwork",
        "Diakopoulos",
        "Amazon",
        "Dastin",
        "Miller",
        "lack",
        "accountability",
        "factors",
        "Citron",
        "Pasquale",
        "question",
        "Kaibel",
        "discrepancy",
        "enthusiasm",
        "panacea",
        "inefficiencies",
        "field",
        "infancy",
        "digitization",
        "automation",
        "view",
        "aim",
        "study",
        "awareness",
        "The Oxford Living Dictionary",
        "two important HR areas",
        "several different research areas",
        "various algorithmic decision-making tools",
        "several AI decision tools",
        "other problem-solving operations",
        "two HR functions",
        "explicit human interference",
        "natural language processing",
        "several research gaps",
        "future research avenues",
        "systematic literature review",
        "HR development methods",
        "future research directions",
        "several ways",
        "workplace decision",
        "human intelligence",
        "algorithmic selection",
        "career development",
        "future performance",
        "person-organization fit",
        "serious consequences",
        "Ötting",
        "existing body",
        "ethical issues",
        "illustrative examples",
        "timely topic",
        "enormous importance",
        "reputational risk",
        "ment process",
        "key terms",
        "descriptive analysis",
        "subsequent discussion",
        "theoretical implications",
        "Conceptual background",
        "computational mechanism",
        "umbrella term",
        "wide array",
        "machine learning",
        "image recognition",
        "existing literature",
        "AI applications",
        "current employees",
        "HRM field",
        "lacking focus",
        "statistical models",
        "new data",
        "present paper",
        "distributive fairness",
        "understanding",
        "end",
        "potential",
        "Huselid",
        "Decisions",
        "algorithms",
        "individuals",
        "company",
        "society",
        "ethics",
        "procedural",
        "Maier",
        "Tambe",
        "Cappelli",
        "bias",
        "fact",
        "Companies",
        "legal",
        "applicants",
        "reason",
        "guidance",
        "detailed",
        "definitions",
        "methodology",
        "illustration",
        "processes",
        "sets",
        "rules",
        "calculations",
        "computer",
        "routinized",
        "basis",
        "prescriptions",
        "acquisition",
        "environment",
        "Russell",
        "Norvig",
        "ML",
        "speech",
        "NLP",
        "Kaplan",
        "Haenlein",
        "Paschen",
        "2.1",
        "many AI decision-making tools",
        "three major types",
        "artificial neural networks",
        "random variable Y",
        "three core elements",
        "three different levels",
        "neural network models",
        "deep learning models",
        "fixed input/output data",
        "reinforcement type learning",
        "Unsupervised ML algorithms",
        "reinforcement learning",
        "ML models",
        "network structures",
        "probabilistic models",
        "unsupervised settings",
        "learning tasks",
        "ML approach",
        "regression-type problems",
        "ground truth",
        "Human experts",
        "human decisions",
        "future instances",
        "same problem",
        "priori labeling",
        "structural behaviors",
        "theme analysis",
        "798 Business Research",
        "separate group",
        "error interactions",
        "dynamic environment",
        "Single nodes",
        "human brain",
        "human thinking",
        "other settings",
        "prediction tasks",
        "expected values",
        "systematic error",
        "human judgment",
        "algorithmic evaluations",
        "computer systems",
        "black box",
        "glass boxes",
        "making sense",
        "human involvement",
        "Cognitive biases",
        "entire model",
        "nonlinear transformation",
        "individual components",
        "input data",
        "Shin",
        "predictions",
        "outputs",
        "labels",
        "patterns",
        "Canhoto",
        "contrast",
        "Murphy",
        "variables",
        "methods",
        "trial",
        "bling",
        "methodologies",
        "Examples",
        "complex",
        "relationship",
        "phases",
        "layers",
        "neurons",
        "Bengio",
        "multiple",
        "stages",
        "features",
        "representations",
        "advantage",
        "Deng",
        "Yu",
        "Reason",
        "estimation",
        "bY",
        "difference",
        "Kauermann",
        "Kuechenhoff",
        "Goodfellow",
        "uncertainty",
        "Kahneman",
        "others",
        "Friedman",
        "Nissenbaum",
        "HRM",
        "Cheng",
        "Hackett",
        "theory",
        "consideration",
        "distinction",
        "interpretability",
        "explainability",
        "Roscher",
        "combination",
        "former",
        "simulatability",
        "parameters",
        "2.2",
        "predictive algorithmic decision-making tool",
        "accurate algorithmic output",
        "explicit human judgments",
        "input data set",
        "historical employment data",
        "algorithmic transparency",
        "historical data",
        "ML model",
        "different sets",
        "HR department",
        "employee satisfaction",
        "turnover intentions",
        "specific criteria",
        "underlying reasons",
        "main reasons",
        "learning process",
        "exposed examples",
        "one way",
        "subsequent analysis",
        "worst case",
        "discriminatory outputs",
        "manual programming",
        "recruitment process",
        "white men",
        "protected group",
        "systematic disadvantage",
        "biased data",
        "historical biases",
        "different biases",
        "preexisting biases",
        "Contextual information",
        "emergent bias",
        "representation bias",
        "specific information",
        "personality characteristics",
        "ML algorithm",
        "valid example",
        "implicit bias",
        "decomposability",
        "level",
        "training",
        "Interpretability",
        "element",
        "domain",
        "interpretations",
        "derive",
        "conclusions",
        "results",
        "prediction",
        "interest",
        "technical",
        "quality",
        "Danks",
        "London",
        "keyword",
        "garbage",
        "stereotypes",
        "Barfield",
        "Pagallo",
        "creation",
        "kind",
        "racist",
        "Veale",
        "Binns",
        "tendencies",
        "past",
        "Datta",
        "Hispanics",
        "applicant",
        "member",
        "job",
        "interview",
        "designer",
        "categories",
        "disadvantages",
        "absence",
        "selection",
        "measurements",
        "information systems Leventhal",
        "new societal knowledge",
        "different fairness definitions",
        "new knowledge",
        "several steps",
        "feature engineering",
        "free lunch",
        "discriminatory outcomes",
        "technical constraints",
        "technical consideration",
        "several reasons",
        "important conditions",
        "human constructs",
        "human interpretation",
        "cultural values",
        "two reasons",
        "different values",
        "design process",
        "cooperative efforts",
        "competitive behavior",
        "equal treatment",
        "meso- level",
        "statistical parity",
        "true outcomes",
        "fair treatment",
        "equal opportunity",
        "individual subjects",
        "representative bias",
        "Technical bias",
        "algorithmic system",
        "system design",
        "measurement data",
        "model testing",
        "best model",
        "800 Business Research",
        "training data",
        "Individual fairness",
        "group membership",
        "computer technology",
        "specific context",
        "cultural context",
        "group fairness",
        "real users",
        "protected) groups",
        "preprocessing",
        "theorems",
        "Wolpert",
        "Macready",
        "choice",
        "benchmark",
        "performance",
        "rotations",
        "example",
        "females",
        "comparison",
        "overrepresented",
        "limited",
        "hardware",
        "software",
        "peripherals",
        "Bozdag",
        "formalization",
        "computers",
        "problem",
        "judgments",
        "intuitions",
        "law",
        "litigation",
        "construction",
        "result",
        "population",
        "shift",
        "mismatch",
        "Table",
        "state",
        "art",
        "competition",
        "individualistic",
        "needs",
        "overview",
        "measures",
        "hand",
        "micro-level",
        "Hardt",
        "notions",
        "sense",
        "positives/negatives",
        "sources",
        "disadvantage",
        "2.3",
        "fairness Name Author Definition Individual fairness Dwork",
        "similar’’ classifications Group fairness",
        "descriptive personal evaluation",
        "considerable adverse consequences",
        "individual differences",
        "actual fairness",
        "fairness perceptions",
        "objective fairness",
        "Subjective fairness",
        "Discriminatory categories",
        "working experience",
        "conscious process",
        "current position",
        "personnel selection",
        "prediction systems",
        "selection context",
        "differential validity",
        "subgroup validity",
        "differential prediction",
        "biased results",
        "Table 1 Definitions",
        "P bY",
        "Pð bY",
        "opportunity Hardt",
        "False-negative rates",
        "1f g",
        "random variable",
        "802 Business Research",
        "selection process",
        "applicant pool",
        "training process",
        "meaningful effects",
        "negative experiences",
        "recruitment context",
        "job offer",
        "job turnover",
        "explicit discrimination",
        "selection measure",
        "essential role",
        "equal probability",
        "conscientious behavior",
        "managerial decisions",
        "Implicit discrimination",
        "implicit attitudes",
        "HR literature",
        "years",
        "Persson",
        "Bertrand",
        "aversion",
        "support",
        "characteristics",
        "Frijters",
        "Cascio",
        "Aguinis",
        "probabilities",
        "estimates",
        "slopes",
        "intercepts",
        "subgroups",
        "Meade",
        "Fetzer",
        "Roth",
        "Bobko",
        "Bartlett",
        "usage",
        "algorithmic",
        "decision-making",
        "regard",
        "justice",
        "reality",
        "Cropanzano",
        "employers",
        "subjects",
        "positive",
        "1jG",
        "recidivism",
        "estimator",
        "likelihood",
        "altruisms",
        "Cohen-Charash",
        "Spector",
        "candidates",
        "Bauer",
        "morale",
        "intentions",
        "acceptance",
        "rejection",
        "dissatisfaction",
        "reduction",
        "elimination",
        "conflicts",
        "Gilliland",
        "McCarthy",
        "Hausknecht",
        "image",
        "systematic literature review approach",
        "systematic literature reviews",
        "Several online platforms",
        "extensive keyword searching",
        "defined individual concepts",
        "three core dimensions",
        "fruitful research avenues",
        "high procedural justice",
        "three fairness dimensions",
        "three dimensions",
        "replicable approach.1",
        "individual fairness",
        "research question",
        "rating companies",
        "Van Hoye",
        "most urgency",
        "equal opportunities",
        "ethical norms",
        "important role",
        "relevant information",
        "algorithmic decision-making",
        "information processing",
        "human raters",
        "employee evaluation",
        "personal contact",
        "reliable picture",
        "discrimination potential",
        "particular field",
        "overall picture",
        "two authors",
        "Distributive justice",
        "informational justice",
        "fairness measures",
        "Search terms",
        "interactional justice",
        "development process",
        "organizational context",
        "evidence-based decisions",
        "decision process",
        "iterative process",
        "interactional fairness",
        "same treatment",
        "possibility",
        "outcome",
        "Rules",
        "equality",
        "equity",
        "accordance",
        "contributions",
        "extent",
        "Leventhal",
        "Consistency",
        "accuracy",
        "representation",
        "parties",
        "correction",
        "appropriateness",
        "dignity",
        "courtesy",
        "respect",
        "share",
        "general",
        "procedures",
        "errors",
        "3 Methods",
        "insights",
        "other",
        "suggestions",
        "Petticrew",
        "Roberts",
        "Crossan",
        "Apaydin",
        "Siddaway",
        "Gough",
        "databases",
        "discussion",
        "keywords",
        "3.1",
        "EBSCO Business Source Premier database",
        "social science citation index",
        "language peer-reviewed journals",
        "first source",
        "singular/plural forms",
        "different spellings",
        "narrow terms",
        "classification terms",
        "complete list",
        "broad coverage",
        "impact factor",
        "reasonable proxy",
        "important publications",
        "reference section",
        "search strings",
        "relevant articles",
        "scholarly literature",
        "electronic databases",
        "terminology",
        "synonyms",
        "contents",
        "priority",
        "balance",
        "sensitivity",
        "specificity",
        "unintentional",
        "omission",
        "SSCI",
        "management",
        "part",
        "Web",
        "Knowledge",
        "breadth",
        "papers"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 10.760407,
      "content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6,\nArticle No. 203,",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 3040236,
      "metadata_storage_name": "Liang-Lin2021_Article_MachineLearningForDigitalTry-o.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L0xpYW5nLUxpbjIwMjFfQXJ0aWNsZV9NYWNoaW5lTGVhcm5pbmdGb3JEaWdpdGFsVHJ5LW8ucGRm0",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Administrator",
      "metadata_title": "01-CVM0189.pdf",
      "metadata_creation_date": "2021-04-14T08:29:06Z",
      "keyphrases": [
        "variable 3D human body shapes",
        "Computational Visual Media",
        "Ming C. Lin1",
        "The Author(s",
        "direct visual impression",
        "M. C. Lin",
        "2D image learning",
        "substantial technological gap",
        "human body estimation",
        "open research issues",
        "three major challenges",
        "human shapes",
        "open issues",
        "accurate estimation",
        "extensive research",
        "open challenges",
        "2 Open problems",
        "Machine learning",
        "doi.org",
        "Review Article",
        "Junbang Liang1",
        "economic benefits",
        "practical constraints",
        "accurate sizing",
        "art approaches",
        "future directions",
        "rapid pace",
        "recent years",
        "retail store",
        "attractive alternative",
        "user experience",
        "fashion shopping",
        "physical try",
        "image- editing",
        "College Park",
        "J. Liang",
        "ultimate goal",
        "real worlds",
        "physical worlds",
        "consumer devices",
        "faithful recovery",
        "sewing patterns",
        "traditional methods",
        "constrained problems",
        "learning-based approaches",
        "machine-learning algorithms",
        "tedious data",
        "challenging problems",
        "other areas",
        "several reasons",
        "material modeling",
        "accurate modeling",
        "faithful modeling",
        "garment material",
        "garment pieces",
        "recent advances",
        "template demonstrations",
        "realistic demonstration",
        "notable impact",
        "unseen images",
        "garment modeling",
        "digital try",
        "Abstract Digital",
        "tremendous potential",
        "problem description",
        "sizing systems",
        "Vol.",
        "June",
        "progress",
        "commerce",
        "people",
        "lives",
        "development",
        "realism",
        "complete",
        "state",
        "Keywords",
        "1 Introduction",
        "Consumers",
        "situation",
        "clothes",
        "needs",
        "attention",
        "technology",
        "1 University",
        "Maryland",
        "USA",
        "mail",
        "liangjb",
        "umd",
        "Manuscript",
        "fast",
        "customer",
        "garments",
        "lossless",
        "transformation",
        "virtual",
        "paper",
        "sizes",
        "ease",
        "design",
        "manipulation",
        "end-users",
        "training",
        "optimization",
        "processing",
        "importance",
        "present",
        "improvements",
        "solutions",
        "significant",
        "section",
        "shoppers",
        "lack",
        "consistency",
        "direct 3D body scanning",
        "full fabric digital model",
        "accurate human shape estimation",
        "parametric human model",
        "commodity mobile devices",
        "uncovered body parts",
        "2D/3D body skeletons",
        "successful digital try",
        "Human body reconstruction",
        "human body geometry",
        "entire human body",
        "notable technology gap",
        "voxel- based representation",
        "3D humanoid mesh",
        "standard body shapes",
        "human-body reconstruction problem",
        "3 Human shape estimation",
        "many other works",
        "machine learning methods",
        "Accurate estimation",
        "digital representation",
        "shape learning",
        "body movements",
        "accurate results",
        "digital worlds",
        "digital surveillance",
        "perceptual gap",
        "3D mesh-based",
        "3D animation",
        "early works",
        "recent works",
        "learning process",
        "Different fabric",
        "actual material",
        "real-world examples",
        "two approaches",
        "2D image-based",
        "photo-realistic rendering",
        "considerable effort",
        "2D images",
        "user-friendly design",
        "realistic animation",
        "other factors",
        "Previous work",
        "cloud computing",
        "animation speed",
        "high-quality, interactive",
        "computer animation",
        "special effects",
        "augmented environments",
        "popular topic",
        "specialized hardware",
        "RGB images",
        "Traditional algorithms",
        "optimization problem",
        "silhouette difference",
        "objective function",
        "target function",
        "response time",
        "network models",
        "real-world datasets",
        "joint positions",
        "various ways",
        "current state",
        "different brands",
        "garment materials",
        "garment database",
        "key consideration",
        "online shopping",
        "regression network",
        "body-joint losses",
        "standardization",
        "persons",
        "proportion",
        "consumers",
        "correspondence",
        "customers",
        "appearance",
        "advantages",
        "drawbacks",
        "large",
        "support",
        "variation",
        "case",
        "system",
        "backend",
        "motion",
        "pose",
        "variety",
        "areas",
        "challenging",
        "interest",
        "excellent",
        "adoption",
        "input",
        "one",
        "size",
        "output",
        "major",
        "tight",
        "advances",
        "parameters",
        "techniques",
        "annotations",
        "most",
        "Ref.",
        "complex data generation pipeline",
        "iterative value correction structure",
        "synthetic data generation pipeline",
        "garment material modeling problem",
        "convenient registration process",
        "larger garment dataset",
        "compelling visual simulation",
        "parameter prediction block",
        "image style transfer",
        "4 Garment material modeling",
        "good pose estimation",
        "single view image",
        "unified human shape",
        "human body parameters",
        "Network structure",
        "recurrent structure",
        "RNN-like structure",
        "regression block",
        "Prediction results",
        "fabric modeling",
        "error correction",
        "visual effects",
        "estimated pose",
        "body pose",
        "human bodies",
        "training data",
        "key contribution",
        "several stages",
        "image feature",
        "input guesses",
        "same time",
        "3D joints",
        "loss computation",
        "global orientation",
        "static multi-view",
        "cyclic form",
        "corrective values",
        "ground-truth parameters",
        "performance gap",
        "real-world images",
        "other variables",
        "skin color",
        "3D backgrounds",
        "subtle elements",
        "higher expense",
        "promising direction",
        "realistic images",
        "important role",
        "Physical recreation",
        "challenging task",
        "physical properties",
        "physical behaviour",
        "semantic primitives",
        "sufficient amount",
        "physical behavior",
        "synthetic result",
        "visual information",
        "camera projection",
        "image inputs",
        "shape parameters",
        "camera calibration",
        "multi-view inputs",
        "shape estimation",
        "multi-stage framework",
        "traditional datasets",
        "recent progress",
        "162 J. Liang",
        "different views",
        "fixed number",
        "universal model",
        "ambiguity",
        "Fig.",
        "step",
        "code",
        "gamma",
        "researchdirections",
        "virtualtryon",
        "humanmultiview",
        "digital",
        "Challenges",
        "permission",
        "2D",
        "features",
        "CNNs",
        "use",
        "gradient",
        "Experiments",
        "art",
        "example",
        "regularization",
        "cases",
        "diversity",
        "addition",
        "hair",
        "GAN",
        "legs",
        "chest",
        "person",
        "Author",
        "Introduction",
        "systems",
        "cloth",
        "type",
        "materials",
        "sewing",
        "researchers",
        "underlying",
        "image signal feature extraction method",
        "temporal sequence learning method",
        "CNN conv5-layer activation visualization",
        "deep neural network model",
        "material parameter sensitivity analysis",
        "spring–mass systems",
        "simple feed-forward operation",
        "varying lighting conditions",
        "high-level visual features",
        "optimal material parameter",
        "traditional optimization methods",
        "image features",
        "machine-learning methods",
        "visual appearance",
        "CNN layer",
        "two implications",
        "physical model",
        "many ways",
        "finite elements",
        "popular model",
        "realistic results",
        "isotropic properties",
        "Poisson ratio",
        "anisotropic model",
        "different behaviors",
        "4.2 Learning-based estimation",
        "fast feedback",
        "art model",
        "solution space",
        "positive coefficient",
        "larger number",
        "based simulation",
        "consequent movement",
        "video recording",
        "other object",
        "real-world scenarios",
        "new insights",
        "challenging problem",
        "natural extension",
        "human body",
        "partial occlusion",
        "differentiable physics",
        "geometric differences",
        "unseen video",
        "real material",
        "material sub-space",
        "material properties",
        "material cloning",
        "material type",
        "material parameters",
        "long time",
        "real time",
        "data samples",
        "motion patterns",
        "garment prototyping",
        "small amount",
        "single piece",
        "material basis",
        "cloth simulations",
        "cloth meshes",
        "LSTM layer",
        "cloth motion",
        "clothes materials",
        "input videos",
        "computer",
        "Young",
        "modulus",
        "choice",
        "weave",
        "predictions",
        "applications",
        "Yang",
        "unc",
        "VideoCloth",
        "respect",
        "deformation",
        "sub-spaces",
        "texture",
        "mapping",
        "framework",
        "clothing",
        "approach",
        "fabric",
        "parts",
        "simulated",
        "skirt",
        "environment",
        "computation",
        "constant diagonal weight matrix",
        "high dimensional simulation problem",
        "finite element method",
        "modern simulation algorithms",
        "differentiablecloth 164 J. Liang",
        "material parameter estimation",
        "stable integration results",
        "other differentiable simulations",
        "Differentiable simulation embedding",
        "implicit Euler method",
        "continuous collision detection",
        "original mesh state",
        "general differentiable collision",
        "task-specific loss function",
        "impact zone optimization",
        "differentiable cloth simulation",
        "high dimensionality",
        "QP problem",
        "constraint matrix",
        "impact zones",
        "M̂a",
        "original paper",
        "general approach",
        "physics simulations",
        "complete � Liang",
        "implicit differentiation",
        "Collision handling",
        "mass matrix",
        "typical usage",
        "Similar processes",
        "pixel space",
        "3D space",
        "visual effect",
        "physics work",
        "rigid bodies",
        "particle-grid systems",
        "first work",
        "follow-up work",
        "computational flow",
        "common approach",
        "collision response",
        "linear solver",
        "input parameters",
        "discontinuous states",
        "several techniques",
        "physical solution",
        "linear solution",
        "solution z",
        "physics simulator",
        "dynamic approach",
        "minimum change",
        "distance function",
        "quadratic form",
        "constraint parameters",
        "motion control",
        "handling algorithm",
        "collision constraints",
        "static LCP",
        "neural network",
        "corresponding gradients",
        "gradient values",
        "difference",
        "target",
        "computing",
        "dynamics",
        "discretization",
        "order",
        "discontinuity",
        "zero-measure",
        "backpropagation",
        "QR",
        "decomposition",
        "pipeline",
        "Derivatives",
        "Jacobian",
        "forces",
        "acceleration",
        "equation",
        "readers",
        "details",
        "Collisions",
        "contacts",
        "data",
        "edu",
        "learning",
        "tasks",
        "implementation",
        "instances",
        "Gz",
        "vertex",
        "numbers",
        "variables",
        "3.1",
        "3.2",
        "multiple adjacent collision pairs",
        "interactive apparel design systems",
        "automatic garment model generation",
        "Material parameter estimation results",
        "Realistic apparel model generation",
        "garment generation algorithm",
        "other body shapes",
        "constant wind force",
        "rigid body dynamics",
        "differentiable simulation pipeline",
        "bending stiffness models",
        "baselines Method Runtime",
        "general cloth model",
        "fashion apparel",
        "material model",
        "garment models",
        "realistic 3D",
        "Garment modeling",
        "reaction force",
        "universal pipeline",
        "Simulation error",
        "L/∂z",
        "loss function",
        "linear system",
        "one contact",
        "linear equation",
        "QR decomposition",
        "high dimensional",
        "QP problems",
        "The scene",
        "three parts",
        "stiffness S",
        "stiffness B.",
        "most measurements",
        "linear part",
        "stiffness matrix",
        "unknown parameters",
        "generalized coordinates",
        "Frobenius norms",
        "simulated result",
        "vertex distances",
        "gradient-based method",
        "smaller errors",
        "computing gradients",
        "rapid changes",
        "fashion trends",
        "growing need",
        "different applications",
        "Application requirements",
        "diverse set",
        "different types",
        "image sketches",
        "general garments",
        "optimization formulation",
        "stiffness error",
        "cloth movement",
        "Linear stretching",
        "many challenges",
        "key observation",
        "inputs",
        "goal",
        "dimensionality",
        "rank",
        "4 vertices",
        "12 variables",
        "experiments",
        "ability",
        "piece",
        "gravity",
        "density",
        "Table 1",
        "comparison",
        "Qiao",
        "dt",
        "q̇",
        "response",
        "L-BFGS.",
        "Values",
        "average",
        "iter",
        "Liang",
        "Gf",
        "inclusion",
        "constraints",
        "neighborhood",
        "approximation",
        "topology",
        "customization",
        "extent",
        "Huang",
        "front",
        "Wang",
        "−",
        "5",
        "conditional generative adversarial network",
        "Image-based clothes changing system",
        "skinned multi-person linear model",
        "learning-based parametric generative model",
        "irregular 3D mesh data",
        "M. C. Lin References",
        "3D garment mesh data",
        "latent space learning",
        "relative geometric information",
        "machine learning algorithms",
        "convolutional neural networks",
        "Convolutional pose machines",
        "regular image data",
        "arbitrary human motions",
        "National Science Foundation",
        "4th International Conference",
        "A. Eds. Springer",
        "2D human representations",
        "garment sewing patterns",
        "U–V space",
        "2D displacement image",
        "Visual Media Vol.",
        "One possible approach",
        "The 2D representation",
        "human shape estimation",
        "deep garment representation",
        "garment material prediction",
        "M. J. SMPL",
        "human body shape",
        "one model",
        "3D Vision",
        "estimation accuracy",
        "A. O.",
        "Computer Science",
        "IEEE Conference",
        "One possibility",
        "M. HS-Nets",
        "limited topology",
        "recent work",
        "two problems",
        "Different topology",
        "different values",
        "traditional CNN",
        "different person",
        "virtual reality",
        "excellent potential",
        "open problems",
        "art methods",
        "daily tasks",
        "good start",
        "multiple garments",
        "optimization-based methods",
        "intersection loss",
        "new solution",
        "garment type",
        "substantial breakthroughs",
        "Iribe Professorship",
        "Öztireli",
        "naked truth",
        "Computer Vision",
        "Lecture Notes",
        "Pattern Recognition",
        "ACM Transactions",
        "Article No.",
        "S.-E.",
        "same body",
        "166 J. Liang",
        "unified representation",
        "external constraints",
        "hard constraints",
        "Z. H.",
        "T. J.",
        "F. L.",
        "P. V.",
        "H. T.",
        "al.",
        "retargeting",
        "T-shirts",
        "skirts",
        "difficulties",
        "shapes",
        "cGAN",
        "6 Conclusions",
        "scenario",
        "wind",
        "applicability",
        "area",
        "investigation",
        "directions",
        "Acknowledgements",
        "research",
        "part",
        "Zhang",
        "Computational",
        "Dibra",
        "Jain",
        "Ziegler",
        "R.",
        "Gross",
        "silhouettes",
        "Proceedings",
        "Black",
        "ECCV",
        "Forsyth",
        "D.",
        "Torr",
        "Zisserman",
        "Berlin",
        "Romero",
        "Kiefel",
        "Bogo",
        "Gehler",
        "loop",
        "Loper",
        "Mahmood",
        "N.",
        "Pons-Moll",
        "G.",
        "Graphics",
        "Wei",
        "Ramakrishna",
        "Kanade",
        "Sheikh",
        "Y.",
        "Y. Realtime multi- person 2D pose estimation",
        "Realtime 3D human pose estimation",
        "Neural Information Processing Systems",
        "V. Differentiable cloth simulation",
        "Joint 3D pose",
        "Shape-aware human pose",
        "part affinity fields",
        "human body shapes",
        "De Avila Belbute-Peres",
        "Scalable differentiable physics",
        "differentiable physics engine",
        "single RGB camera",
        "3D human reconstruction",
        "IEEE International Conference",
        "3D people models",
        "Video based reconstruction",
        "IEEE conference",
        "human digitization",
        "Learning-based cloth",
        "human shape",
        "European Conference",
        "33rd Conference",
        "shape reconstruction",
        "Y. Deephuman",
        "Y. L.",
        "single image",
        "Pons- Moll",
        "I. Bodynet",
        "Volumetric inference",
        "high-resolution clothed",
        "accurate scans",
        "multi-view images",
        "Physics-inspired garment",
        "arXiv preprint",
        "inverse problems",
        "B. L.",
        "H.-P.",
        "C. VNect",
        "end recovery",
        "H. PIFu",
        "L. C.",
        "single-view image",
        "material recovery",
        "deep learning",
        "a second",
        "J. FACSIMILE",
        "T. Denserac",
        "M. C.",
        "J. B.",
        "Graphics Vol.",
        "Z. R.",
        "M. J.",
        "J. Z.",
        "W. T.",
        "D. W.",
        "K. A.",
        "Cao",
        "Simon",
        "S.",
        "Mehta",
        "Sridhar",
        "Sotnychenko",
        "Rhodin",
        "Shafiei",
        "Seidel",
        "Xu",
        "Casas",
        "Theobalt",
        "Alldieck",
        "Magnor",
        "Kanazawa",
        "Jacobs",
        "Malik",
        "Varol",
        "Ceylan",
        "Russell",
        "Yumer",
        "E.",
        "Laptev",
        "Zheng",
        "Dai",
        "Q.",
        "Liu",
        "Saito",
        "Natsume",
        "Morishima",
        "Pixel-aligned",
        "function",
        "Zhu",
        "Tung",
        "Smith",
        "X.",
        "Mavroidis",
        "Fast",
        "less",
        "Bhatnagar",
        "Kolotouros",
        "Pavlakos",
        "Daniilidis",
        "modelfitting",
        "Lin",
        "Pan",
        "Amert",
        "Berg",
        "Koltun",
        "control",
        "F.",
        "Allen",
        "Tenenbaum",
        "Kolter",
        "Advances",
        "Degrave",
        "Hermans",
        "Dambre",
        "Wyffels",
        "robotics",
        "Frontiers",
        "Freeman",
        "Wu",
        "real-time differentiable physical simulator",
        "3D garment generation",
        "shared shape space",
        "multimodal garment design",
        "physical simulation",
        "Computer-Aided Design",
        "W. ChainQueen",
        "International Conference",
        "Y. M.",
        "T. M.",
        "F. DiffTaichi",
        "K. X.",
        "X. Y.",
        "X. F.",
        "H. Automatic",
        "two images",
        "Virtual Reality",
        "T. Y.",
        "soft robotics",
        "N. J.",
        "Rus",
        "Matusik",
        "Automation",
        "Hu",
        "Anderson",
        "Sun",
        "Carr",
        "Ragan-Kelley",
        "Durand",
        "programming",
        "Zeng",
        "Bruniaux",
        "P.",
        "Tao",
        "Yao",
        "V.",
        "Zhao",
        "Visualization",
        "Popović",
        "Mitra"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 9.207208,
      "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data  (2018) 5:33  \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence:   \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n  • K anonymity\n  • L diversity\n  • T closeness\n  • Randomization\n  • Data distribution\n  • Cryptographic techniques\n  • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n  • More number of Mappers and Reducers were used as data volume increased.\n  • Results before and after randomization were significantly different.\n  • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n  • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018   Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data  (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Prote ction",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1156098,
      "metadata_storage_name": "s40537-018-0141-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTgtMDE0MS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "P. Ram Mohan Rao ",
      "metadata_title": "Privacy preservation techniques in big data analytics: a survey",
      "metadata_creation_date": "2018-09-20T05:58:23Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "A. P. Siva Kumar3",
        "P. Ram Mohan Rao",
        "third party data analyst",
        "modernistic privacy preservation technique",
        "Privacy preservation Open Access",
        "Creative Commons license",
        "Ram Mohan Rao",
        "S. Murali Krishna2",
        "diverse applica- tions",
        "person specific private",
        "value added services",
        "web site logs",
        "creat iveco mmons",
        "original author(s",
        "J Big Data",
        "Privacy preservation techniques",
        "privacy preserving techniques",
        "The data holder",
        "various privacy threats",
        "user activity data",
        "big data analytics",
        "The Author",
        "privacy violations",
        "privacy concerns",
        "various organizations",
        "various sources",
        "author information",
        "domain areas",
        "able availability",
        "network connectivity",
        "scale data",
        "sensitive data",
        "zip code",
        "shopping cart",
        "public domain",
        "deeper insights",
        "hidden patterns",
        "important decisions",
        "ecommerce sites",
        "buying habits",
        "Face book",
        "inference attacks",
        "Incredible amounts",
        "supply chain",
        "sion streaming",
        "social media",
        "smart phones",
        "voluminous data",
        "decision making",
        "dation systems",
        "data lake",
        "unstructured data",
        "unrestricted use",
        "appropriate credit",
        "MLR Institute",
        "Full list",
        "research challenges",
        "data utility",
        "movie recommendation",
        "digital technology",
        "exponential growth",
        "prominent applications",
        "doi.org",
        "computer technology",
        "Flip kart",
        "SURVEY PAPER",
        "Introduction",
        "volume",
        "variety",
        "computers",
        "storage",
        "gender",
        "disease",
        "caste",
        "religion",
        "businesses",
        "customers",
        "prediction",
        "casting",
        "Amazon",
        "products",
        "friends",
        "places",
        "interest",
        "number",
        "merits",
        "Abstract",
        "hospitals",
        "banks",
        "retail",
        "virtue",
        "humans",
        "machines",
        "Tons",
        "serious",
        "models",
        "limitations",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "rammohan04",
        "1 Department",
        "Science",
        "Engineering",
        "Hyderabad",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "trade",
        "Multidimensional Sensitivity Based Anonymization",
        "Many Privacy preserving techniques",
        "many smart phones users",
        "social networking application",
        "social media sites",
        "hospital holding patient",
        "12Ram Mohan Rao",
        "regular business model",
        "serious privacy breach",
        "Privacy preservation laws",
        "Privacy preservation methods",
        "privacy preservation techniques",
        "smart phone users",
        "maximum data utility",
        "external data sources",
        "private informa- tion",
        "key privacy threats",
        "Data analytics activity",
        "serious privacy threat",
        "person specific data",
        "Cryptographic techniques",
        "Many organizations",
        "Many countries",
        "many apps",
        "users data",
        "medica- tion",
        "specific problem",
        "regular basis",
        "privacy statement",
        "data Privacy",
        "privacy attacks",
        "data holder",
        "mobile apps",
        "ecommerce site",
        "data leakage",
        "Personal embracement",
        "opinion data",
        "new friends",
        "census data",
        "electoral results",
        "medical shop",
        "family member",
        "major reason",
        "Previous research",
        "K anonymity",
        "L diversity",
        "T closeness",
        "Data distribution",
        "private information",
        "individual privacy",
        "sentiment analysis",
        "statistical analysis",
        "access control",
        "various offers",
        "one community",
        "ability",
        "websites",
        "responsibility",
        "contacts",
        "files",
        "camera",
        "conditions",
        "need",
        "Surveillance",
        "Disclosure",
        "crimination",
        "abuse",
        "habits",
        "recommendations",
        "people",
        "transactions",
        "Zip",
        "sensitive",
        "disorder",
        "bias",
        "inequality",
        "instance",
        "government",
        "example",
        "medicines",
        "reminder",
        "Lack",
        "awareness",
        "list",
        "Randomization",
        "MDSBA",
        "Sno Zip Age Salary Disease",
        "Table 3 L diversity privacy preservation technique",
        "Sno Zip Age Disease",
        "equivalence classes attribute disclosure",
        "minimal gener- alization",
        "low income group",
        "sensi- tive attribute",
        "ground knowledge attack",
        "three equivalence classes",
        "huge data loss",
        "40 20k Skin allergy",
        "T closeness measure",
        "2* 6k Cardiac problem",
        "7k Cardiac problem",
        "22k Cardiac problem",
        "K anonymity algorithm",
        "K indistinguishable records",
        "Personalized privacy",
        "age attributes",
        "sensitive attribute",
        "40 Skin allergy",
        "Identity disclosure",
        "three persons",
        "40 Cardiac problem",
        "k value",
        "57677 zip codes",
        "larity attack",
        "de identification",
        "two attacks",
        "particular person",
        "two attributes",
        "vacy threat",
        "important aspect",
        "overall distribution",
        "entire records",
        "close- ness",
        "data analytics",
        "anonymized data",
        "patient data",
        "homogeneity attack",
        "40 24k Cancer",
        "576** zip",
        "3 indistinguishable",
        "Table 1",
        "40 Cancer",
        "Anonymization",
        "process",
        "attempt",
        "algorithms",
        "Incognito",
        "drian",
        "result",
        "generalization",
        "John",
        "Achieving",
        "suppression",
        "values",
        "salaries",
        "others",
        "improvement",
        "distance",
        "distributions",
        "threshold",
        "Table 4 T closeness privacy preservation technique",
        "Hadoop Distributed File System",
        "2* 9k Skin allergy",
        "same attrib- utes",
        "pre processing time",
        "third party analyst",
        "40 42k Cardiac problem",
        "Map Reduce Job",
        "Person specific information",
        "Data distribution technique",
        "age groups",
        "privacy breach",
        "time complexity",
        "one party",
        "Randomization technique",
        "low income",
        "large datasets",
        "employee database",
        "data- base",
        "More number",
        "adversary attack",
        "40 8k Flu",
        "many sites",
        "two ways",
        "aggregate functions",
        "retail store",
        "various branches",
        "Clustering algorithms",
        "different sites",
        "different organizations",
        "other parties",
        "participating sites",
        "crime investigations",
        "police officials",
        "particular criminal",
        "other records",
        "10k records",
        "5k records",
        "proper distribution",
        "probability distribution",
        "Horizontal distribution",
        "Vertical distribution",
        "attribute disclosure",
        "one site",
        "data collection",
        "data volume",
        "aggregate data",
        "respect",
        "ficult",
        "noise",
        "surveys",
        "sis",
        "knowledge",
        "anonymization",
        "head",
        "experiment",
        "employees",
        "order",
        "observations",
        "Mappers",
        "Reducers",
        "Results",
        "outliers",
        "cost",
        "Fig.",
        "operations",
        "sales",
        "analytics",
        "computations",
        "Classification",
        "situations",
        "custodian",
        "details",
        "sion",
        "attributes",
        "conventional Hadoop Distrib- uted Files System",
        "Apache Pig scripting language",
        "large scale data sets",
        "Apache MAP REDUCE",
        "large data sets",
        "tional Anonymization techniques",
        "data collection time",
        "two data items",
        "personal embarrass- ment",
        "four quasi identifiers",
        "conventional encryption techniques",
        "Differential privacy techniques",
        "conventional methods",
        "cryptographic techniques",
        "Data Anonymization",
        "sales data",
        "Data utility",
        "data records",
        "different parties",
        "police investigation",
        "health department",
        "privacy preservation",
        "aggregate computations",
        "single location",
        "single organization",
        "multiparty computation",
        "reduced loss",
        "informa- tion",
        "different nodes",
        "different bags",
        "sensitive information",
        "aggregate information",
        "information loss",
        "vertical distribution",
        "investigating officer",
        "similar technique",
        "generali- zation",
        "class values",
        "vulnerability",
        "datasets",
        "employer",
        "bank",
        "insights",
        "character",
        "overlaps",
        "inputs",
        "function",
        "case",
        "Generalization",
        "issues",
        "scalability",
        "version",
        "improved",
        "predefined",
        "blocks",
        "64 MB",
        "128 MB",
        "filters",
        "bottom",
        "Cryptographic techniques Data distribution Randomization",
        "novel privacy preservation model",
        "Various privacy preservation techniques",
        "soft computing techniques",
        "script- ing language",
        "systematic literature review",
        "large scale heterogeneous",
        "serious privacy threats",
        "identity dis- closure",
        "vacy preservation law",
        "large scale data",
        "stream- ing data",
        "data mining algorithms",
        "Apache Pig script",
        "Data Lake concept",
        "Map Reduce job",
        "various vulnerabilities",
        "Anonymization techniques",
        "robust techniques",
        "attribute preservation",
        "New techniques",
        "privacy problems",
        "privacy hazards",
        "privacy breaches",
        "identity disclosure",
        "law enforcement",
        "data privacy",
        "different groups",
        "development effort",
        "code efficiency",
        "concrete solution",
        "clustering problems",
        "Machine learning",
        "appropriate solution",
        "personal embarrassment",
        "European Union",
        "technological solutions",
        "smart phone",
        "raw format",
        "data sets",
        "Data ingestion",
        "external sources",
        "specific information",
        "personal information",
        "diverse sources",
        "strong need",
        "Table 5 Comparison",
        "native form",
        "time people",
        "bag",
        "method",
        "implementation",
        "rest",
        "Analysis",
        "features",
        "type",
        "complexity",
        "Suitability",
        "Damage",
        "Accuracy",
        "results",
        "discussions",
        "part",
        "nisms",
        "More",
        "challenges",
        "Scalable",
        "transformation",
        "protection",
        "discrimination",
        "lance",
        "Conclusion",
        "Conventional",
        "classification",
        "governments",
        "countries",
        "selves",
        "Lot",
        "messages",
        "chats",
        "leakage",
        "repository",
        "Novel privacy preservation model",
        "rigorous train- ing",
        "Advanced computing techniques",
        "privacy preserving algorithms",
        "massive parallel computing",
        "social media pages",
        "closed circuit television",
        "Sri Venkateswara College",
        "deep learning techniques",
        "existing data sets",
        "SQOOP relational data",
        "social media data",
        "A Data lake",
        "Data Lake Sqoop",
        "Ph.D. work",
        "data processing techniques",
        "machine learning",
        "Hadoop map",
        "sensitive attributes",
        "memory processing",
        "fast processing",
        "proposed model",
        "website logs",
        "customers opinion",
        "different sources",
        "HIVE tables",
        "Abbreviations CCTV",
        "literature survey",
        "necessary amendments",
        "final manuscript",
        "Computer Science",
        "Andhra Pradesh",
        "necessary corrections",
        "Competing interests",
        "many repositories",
        "Springer Nature",
        "jurisdictional claims",
        "institutional affiliations",
        "Ducange Pietro",
        "Pecori Riccardo",
        "Mezzina Paolo",
        "marketing strategies",
        "Soft Comput",
        "Chauhan Arun",
        "Kummamuru Krishna",
        "Toshniwal Durga",
        "transactional data",
        "Load data",
        "Apache Spark",
        "Author details",
        "Apache Flume",
        "intelligent algorithm",
        "Authors’ contributions",
        "JNTU Anantapur",
        "4 JNTU",
        "layers",
        "training",
        "HDFS",
        "job",
        "tokenization",
        "PRMR",
        "paper",
        "SMK",
        "APSK",
        "Technology",
        "Tirupati",
        "Anantapuramu",
        "Acknowledgements",
        "guides",
        "RDBMS",
        "�cal",
        "Availability",
        "materials",
        "one",
        "Funding",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "References",
        "glimpse",
        "framework",
        "Prediction",
        "visit",
        "tweets",
        "23rd ACM SIGMOD-SIGACT-SIGART symp. principles",
        "Privacy-preserving social media data publishing",
        "Diane L. Disclosure-limited data dissemination",
        "eighth ACM SIGKDD international conference",
        "IEEE Trans Knowl Data Eng",
        "Ramakrishnan R. Mondrian multidimensional k-anonymity",
        "J Am Stat Assoc",
        "Int J Comput Vision",
        "IEEE Trans Ind Inf.",
        "Future Gen Comput Syst",
        "practical privacy-preserving data aggregation",
        "22nd international confer- ence",
        "privacy-preserving data mining models",
        "ACM SIGMOD international conference",
        "Ramakrishnan R. Incognito",
        "J Off Stat",
        "Int J Uncertain",
        "Knowl Inf Syst.",
        "scalable multidimensional anonymization",
        "IEEE international conference",
        "self personal data",
        "big metering data",
        "efficient full-domain k-anonymity",
        "data mining solution",
        "Disclosure limitation methods",
        "big data privacy",
        "Personalized privacy preservation",
        "k-anonymity privacy protection",
        "privacy-preserving query",
        "Lambert Diane",
        "SRI International",
        "practical applications",
        "Fuzziness Knowl",
        "Williams R.",
        "Jiang R",
        "Lu R",
        "tabular data",
        "data access",
        "data engineering",
        "optimal k-anonymity",
        "Yang D",
        "Bingqing Q",
        "Cudre-Mauroux P",
        "Liu Y",
        "PDA) scheme",
        "smart grid",
        "Duncan GT",
        "statistical agencies",
        "Spiller K",
        "Hettig M",
        "Kiss E",
        "Kassel J-F",
        "Weber S",
        "Harbach M",
        "android apps",
        "Smith M",
        "usable privacy",
        "Bayardo RJ",
        "Agrawal A.",
        "optimal k-anonymization",
        "Iyengar S.",
        "privacy constraints",
        "knowledge discovery",
        "New York",
        "LeFevre K",
        "DeWitt DJ",
        "Latanya Sweeney",
        "Technical report",
        "Sweeney Latanya",
        "database systems",
        "Machanavajjhala A",
        "Xiao X",
        "Yufei T.",
        "Rubner Y",
        "Tomasi T",
        "Guibas LJ.",
        "earth mover",
        "image retrieval",
        "Aggarwal CC",
        "Philip SY",
        "general survey",
        "Choo KK",
        "high performance",
        "Wang K",
        "Yu PS",
        "Chakraborty S",
        "Fung BCM",
        "down specialization",
        "Zhang X",
        "A MapReduce",
        "disclosure risk",
        "Bottom-up generalization",
        "recommendation",
        "ISSN",
        "Print",
        "Electronic",
        "Confidentiality",
        "theory",
        "Measures",
        "harm",
        "users",
        "thoughts",
        "Self-Tracking",
        "Cham",
        "lan",
        "threats",
        "editor",
        "Symposium",
        "security",
        "SOUPS",
        "Newcastle",
        "UK",
        "July",
        "Proceedings",
        "ICDE",
        "Piscataway",
        "management",
        "Samarati",
        "Pierangela",
        "enforcement",
        "Proc.",
        "PODS",
        "L-diversity",
        "metric",
        "Springer",
        "Fourth",
        "ICDM",
        "Top",
        "approach",
        "scalable two-phase top-down specialization approach",
        "IEEE Trans Parallel Distrib Syst",
        "Third international conference",
        "analytics access control",
        "Secur ity/Abstr act",
        "Priva cy-Prote ction",
        "big data anonymity",
        "green computing",
        "Al-Zobbi M",
        "Shahrestani S",
        "Ruan C",
        "29. Schneider C.",
        "government regulations",
        "techn ologi",
        "Stren gthen",
        "data anonymization",
        "MapReduce privacy",
        "IBM Blogs",
        "preservation",
        "cloud",
        "CGC",
        "Trustcom/BigDataSE/ICESS",
        "bigge",
        "chall",
        "TCS",
        "conte",
        "Cyber"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 9.070947,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1434817,
      "metadata_storage_name": "s40708-020-00109-x.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDcwOC0wMjAtMDAxMDkteC5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Ferdousi Sabera Rawnaque ",
      "metadata_title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "metadata_creation_date": "2020-09-18T02:02:41Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Khondaker Abdullah Al Mamun",
        "high time resolution advantages",
        "Physiological response measuring techniques",
        "consumer emotion recognition-based experiments",
        "other third party material",
        "neural record- ing techniques",
        "video advertisement-based Neuromarketing experiments",
        "other machine learning algorithms",
        "Creative Commons licence",
        "Support Vector Machine",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "empirical research findings",
        "magnetic resonance imaging",
        "heart rate monitoring",
        "traditional filtering methods",
        "independent component analysis",
        "Linear Discriminant Analysis",
        "highest average accuracy",
        "market- ing research",
        "consumer response prediction",
        "Artificial Neural Network",
        "prefrontal alpha band",
        "skin conductance recording",
        "Brain computer interface",
        "brain–computer interface",
        "prevalent marketing stimuli",
        "first systematic review",
        "Neural recording",
        "unspoken response",
        "Neuromarket- ing",
        "consumer goods",
        "neural signal",
        "consumer preferences",
        "Brain Inf.",
        "brain recordings",
        "doi.org",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "commercial area",
        "last 5 years",
        "valid databases",
        "promotion forms",
        "asymmetry theory",
        "many researchers",
        "low cost",
        "eye tracking",
        "facial mapping",
        "artifact removal",
        "future researchers",
        "vital information",
        "novel contributions",
        "The Author",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "BCI) technology",
        "interdisciplinary bridge",
        "ultimate sale",
        "targeted audiences",
        "expanding economy",
        "effective marketing",
        "57 relevant literatures",
        "intended use",
        "permitted use",
        "good product",
        "new businesses",
        "Neuromarketing field",
        "Technological advancements",
        "opportunities",
        "Abstract",
        "academic",
        "interest",
        "interpreting",
        "tool",
        "consumers",
        "article",
        "purpose",
        "authors",
        "total",
        "basic",
        "trend",
        "nals",
        "electroencephalogram",
        "EEG",
        "functional",
        "fMRI",
        "parallel",
        "classification",
        "ANN",
        "SVM",
        "LDA",
        "Keywords",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "1 Introduction",
        "application",
        "invasive",
        "neuroscience",
        "perception",
        "changing",
        "non-invasive brain signal record- ing techniques",
        "1 Advanced Intelligent Multidisciplinary Systems Lab",
        "transcranial mag- netic stimulator",
        "Functional mag- netic resonance",
        "Open Access Brain Informatics",
        "market- ing stimuli",
        "image processing techniques",
        "nal recording techniques",
        "functional near-infrared spectroscopy",
        "com- petitive market",
        "United International University",
        "positron emission tomography",
        "machine learning algorithms",
        "skin conductance measurements",
        "neuronal activ- ity",
        "Different stimuli trigger",
        "marketing research techniques",
        "ventional market research",
        "traditional survey methods",
        "neural recording devices",
        "various marketing stimuli",
        "consumers’ purchase decisions",
        "Neuromarketing World Forum",
        "Advanced Research",
        "neuronal signals",
        "human brain",
        "traditional marketing",
        "Neuro- marketing",
        "academic research",
        "qualitative assessment",
        "posteriori analysis",
        "Full list",
        "author information",
        "group discussion",
        "personal interviews",
        "consumer feedback",
        "time requirement",
        "high cost",
        "unreliable information",
        "inaccurate results",
        "possible meanings",
        "new door",
        "buying behavior",
        "tal states",
        "physiological signals",
        "heart rate",
        "physiological responses",
        "spectral analysis",
        "accurate depiction",
        "Early years",
        "contro- versy",
        "high promises",
        "consumer mind",
        "buy buttons",
        "consumer behavior",
        "Business Association",
        "annual event",
        "Neuromarketing research",
        "field trials",
        "unspoken cognitive",
        "emotional responses",
        "Neuromarketing Science",
        "consumer response",
        "associated response",
        "quantitative",
        "products",
        "surveys",
        "Correspondence",
        "frawnaque",
        "umassd",
        "Institute",
        "Dhaka",
        "Bangladesh",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "19Rawnaque",
        "observations",
        "approaches",
        "limitations",
        "contrast",
        "customer",
        "electroencephalography",
        "magnetoencephalography",
        "MEG",
        "TMS",
        "fNIRS",
        "examples",
        "withdrawal",
        "change",
        "brainwaves",
        "researchers",
        "help",
        "excitement",
        "engagement",
        "stress",
        "insight",
        "audience",
        "preferences",
        "likes",
        "academician",
        "marketers",
        "due",
        "lack",
        "groundwork",
        "claim",
        "scrutiny",
        "scope",
        "NMSBA",
        "gap",
        "dialogue",
        "other physiological signal recording device",
        "best brain signal recording tool",
        "address inclusion–exclusion criteria",
        "relevant academic journal articles",
        "neural recording tools",
        "industry–academia collaboration",
        "brief intro- duction",
        "unbiased evidence collection",
        "clear con- clusions",
        "Numerous literature reviews",
        "other literature review",
        "mar- keting stimuli",
        "date research scopes",
        "systematic literature review",
        "new research field",
        "signal processing",
        "inclusion criteria",
        "Academic research",
        "relevant inference",
        "new perspective",
        "brain signals",
        "cal tools",
        "systematic review",
        "research question",
        "brain regions",
        "150 consumer neuroscience",
        "big brands",
        "efficient way",
        "lytical accuracy",
        "building blocks",
        "theoretical aspect",
        "business ethics",
        "current methods",
        "tech- niques",
        "art findings",
        "synthesized result",
        "impartial conclusion",
        "exhaustive search",
        "experimental findings",
        "neu- ral",
        "biometric data",
        "Book chapters",
        "marketing stimuli",
        "empirical experiments",
        "engineering part",
        "pretational methodologies",
        "important results",
        "lowing questions",
        "Neuromarketing experiments",
        "platform",
        "panies",
        "globe",
        "Google",
        "Microsoft",
        "Unilever",
        "insights",
        "tailored",
        "breakthrough",
        "acceptance",
        "world",
        "capacities",
        "management",
        "psychology",
        "focus",
        "regard",
        "premises",
        "types",
        "analysis",
        "techniques",
        "comprehensive",
        "knowledge",
        "methodology",
        "state",
        "synthesis",
        "recommendation",
        "body",
        "objective",
        "obligation",
        "investigation",
        "existing",
        "literatures",
        "evance",
        "studies",
        "analytical",
        "BCI",
        "language",
        "English",
        "brain signal recording techniques",
        "Neural response recording techniques",
        "Brain signal processing",
        "relevant existing literatures",
        "Machine learning applications",
        "autonomous nervous system",
        "Neuromarket- ing field",
        "different brain regions",
        "galvanic skin response",
        "Wiley Online Library",
        "Neuromarketing Product Chew",
        "neural data",
        "3  Systematic review",
        "database source",
        "book chapters",
        "six databases",
        "Taylor Francis",
        "initial article",
        "critical analy",
        "action coding",
        "successful means",
        "withdrawal actions",
        "engineering perspective",
        "statistical analy",
        "lyze activities",
        "Price Çakar",
        "Grigaliūnaitė",
        "Soria Morillo",
        "marketing strategies",
        "Neuromarketing questions",
        "search item",
        "Science Direct",
        "IEEE Xplore",
        "Table 1 Number",
        "database Results",
        "Emerald insight",
        "Neuromarketing studies",
        "following dimensions",
        "Marketing stimuli",
        "Table 2 Studies",
        "57 research articles",
        "931 articles",
        "internet",
        "Neuro-marketing",
        "publications",
        "aggregation",
        "Sage",
        "mulation",
        "title",
        "abstract",
        "keywords",
        "sis",
        "terms",
        "trends",
        "advancements",
        "Activation",
        "facial",
        "consumer",
        "arousal",
        "attention",
        "study",
        "Name",
        "tools",
        "paper",
        "form",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Touchette",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "al.",
        "Gong",
        "Pilelienė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Promotion",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "neural recording systems",
        "signal capturing technologies",
        "heart rate Cherubino",
        "EEG Soria Morillo",
        "brain region",
        "brain functions",
        "brain anatomy",
        "brain areas",
        "brain activity",
        "EMG Missagila",
        "Eye tracking",
        "Galvanic skin",
        "atic review",
        "human behavior",
        "cal functions",
        "true essence",
        "anatomical functionality",
        "high accuracy",
        "external stimuli",
        "fNIRS Çakir",
        "Neuromarketing Cherubino",
        "fMRI Venkatraman",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "subjects",
        "matter",
        "interconnection",
        "actions",
        "physiological",
        "hidden",
        "ences",
        "section",
        "targeted",
        "signals",
        "associated",
        "number",
        "3D (three dimensional) image",
        "machine learning techniques",
        "tral nervous system",
        "predictive modeling framework",
        "3D visualiza- tion",
        "other marketing models",
        "consumer behavior researchers",
        "E-commerce prod- ucts",
        "gift card limit",
        "virtual 3D bracelet",
        "30 existing image",
        "3D shapes",
        "consumer demand",
        "Consumer response",
        "consumer choice",
        "different strategies",
        "market- ing",
        "quantitative assessment",
        "rological data",
        "thinking procedures",
        "last 5  years",
        "two forms",
        "physical object",
        "important role",
        "user esthetics",
        "new area",
        "mathematical model",
        "lis superformula",
        "school bags",
        "first-time buyers",
        "partici- pants",
        "commerce website",
        "negative emotion",
        "necessary buttons",
        "sorting options",
        "Retail businesses",
        "large amount",
        "large number",
        "mock shop",
        "right-handed participants",
        "test participants",
        "different designs",
        "shoe designs",
        "E-commerce products",
        "retail products",
        "test subjects",
        "normal vision",
        "brain response",
        "user experience",
        "42 product images",
        "product search",
        "wrong product",
        "possible preference",
        "60 bracelet",
        "medications",
        "history",
        "neuropathology",
        "Nemorin",
        "price",
        "promotions",
        "service",
        "beverage",
        "decision",
        "objects",
        "recognition",
        "bracelets",
        "advantage",
        "apparels",
        "accessory",
        "items",
        "shirts",
        "sweaters",
        "shoes",
        "wrist",
        "watches",
        "view",
        "maximum",
        "money",
        "thousands",
        "blueprints",
        "manufacturing",
        "failures",
        "3.1",
        "corporate social respon- sibilities",
        "frontal asymmetry theory",
        "female undergraduate students",
        "capital white letter",
        "luxury) prod- ucts",
        "high emotional value",
        "higher emotional value",
        "shoe selection time",
        "high-resolution computer screen",
        "60 basic brand items",
        "wine tasting experiment",
        "regular brand products",
        "luxury brand products",
        "brain response-based prediction",
        "consumer brain response",
        "60 luxury items",
        "gift value",
        "brand images",
        "brand names",
        "brand logos",
        "national brand",
        "social atmosphere",
        "apparel products",
        "branded products",
        "shoe experiment",
        "EEG signals",
        "Likert scale",
        "self-report-based methods",
        "sales data",
        "12.1% profit growth",
        "36.4% profit growth",
        "young adults",
        "random order",
        "alone atmosphere",
        "brain responses",
        "consumer attitude",
        "Tahoma font",
        "black background",
        "other hand",
        "implicit response",
        "different types",
        "important factor",
        "interesting concept",
        "tional companies",
        "responsible companies",
        "decision-making time",
        "passive role",
        "keting strategies",
        "lower degree",
        "con- trol",
        "pivotal role",
        "purchase decision",
        "two wines",
        "lesser price",
        "rank",
        "simulation",
        "choice",
        "Davidson",
        "comparison",
        "brands",
        "blocks",
        "popular",
        "experiments",
        "branding",
        "user",
        "first-time",
        "promotional",
        "relation",
        "author",
        "company",
        "socially",
        "influence",
        "discounts",
        "gifts",
        "gift-giving",
        "ambiguity",
        "40",
        "mon experimental design procedure",
        "merchandise product advertisement clips",
        "wine tasting session",
        "older adults’ reaction",
        "mated review systems",
        "different aged population",
        "facial biometric sensors",
        "TV commer- cials",
        "six smartphone commercials",
        "super bowl commercials",
        "promotion-based Neuromarketing experiments",
        "The Neuromarketing experiments",
        "sustainable product designs",
        "seven TV commercials",
        "unfavorable TV commercials",
        "cognitive neurophysiological indices",
        "consecutive advertise- ments",
        "experimental setting",
        "conventional product",
        "14 TV commercials",
        "100 TV commercials",
        "different brands",
        "ral indices",
        "Nestle advertisement",
        "television commercials",
        "consecutive advertisements",
        "TV advertisements",
        "equal price",
        "short movies",
        "key focus",
        "higher rate",
        "ful promotions",
        "memory rate",
        "time distance",
        "neutral stimuli",
        "white screen",
        "green scenario",
        "electrical activity",
        "advanced algorithms",
        "emotional changes",
        "cerebral activity",
        "strong images",
        "pulse analysis",
        "print media",
        "animated commercial",
        "verbal narrative",
        "ing commercial",
        "visual narrative",
        "graduate students",
        "class rank",
        "emotional states",
        "visual attentions",
        "lit- eratures",
        "standard time",
        "video advertisements",
        "neural response",
        "consumer engagement",
        "audience brain",
        "web page",
        "user preference",
        "audiences’ taste",
        "user attention",
        "favorable advertisements",
        "static advertisements",
        "50 commercials",
        "communication",
        "Neuromarketers",
        "air",
        "extract",
        "happiness",
        "surprise",
        "10 s",
        "participants",
        "observation",
        "Krugman",
        "success",
        "neuroimaging",
        "gies",
        "TVC",
        "30 s",
        "other",
        "videos",
        "social adver- tisements",
        "social advertisement stimuli",
        "social groups",
        "documentary film",
        "gaming video",
        "true response",
        "different purposes",
        "gender-related advertisements",
        "appli- cation",
        "cigarette commercials",
        "smoking cessa",
        "atten- tion",
        "substantial role",
        "advertisement industry",
        "gender-targeted marketing",
        "tion frames",
        "drama",
        "Neuromarketing",
        "The",
        "ads",
        "messages",
        "smokers",
        "celebrity",
        "endorsement",
        "Missaglia",
        "research"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 9.010597,
      "content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchical c",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1012456,
      "metadata_storage_name": "s40493-015-0015-3.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxNS0zLnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-19T12:42:36Z",
      "keyphrases": [
        "Creative Commons Attribution License",
        "mobile ad hoc networks",
        "Günther Pernul",
        "Universitätsstraße",
        "descriptive scenario-based analysis",
        "general prob- lem",
        "one novel idea",
        "Open Access article",
        "single building blocks",
        "RESEARCH Open Access",
        "online reputation systems",
        "new reputation systems",
        "hierarchical component taxonomy",
        "most reputation systems",
        "common reputation models",
        "Johannes Sänger",
        "hierarchical taxonomy",
        "common systems",
        "peer networks",
        "social networks",
        "component repository",
        "common models",
        "Christian Richthammer",
        "various disciplines",
        "application areas",
        "promising approaches",
        "implementation level",
        "obvious utility",
        "practical point",
        "last decade",
        "wide range",
        "considerable research",
        "data accuracy",
        "several environments",
        "statistical approaches",
        "personal preferences",
        "software engineering",
        "unrestricted use",
        "effective use",
        "computation engines",
        "computation phase",
        "graph-based models",
        "natural framework",
        "design process",
        "design knowledge",
        "Design approaches",
        "computation methods",
        "Reusable components",
        "Trust Management",
        "Trust pattern",
        "reputation-based trust",
        "context information",
        "original work",
        "Journal",
        "DOI",
        "Correspondence",
        "saenger",
        "wiwi",
        "regensburg",
        "University",
        "Germany",
        "Abstract",
        "problem",
        "scratch",
        "concepts",
        "achievements",
        "others",
        "shuffle",
        "reuse",
        "respect",
        "order",
        "conceptual",
        "view",
        "properties",
        "literature",
        "aspects",
        "Keywords",
        "Reusability",
        "Introduction",
        "metrics",
        "commerce",
        "eBay",
        "buyers",
        "relevance",
        "quality",
        "arithmetic",
        "multiple",
        "factors",
        "propagation",
        "solutions",
        "authors",
        "proposals",
        "development",
        "attention",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "mailto",
        "Page",
        "community",
        "efits",
        "specialists",
        "reliability",
        "goal",
        "classification",
        "functions",
        "many applied computation techniques",
        "design science research paradigm",
        "design pattern-like solution",
        "rational choice mechanism",
        "general problem context",
        "multifaceted terms trust",
        "computational trust models",
        "current models",
        "reputation models",
        "building block",
        "web services",
        "research gap",
        "objec- tives",
        "same time",
        "increasing distribution",
        "decision making",
        "network environments",
        "common understanding",
        "various fields",
        "multidimension- ality",
        "abstract concept",
        "behavioral dimension",
        "interpersonal phenomenon",
        "particular level",
        "subjective probability",
        "Sänger",
        "Multiple authors",
        "plex definitions",
        "reputation systems",
        "conceptual level",
        "following section",
        "important artifacts",
        "bility trust",
        "particular action",
        "future work",
        "several properties",
        "means",
        "rest",
        "paper",
        "guidelines",
        "Hevner",
        "overview",
        "motivation",
        "approach",
        "contribution",
        "name",
        "plans",
        "success",
        "Internet",
        "connectivity",
        "notion",
        "regard",
        "objectives",
        "topic",
        "decades",
        "uniform",
        "Reasons",
        "circumstance",
        "credibility",
        "confidence",
        "emotional",
        "nature",
        "sociologists",
        "psychologists",
        "Economists",
        "online",
        "Gambetta",
        "distrust",
        "agent",
        "group",
        "capacity",
        "security",
        "risk",
        "variety",
        "Table",
        "basis",
        "two common ways",
        "multiple discrete levels",
        "hard security mechanism",
        "One negative experience",
        "Propagative One property",
        "one aggregated picture",
        "many trust models",
        "multiple trust chains",
        "several trust models",
        "several models",
        "propagative nature",
        "aggregated value",
        "different chains",
        "recent years",
        "new experiences",
        "time-based aging",
        "greater importance",
        "old experiences",
        "delicious meal",
        "rela- tionships",
        "final decision",
        "greater influence",
        "distance-based aging",
        "subjective nature",
        "high level",
        "binary manner",
        "maximum value",
        "human nature",
        "hard evidence",
        "one agent",
        "other agents",
        "untrustworthy agents",
        "Dynamic Trust",
        "Context-dependent Trust",
        "trust value",
        "overall trust",
        "trust information",
        "trust statements",
        "lower trust",
        "Reflexive Trust",
        "specific context",
        "particular restaurant",
        "specific aspects",
        "long time",
        "particular agent",
        "distant nodes",
        "reviewer Rachel",
        "book review",
        "trust transitivity",
        "Policy-based trust",
        "same context",
        "values",
        "Table 1",
        "Overview",
        "acteristics",
        "example",
        "Alice",
        "Bob",
        "doctor",
        "cook",
        "customer",
        "food",
        "service",
        "combination",
        "amount",
        "use",
        "propagativity",
        "turn",
        "Claire",
        "Christianson",
        "Harbison",
        "transitive",
        "reverse",
        "case",
        "Composable",
        "Composition",
        "zon",
        "opinion",
        "numbers",
        "continuous",
        "variable",
        "interval",
        "reinforcing",
        "trustworthiness",
        "consideration",
        "establishment",
        "exchange",
        "credentials",
        "contrast",
        "history",
        "interactions",
        "estimation",
        "three process steps filtering",
        "most reputation-based trust models",
        "three fundamental phases",
        "repu- tation system",
        "function-based component classification",
        "current reputation models",
        "existing trust models",
        "three steps",
        "existing models",
        "component classes",
        "generic process",
        "two steps",
        "existing approaches",
        "existing systems",
        "soft security",
        "collective measure",
        "global value",
        "personal experience",
        "many approvedmodels",
        "promising thoughts",
        "generic mechanism",
        "compo- nent",
        "accepted principles",
        "one hand",
        "other hand",
        "critical question",
        "communica- tion",
        "feedback generation/collection",
        "feedback distribution",
        "central part",
        "common trust",
        "feedback aggregation",
        "existing concepts",
        "academic community",
        "context setting",
        "single components",
        "reusable components",
        "trust properties",
        "Research gap",
        "worthiness",
        "work",
        "thing",
        "character",
        "referrals",
        "ratings",
        "reviews",
        "members",
        "someone",
        "ideas",
        "benefits",
        "years",
        "Rehak",
        "instance",
        "capabilities",
        "sound",
        "new",
        "repository",
        "artifacts",
        "developers",
        "researchers",
        "novel",
        "way",
        "implemented",
        "communication",
        "ability",
        "table",
        "computationmethods",
        "analysis",
        "identification",
        "logical",
        "Figure",
        "preparation",
        "storage",
        "weighting",
        "trustor",
        "novel hierarchical component taxonomy",
        "common rep- utation systems",
        "three generic process steps",
        "two broad categories",
        "reputa- tion systems",
        "subsequent aggre- gation",
        "current trust models",
        "second process step",
        "single process steps",
        "Common reputation systems",
        "first two steps",
        "several reputation scores",
        "common examples",
        "subsequent computing",
        "utation information",
        "second case",
        "computation process",
        "trust relation",
        "first phase",
        "reputation value",
        "personalization parameters",
        "past behavior",
        "sonal experience",
        "other sources",
        "personal collections",
        "ferent peers",
        "distributed network",
        "different sources",
        "uniform format",
        "specific situation",
        "weight factors",
        "struc- ture",
        "decentralized environments",
        "functional blocks",
        "different surveys",
        "computation engine",
        "last phase",
        "first question",
        "filtering process",
        "hard selection",
        "soft selection",
        "secondary classes",
        "reputation data",
        "information amount",
        "extra information",
        "available data",
        "Preparation techniques",
        "preparation phase",
        "Figure 2 Classes",
        "public storage",
        "input data",
        "trustee",
        "context",
        "normalization",
        "output",
        "aggregation",
        "need",
        "processing",
        "line",
        "Zhang",
        "difference",
        "algo",
        "rithm",
        "structure",
        "decentralized/hybrid",
        "meaning",
        "transparency",
        "design",
        "section",
        "detail",
        "primary",
        "three broad classes attribute-based, statistic- based",
        "simple ballot stuffing attacks",
        "payment Sänger",
        "statistical filter technique",
        "one application area",
        "value imbalance problem",
        "Bayesian reputation systems",
        "cluster analysis approaches",
        "following classes",
        "Attribute-based filtering",
        "Attribute-based filters",
        "simple logic",
        "statistical patterns",
        "Clustering-based filter",
        "filtering step",
        "Other models",
        "filtering phase",
        "single attributes",
        "reference value",
        "huge amounts",
        "initial filtering",
        "weighting phase",
        "last 12 months",
        "positive, neutral",
        "Statistic-based filtering",
        "false rumors",
        "exemplary procedure",
        "cluster filtering",
        "available information",
        "current situation",
        "various data",
        "different factors",
        "different prices",
        "product types",
        "malicious seller",
        "cheap products",
        "many current",
        "reputation calculation",
        "Reputation data",
        "high reputation",
        "negative ratings",
        "positive ratings",
        "unfair ratings",
        "false ratings",
        "Further techniques",
        "majority rule",
        "filtering techniques",
        "weighting techniques",
        "false feedback",
        "other parties",
        "common experience",
        "fake transactions",
        "obvious way",
        "crucial attribute",
        "Context comparability",
        "constraint-factor",
        "lightweight",
        "Time",
        "Sporas",
        "party",
        "robustness",
        "spread",
        "Whitby",
        "dishonest",
        "advisor",
        "rater",
        "iCLUB",
        "clusters",
        "evaluations",
        "bootstrapping",
        "Dellarocas",
        "characteristics",
        "reason",
        "discounting",
        "methods",
        "non",
        "expensive",
        "situations",
        "1.",
        "several trust/reputa- tion values",
        "biometric identity trust model",
        "human decision makers",
        "adaptive forgetting scheme",
        "distinct information sources",
        "common aggregation techniques",
        "one important factor",
        "different application areas",
        "simple aggregation techniques",
        "low feedback reputation",
        "reputation values",
        "forgetting factor",
        "low impact",
        "Simple arithmetic",
        "critical factor",
        "first-hand information",
        "Different actors",
        "different perceptions",
        "aggregation process",
        "aggregated reputation",
        "good reputation",
        "reputation scoring",
        "Reputation systems",
        "file-sharing networks",
        "successful transaction",
        "customer satisfaction",
        "network structures",
        "transitivity) rate",
        "honest feedback",
        "recursive algorithm",
        "stronger punishment",
        "bad behavior",
        "good behavior",
        "dynamic nature",
        "Old feedback",
        "Jøsang",
        "Personal preferences",
        "various end-users",
        "direct experience",
        "weight-factor(s",
        "third step",
        "iterative manner",
        "single steps",
        "first class",
        "Criteria comparability",
        "different communities",
        "Rating value",
        "trust chain",
        "Other distinctions",
        "propagation degree",
        "web graph",
        "example measure",
        "time-based weighting",
        "new referrals",
        "agents’ reliability",
        "approaches",
        "evaluation",
        "costs",
        "level",
        "anonymity",
        "number",
        "peers",
        "differences",
        "recommendation",
        "second",
        "distance",
        "reputation-factor",
        "nodes",
        "honesty",
        "concept",
        "consequence",
        "bases",
        "calculation",
        "Google",
        "PageRank",
        "position",
        "website",
        "Epinions",
        "reviewers",
        "effects",
        "Sun",
        "adaptation",
        "importance",
        "significance",
        "newcomers",
        "tuple",
        "input",
        "phase",
        "cases",
        "list",
        "algorithms",
        "fuzzy",
        "2.",
        "4.",
        "5.",
        "beta PDF function parameter tuple",
        "human decision making process",
        "beta probability density function",
        "solid mathematical basis",
        "More complex solutions",
        "The beta PDF",
        "Beta Reputation system",
        "exact reputation score",
        "priori reputation score",
        "online social networks",
        "Other aggregation techniques",
        "Bayesian probabilistic models",
        "Hidden Markov Model",
        "graph-based flow model",
        "prominent trust models",
        "beta model",
        "Bayesian probability",
        "probability distributions",
        "probabilistic approach",
        "Markov Models",
        "Applied techniques",
        "graph-based approach",
        "belief model",
        "model truth",
        "Reputation values",
        "overall reputation",
        "basic way",
        "descending order",
        "message boards",
        "citation counts",
        "academic literature",
        "understandable algorithm",
        "last years",
        "statistical approach",
        "binary events",
        "Dempster-Shafer theory",
        "subjective logic",
        "machine learning",
        "dynamic behavior",
        "classical logic",
        "linguistic approach",
        "prominent example",
        "graph theory",
        "incoming edges",
        "outgoing edges",
        "several factors",
        "Popular algorithms",
        "Eigentrust Algorithm",
        "many models",
        "fuzzy logic",
        "trust management",
        "transitive trust",
        "trust chains",
        "fuzzy models",
        "new ratings",
        "Other examples",
        "ranking algorithms",
        "impact factor",
        "different measures",
        "one edge",
        "two nodes",
        "summation",
        "average",
        "proxy",
        "systems",
        "Slashdot",
        "implementation",
        "Amazon",
        "positive",
        "composition",
        "result",
        "weakness",
        "uncertainty",
        "DST",
        "Yu",
        "Singh",
        "generalization",
        "Malik",
        "falsity",
        "degree",
        "agent/resource",
        "REGRET",
        "information",
        "people",
        "field",
        "centrality",
        "Eigenvector",
        "betweenness",
        "α",
        "β",
        "reputa- tion system",
        "single logical components",
        "aggregation phases",
        "incremental nature",
        "one component",
        "primary class",
        "aggregation techniques",
        "hybrid model",
        "new classes",
        "novel models",
        "hierarchical c",
        "taxonomy",
        "heuristic",
        "knowledge",
        "extension"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 8.577549,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-s",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 548509,
      "metadata_storage_name": "s13640-020-00545-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMjAtMDA1NDUtei5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "metadata_creation_date": "2021-01-12T23:22:39Z",
      "keyphrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "doi.org",
        "orcid.org",
        "commerce platforms",
        "Correspondence",
        "shaoshuaib",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "BERT",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "creativecommons",
        "licenses",
        "crossmark",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "BERT model",
        "Related work",
        "present stage",
        "good results",
        "extraction capacity",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social information Preprocessing",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "Background processing",
        "subsequent processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "characteristic dimension",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "Data label",
        "tf matrix",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "dividend",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "BERT-Base",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "Text classification BERT fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "Text classification fine-tuning",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "official recommended values",
        "feature matching scheme",
        "text information token",
        "full connection layer",
        "additional 9500 text data",
        "natural language processing",
        "machine learning scheme",
        "word segmentation process",
        "social information data",
        "5.2 Classification scheme",
        "text length",
        "text description",
        "input BERT",
        "first token",
        "TF-IDF model",
        "classification problem",
        "deep learning",
        "learning rate",
        "TF-IDF-based model",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "commodities",
        "social e-commerce market",
        "standard description text",
        "social e-commerce environment",
        "knowledge distillation technology",
        "standard product names",
        "large-scale data marking",
        "social e-commerce classification",
        "model recognition rate",
        "product information",
        "test data",
        "text-based classification",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "high correlation",
        "work",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "semi"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 7.504171,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relatives",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2986829,
      "metadata_storage_name": "s40493-015-0019-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ5My0wMTUtMDAxOS16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Partheeban Chandrasekaran",
      "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "metadata_creation_date": "2015-09-04T09:59:41Z",
      "keyphrases": [
        "new classi- fication scheme",
        "peer file-sharing applications",
        "social networking websites",
        "comprehensive test tool",
        "Attribution 4.0 International License",
        "original author(s",
        "three trust algorithms",
        "various trust properties",
        "computational trust mechanisms",
        "online community-based systems",
        "Creative Commons license",
        "computational trust models",
        "social trust models",
        "three reputation systems",
        "different trust models",
        "RESEARCH Open Access",
        "trust assessment workflow",
        "existing reputation systems",
        "existing models",
        "Many models",
        "different stages",
        "Trust Management",
        "trust relationship",
        "Multi-agent systems",
        "Computer Engineering",
        "Introduction Motivation",
        "simple averaging",
        "based scores",
        "Advogato website",
        "application-dependent metrics",
        "transformation stage",
        "unrestricted use",
        "appropriate credit",
        "first contribution",
        "second contribution",
        "evaluation schemes",
        "Trust testbed",
        "Esfandiari Journal",
        "latest model",
        "single model",
        "generic testbed",
        "increasing need",
        "past transactions",
        "graph transformations",
        "Carleton University",
        "larger number",
        "Partheeban Chandrasekaran",
        "slandering attacks",
        "two agents",
        "Babak Esfandiari",
        "DOI",
        "experiments",
        "analysis",
        "Correspondence",
        "Department",
        "1125 Colonel",
        "Drive",
        "Ottawa",
        "Ontario",
        "Canada",
        "Abstract",
        "tesbed",
        "flexibility",
        "design",
        "prototype",
        "EigenTrust",
        "PeerTrust",
        "Appleseed",
        "vulnerabilites",
        "compliance",
        "example",
        "discrepancies",
        "trade-offs",
        "resistance",
        "self-promotion",
        "Keywords",
        "growth",
        "users",
        "history",
        "interactions",
        "calculations",
        "ratings",
        "eBay",
        "researcher",
        "tools",
        "evaluations",
        "set",
        "vulnerabilities",
        "types",
        "Overview",
        "solution",
        "contributions",
        "paper",
        "family",
        "process",
        "succession",
        "vertices",
        "meaning",
        "edges",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "crossmark",
        "org",
        "mailto",
        "Page",
        "presence",
        "existence",
        "variety",
        "certificate authority-based PKI trust models",
        "Trust management systems aid agents",
        "reputation-based trust management systems",
        "three different trust algorithms",
        "public key infras",
        "Social trust models",
        "basic trust properties",
        "social trust algorithms",
        "existing trust algorithms",
        "Beta Reputation System",
        "three systems",
        "reputation systems",
        "chaining algorithms",
        "mutual trust",
        "trust assertions",
        "trust score",
        "existing testbeds",
        "subtle interplay",
        "higher sensitivity",
        "lower sensitivity",
        "literature review",
        "implementation details",
        "actual mechanisms",
        "earlier direct",
        "indirect interactions",
        "e-commerce website",
        "third party",
        "Various inputs",
        "file-sharing network",
        "blogging website",
        "particular level",
        "specific context",
        "simple attacks",
        "self-promoting attacks",
        "Problem description",
        "research problem",
        "particular agent",
        "one attack",
        "friend chain",
        "past experiences",
        "testbed prototype",
        "aggregated trustworthiness",
        "evaluation results",
        "range",
        "differences",
        "way",
        "slandering",
        "Organization",
        "section",
        "Background",
        "state",
        "discussion",
        "Conclusions",
        "limitations",
        "tructures",
        "certificates",
        "The",
        "means",
        "order",
        "instance",
        "sellers",
        "trustworthy",
        "product",
        "drawbacks",
        "Chandrasekaran",
        "root",
        "recommendations",
        "turn",
        "recommenders",
        "Nature",
        "TRAVOS",
        "BRS",
        "satisfaction",
        "transaction",
        "P2P",
        "Aberer",
        "Despotovic",
        "plaints",
        "Advogato",
        "goal",
        "spam",
        "community",
        "TidalTrust",
        "beta probability density function",
        "late indirect trust scores",
        "other agents’ past experiences",
        "Spreading Activation model",
        "local trust scores",
        "global trust algorithms",
        "global trust score",
        "local trust algorithms",
        "three algorithms",
        "general trust",
        "Eigen- Trust",
        "Trust decisions",
        "similarity score",
        "file authenticity",
        "recommenda- tions",
        "different inputs",
        "thresholding techniques",
        "edge weights",
        "maximum “flow",
        "direct experience",
        "party recommendations",
        "different families",
        "next sections",
        "detailed descriptions",
        "respective sections",
        "satisfaction ratings",
        "trust graph",
        "direct interactions",
        "truster agent",
        "Credence",
        "votes",
        "files",
        "trustee",
        "gossiping",
        "information",
        "perspective",
        "survey",
        "Marsh",
        "Abdul-Rahman",
        "case",
        "percentage",
        "top",
        "value",
        "energy",
        "tor",
        "short",
        "third",
        "one",
        "scope",
        "testbed",
        "details",
        "output",
        "readers",
        "high local trust values",
        "low local trust values",
        "local direct trust score",
        "normalized local trust score",
        "global trust score computations",
        "current trust scores",
        "trans- action ratings",
        "local trust vector",
        "global trust scores",
        "fellow malicious agents",
        "gence threshold Tc",
        "global value",
        "trust relationships",
        "trust propagation",
        "trust graphs",
        "flow-based algorithm",
        "trust seed",
        "path length",
        "termination condition",
        "weighted graph",
        "Tij trij",
        "cij elements",
        "factor decay",
        "decay factor",
        "honest agents",
        "agent u",
        "agent j",
        "satisfaction rating",
        "transaction rating",
        "raters",
        "Eq.",
        "transactions",
        "system",
        "method",
        "input",
        "sij",
        "sum",
        "truster",
        "friends",
        "cijcjk",
        "matrix",
        "tik",
        "opinion",
        "number",
        "hops",
        "i.",
        "rank",
        "nodes",
        "trustworthiness",
        "sink",
        "amount",
        "places",
        "loops",
        "Ai",
        "step",
        "∑",
        "∈",
        "open-source message- driven simulation engine",
        "two different targets",
        "two testbed implementations",
        "document recommendation systems",
        "beta probability distribution",
        "local trust algorithm",
        "reputation estimation errors",
        "two testbed models",
        "general-purpose trust models",
        "previous reputation value",
        "art painting sales",
        "general trust systems",
        "The Agent Reputation",
        "beta distribution",
        "two roles",
        "initial value",
        "real systems",
        "trust assessment",
        "brief survey",
        "important role",
        "case studies",
        "direct feedbacks",
        "current reputation",
        "single fixpoint",
        "important stage",
        "one trustworthiness",
        "dom variables",
        "various attacks",
        "particular era",
        "market values",
        "specific era",
        "other eras",
        "reputation scores",
        "trust algorithms",
        "next section",
        "rating function",
        "desirable properties",
        "negative experiences",
        "mutual ratings",
        "target pairing",
        "other agents",
        "Guha Guha",
        "seed",
        "methods",
        "look",
        "andMacau",
        "TREET",
        "investigation",
        "graph",
        "documents",
        "many",
        "specialization",
        "subclass",
        "Hazard",
        "Singh",
        "authors",
        "rater",
        "favor",
        "payoff",
        "belief",
        "likelihood",
        "future",
        "definitions",
        "Monotonicity",
        "computed",
        "b.",
        "Unambiguity",
        "convergence",
        "time",
        "Accuracy",
        "total",
        "update",
        "positive",
        "comparison",
        "resilience",
        "performance",
        "domain",
        "client",
        "paintings",
        "appraisals",
        "expert",
        "dedicated distributed infras- tructure",
        "online product review websites",
        "trust score rep- resentation",
        "graphical user interface",
        "first person point",
        "P2P file-sharing networks",
        "honest sales ratio",
        "trust calculation schemes",
        "decentralized trust algorithms",
        "highest bank balance",
        "Value Imbalance attack",
        "general marketplace scenario",
        "various reputation systems",
        "Reputation Lag attack",
        "honest profit ratio",
        "multiple buyer accounts",
        "Multiple seller accounts",
        "The ART testbed",
        "distributed version",
        "utility value",
        "Proliferation attack",
        "cheater sales",
        "other systems",
        "marketplace domain",
        "Reputation Experimentation",
        "trust mechanism",
        "trust management",
        "The Trust",
        "future interactions",
        "possible messages",
        "time interval",
        "The engine",
        "new clients",
        "correct ability",
        "problem domain",
        "Evaluation Testbed",
        "varying prices",
        "inexpensive items",
        "market competition",
        "selling price",
        "future transactions",
        "following metrics",
        "evaluation metrics",
        "simulation engine",
        "simulation days",
        "trustworthy agents",
        "right agents",
        "participating agents",
        "different agents",
        "centralized version",
        "sale price",
        "random number",
        "collusion attacks",
        "winning agent",
        "1,000 different products",
        "departing agent",
        "protocol",
        "Themessages",
        "track",
        "results",
        "database",
        "GUI",
        "runtime",
        "platform",
        "service",
        "cooperation",
        "additional",
        "model",
        "buyers",
        "influence",
        "cost",
        "purchase",
        "probability",
        "initialization",
        "offers",
        "feedback",
        "experience",
        "White-Washing",
        "Self-Promoting",
        "limitation",
        "restriction",
        "others",
        "knowledge",
        "100",
        "many trust management systems",
        "obtain feedback history graph",
        "discrete event simulator",
        "generic evaluation metrics",
        "particular trust algorithm",
        "A Reputation Graph",
        "trust assessment process",
        "Existing trust models",
        "new systems",
        "decision-making process",
        "evaluation framework",
        "labelled graph",
        "agent A",
        "Summary Trust",
        "social trust",
        "trust properties",
        "nth-hand trust",
        "application domain",
        "rational decisions",
        "abstraction layer",
        "n feedbacks",
        "ith satisfaction",
        "different ranges",
        "labelled arcs",
        "other things",
        "next step",
        "transitive closure",
        "target agent",
        "agent B",
        "indirect feedback",
        "feedback histories",
        "following stages",
        "past behavior",
        "R N",
        "common set",
        "final stage",
        "abstract model",
        "feedback value",
        "output requirements",
        "R.",
        "tool",
        "manymodels",
        "attacks",
        "andmodel",
        "developers",
        "foundation",
        "expectation",
        "individual",
        "observations",
        "estimation",
        "combination",
        "rest",
        "graphs",
        "group",
        "actions",
        "list",
        "FHG",
        "downloader",
        "uploader",
        "values",
        "timestamps",
        "directed",
        "second",
        "E.",
        "∅",
        "single reputa- tion score",
        "absolute global reputation score",
        "new classification scheme",
        "continuous reputation values",
        "global reputation scores",
        "existing classification criteria",
        "stage transi- tions",
        "top k agents",
        "local reputation scores",
        "obtain trust graph",
        "Global algorithms",
        "existing literature",
        "one scores",
        "Reputation algorithms",
        "particular task",
        "two groups",
        "clar- ity",
        "Two methods",
        "trust models",
        "above sections",
        "Apple- seed",
        "other hand",
        "same codomain",
        "other words",
        "reputation graph",
        "various algorithms",
        "different algorithms",
        "edge labels",
        "local algorithms",
        "other method",
        "intermediate graph",
        "reflexive property",
        "incoming arcs",
        "among agents",
        "ranking algorithms",
        "One method",
        "trusting agent",
        "EigenTrust outputs",
        "one stage",
        "looping",
        "degree",
        "Figs",
        "weights",
        "Fig.",
        "Examples",
        "decision",
        "TG",
        "ourmodel",
        "stages",
        "workflow",
        "Classifying",
        "RG",
        "nisms",
        "threshold",
        "transitions",
        "addition",
        "Table",
        "semantics",
        "relative",
        "Trust Algorithm Transitions Input",
        "satisfaction local absoluteratings Ranking",
        "continuous feedback values",
        "trust models Stage",
        "graph transformation function",
        "Stage transitions",
        "global absoluteratings",
        "local absolutescores",
        "Trust algorithms",
        "functional view",
        "evaluation mechanisms",
        "global relativeratings",
        "N/A relatives",
        "Reputation Scores",
        "experimenter",
        "inputs",
        "outputs",
        "classification",
        "AppleSeed",
        "3 complaints",
        "3 certificates"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 7.0367537,
      "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\n\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\n\nn\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C)−H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN (AD − CB)2\n\n(A+ C)(B+ D)(A+ B)(C + D)\n\n(5)χ2(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf\n+ is the number of documents in the positive class that contain feature f, DFf\n\n− \nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and mf, mc\n\nf are \nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc −mf\n\n)2\n\n(8)mc =\n\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1− qf\n)\n\n(\n\n1− pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf\n+ and DFf\n\n− are the raw counts of documents that contain f in the positive and \nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs. 12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5 GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table 2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nTu\nrk\n\nis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\no",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1280780,
      "metadata_storage_name": "s13673-018-0135-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTgtMDEzNS04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tuba Parlar ",
      "metadata_title": "QER: a new feature selection method for sentiment analysis",
      "metadata_creation_date": "2018-04-18T08:33:56Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "naïve Bayes multinomial classifier",
        "Selma Ayşe Özel2",
        "natural language processing task",
        "Text classification Open Access",
        "document frequency difference methods",
        "new feature selection method",
        "Creative Commons license",
        "other feature selection methods",
        "query expansion ranking",
        "query expansion term",
        "optimal orthogonal centroid",
        "four classi- fiers",
        "support vector machines",
        "third class label",
        "decision making process",
        "two-class classification problem",
        "original author(s",
        "Internet World Stats",
        "review docu- ments",
        "Hum. Cent. Comput",
        "statistical methods",
        "learning methods",
        "senti- ments",
        "decision trees",
        "classification accuracy",
        "sentiment classification",
        "author information",
        "Tuba Parlar1",
        "Fei Song3",
        "important piece",
        "social media",
        "top 20 countries",
        "highest numbers",
        "Internet users",
        "exploding growth",
        "main reasons",
        "low ranks",
        "multiple kinds",
        "product reviews",
        "Machine learning",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Parlar",
        "Inf. Sci.",
        "Kemal University",
        "Full list",
        "ferent approaches",
        "data preprocessing",
        "other people",
        "The Author",
        "classification performance",
        "opinionated information",
        "information retrieval",
        "Chi square",
        "information gain",
        "sentiment analysis",
        "ent languages",
        "noisy features",
        "doi.org",
        "orcid.org",
        "irrelevant features",
        "major source",
        "online documents",
        "many researchers",
        "different domains",
        "Turkish reviews",
        "English languages",
        "QER",
        "Introduction",
        "most",
        "opinions",
        "Turkey",
        "area",
        "SA",
        "general",
        "internetworldstats",
        "Abstract",
        "sentiments",
        "order",
        "non-informative",
        "study",
        "field",
        "ling",
        "movie",
        "performances",
        "classifiers",
        "results",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "tparlar",
        "mku",
        "1 Department",
        "Mathematics",
        "Mustafa",
        "Antakya",
        "Hatay",
        "end",
        "crossmark",
        "crossref",
        "Page",
        "19Parlar",
        "studies",
        "query expansion term weighting methods",
        "other common feature selection methods",
        "ten different review documents datasets",
        "maximum entropy mod- elling",
        "naïve Bayes multinomial",
        "traditional topic-based text classification",
        "good feature selection",
        "information retrieval systems",
        "Natural Language Processing",
        "machine learning methods",
        "text classification task",
        "document frequency difference",
        "machine learning approaches",
        "four text classifiers",
        "kitchen appliances reviews",
        "different document domains",
        "Related work SA",
        "feature space",
        "feature sizes",
        "training datasets",
        "different reviews",
        "classification accu",
        "particular review",
        "related attributes",
        "different languages",
        "LCD screen",
        "sentiment-expressing words",
        "experimental settings",
        "important topic",
        "Artificial Intelligence",
        "opinion mining",
        "useful applications",
        "negative ratings",
        "Hum. Cent",
        "popular implementation",
        "movie reviews",
        "book reviews",
        "search performance",
        "performance measures",
        "testing results",
        "Numerous studies",
        "SA problems",
        "overall sentiments",
        "non-informative features",
        "great pictures",
        "NB",
        "SVM",
        "DT",
        "MEM",
        "differences",
        "example",
        "camera",
        "people",
        "breath",
        "observation",
        "account",
        "paper",
        "dimensionality",
        "aim",
        "texts",
        "Turkish",
        "English",
        "effectiveness",
        "DFD",
        "OCFS",
        "goal",
        "racy",
        "respect",
        "rest",
        "Experiments",
        "Conclusion",
        "evalua",
        "emotions",
        "entities",
        "products",
        "services",
        "organizations",
        "als",
        "issues",
        "events",
        "kind",
        "many",
        "popularity",
        "user",
        "companies",
        "businesses",
        "Comput",
        "models",
        "cally",
        "Pang",
        "researchers",
        "effects",
        "Yang",
        "Yu",
        "C4.5",
        "Tabu heuristic search-enhanced Markov blanket model",
        "karşılaştırmalısın",
        "information gain feature selection method",
        "document frequency feature selection method",
        "minimum redundancy maximum relevancy",
        "Weighted Log Likelihood Ratio",
        "Weka [33] data mining tool",
        "DFD feature selection method",
        "reduced feature sizes",
        "Machine learning algorithms",
        "product review dataset",
        "Chinese online reviews",
        "movie review dataset",
        "The Turkish language",
        "feature sets",
        "Boynukalın",
        "n words",
        "lish reviews",
        "product datasets",
        "execution time",
        "large number",
        "comparative experiments",
        "significant improvement",
        "Altaic branch",
        "Ural-Altaic family",
        "agglutinative language",
        "longer sentence",
        "ture space",
        "emotional analysis",
        "initial version",
        "DT classifiers",
        "statistical analysis",
        "IG methods",
        "word relation",
        "sentiment features",
        "related features",
        "unigram features",
        "single word",
        "one classifier",
        "SVM classifiers",
        "two types",
        "high accuracy",
        "95% average accuracy",
        "English language",
        "Complementary NB",
        "MEM classifier",
        "English datasets",
        "SA studies",
        "Nicholls",
        "difference",
        "Agarwal",
        "mRMR",
        "NBM",
        "Abbasi",
        "EWGA",
        "Xia",
        "design",
        "Bai",
        "vocabulary",
        "Mladenovic",
        "mapping",
        "Zheng",
        "languages",
        "Republic",
        "Finnish",
        "garian",
        "instance",
        "something",
        "teristics",
        "WLLR",
        "combinations",
        "n-grams",
        "sequences",
        "Akba",
        "CHI2",
        "85.",
        "92",
        "optimal orthogonal cen- troid",
        "new query expansion ranking",
        "traditional text classification tasks",
        "several feature selection methods",
        "common feature selection methods",
        "high information gain scores",
        "follow- ing reasons",
        "decision tree classifier",
        "Turkish review datasets",
        "English review datasets",
        "good classification performance",
        "large vocabulary sizes",
        "many classification problems",
        "J48 classification methods",
        "total feature sizes",
        "Chi square score",
        "Such methods",
        "higher score",
        "low score",
        "feature f",
        "four classifiers",
        "tain measures",
        "same time",
        "effec- tiveness",
        "distinct values",
        "same way",
        "contingency table",
        "total number",
        "related class",
        "class c",
        "other class",
        "many documents",
        "valuable features",
        "previous study",
        "different classes",
        "P(aj",
        "m number",
        "probability distribution",
        "entropy values",
        "attribute value",
        "experiments",
        "SMO",
        "LR",
        "specific",
        "efficiency",
        "range",
        "500 increments",
        "improvement",
        "content",
        "power",
        "tainty",
        "Hum",
        "Cent",
        "definitions",
        "reviews",
        "positive",
        "egories",
        "keyword",
        "formulas",
        "dependence",
        "3000",
        "The Chi square statistics",
        "Query expansion ranking method",
        "probabilistic weighting model",
        "similar document frequencies",
        "high dimensional vectors",
        "Document frequency difference",
        "query expansion methods",
        "query expansion technique",
        "Optimal orthogonal centroid",
        "orthogonal centroid algorithm",
        "many sentiment-expressing features",
        "one specific class",
        "document vectors",
        "CHI2 method",
        "bigger difference",
        "document collection",
        "One problem",
        "One limitation",
        "new terms",
        "original query",
        "expanded query",
        "big impact",
        "H(C",
        "+ D",
        "big issue",
        "sentiment word",
        "optimized form",
        "Docu- ments",
        "age vector",
        "DFD method",
        "high scores",
        "centroid m",
        "OCFS method",
        "negative class",
        "rare features",
        "individual review",
        "xi N",
        "positive class",
        "highest scores",
        "Nc N",
        "relevant documents",
        "equal number",
        "classes",
        "dataset",
        "text",
        "|A",
        "aj",
        "CB",
        "classification",
        "Song",
        "DFf",
        "weights",
        "importance",
        "subset",
        "sum",
        "distances",
        "subspace",
        "mc",
        "mf",
        "values",
        "centroids",
        "IR",
        "Scoref",
        "Harman",
        "∑",
        "χ",
        "tional topic-based text classification",
        "QER feature selection method",
        "information retrieval strategy",
        "statistical language processing",
        "informa- tion gain",
        "feature selection methods",
        "efficient feature selector",
        "information retrieval system",
        "score computation method",
        "k new terms",
        "classification process",
        "score formula",
        "ranking score",
        "k terms",
        "high scored",
        "Sparck Jones",
        "raw counts",
        "negative classes",
        "small constants",
        "similar constants",
        "data smoothing",
        "fea- ture",
        "multiple classes",
        "two classes",
        "greedy approach",
        "related classes",
        "high values",
        "distance/similarity measures",
        "document frequencies",
        "Q. Equation",
        "term f",
        "negative documents",
        "related documents",
        "valuable terms",
        "top-k scores",
        "ability scores",
        "compute scores",
        "lowest scores",
        "zero probabilities",
        "query Q",
        "probability calculations",
        "sentiment-expressing features",
        "positive documents",
        "precision",
        "man",
        "set",
        "pf",
        "qf",
        "Robertson",
        "Eq.",
        "ratio",
        "Eqs",
        "log2",
        "DF",
        "N−",
        "numbers",
        "numerators",
        "denominators",
        "CHI",
        "collection",
        "vectors",
        "review",
        "introduction",
        "1−",
        "2.5 GHz Intel Core i7 processor",
        "The English movie review dataset",
        "The Turkish product review dataset",
        "The Turkish movie reviews",
        "English product review dataset",
        "English product reviews",
        "multi-fold cross validations",
        "fivefold cross validation",
        "Turkish product reviews",
        "same dataset sizes",
        "punctua- tion marks",
        "five F measures",
        "large feature space",
        "average F measures",
        "final classification results",
        "English reviews",
        "final P",
        "classification system",
        "five folds",
        "punctuation marks",
        "question marks",
        "addi- tion",
        "five portions",
        "reduc- tion",
        "978 negative reviews",
        "700 negative reviews",
        "1000 negative reviews",
        "feature spaces",
        "four subsets",
        "elec- tronics",
        "kitchen appliances",
        "existing work",
        "similar datasets",
        "composite score",
        "harmonic mean",
        "Experimental settings",
        "MacBook Pro",
        "16 GB 1600 MHz",
        "NLTK [38] library",
        "case normalization",
        "docu- ments",
        "two probabilities",
        "commerce website",
        "Performance evaluation",
        "classified items",
        "LR classifier",
        "document",
        "quencies",
        "beyazperde",
        "1057 positive",
        "hepsiburada",
        "books",
        "DVDs",
        "700 positive",
        "Lee",
        "1000 positive",
        "Blitzer",
        "electronics",
        "recall",
        "Python",
        "words",
        "exclamation",
        "polarity",
        "suffixes",
        "J48",
        "MEM.",
        "couple",
        "thousands",
        "training",
        "remaining",
        "testing",
        "Table",
        "memory",
        "features",
        "NBM SVM J48 LR Movie",
        "NBM SVM J48 LR Features",
        "five folds cross validation",
        "six fea- ture sizes",
        "smaller fea- ture sizes",
        "five feature selection methods",
        "best micro-average F measure",
        "bigger feature sizes",
        "post hoc tests",
        "desirable size n",
        "senti- ment analysis",
        "electronic review dataset",
        "topical text classification",
        "kitchen review datasets",
        "best classification results",
        "electronics review dataset",
        "Table 1 Baseline results",
        "NBM classifier",
        "best performer",
        "top-ranked features",
        "new method",
        "best results",
        "text classifiers",
        "possible improvements",
        "total features",
        "related formulas",
        "ing classifier",
        "same performance",
        "different documents",
        "kitchen reviews",
        "univariate ANOVA",
        "significant differences",
        "three groups",
        "95% confidence level",
        "two groups",
        "new results",
        "baseline systems",
        "low frequencies",
        "same intuition",
        "baselines",
        "Book",
        "scores",
        "settings",
        "120 experiments",
        "pairs",
        "bold-face",
        "Note",
        "expressing",
        "traditional",
        "Cai",
        "2×",
        "C FS C H I2",
        "text classifi- cation",
        "traditional topic-based text",
        "Q ER D",
        "review datasets",
        "QER method",
        "One reason",
        "decision-tree-based solution",
        "smaller set",
        "ss ifi",
        "ze F",
        "evant features",
        "result",
        "classifier",
        "rs",
        "rk",
        "FD",
        "IG"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 6.772005,
      "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2469691,
      "metadata_storage_name": "s41109-020-00330-x.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MTEwOS0wMjAtMDAzMzAteC5wZGY1",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": " Shun Kodate ",
      "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network",
      "metadata_creation_date": "2020-11-12T15:20:34Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Computational social science Open Access",
        "other third party material",
        "eight local network indices",
        "various online transaction platforms",
        "Applied Network Science",
        "Creative Commons licence",
        "random forest classifiers",
        "Appl Netw Sci",
        "eight network indices",
        "present descriptive analysis",
        "credit card fraud",
        "Network analysis",
        "present study",
        "problematic transaction",
        "appropriate credit",
        "credit line",
        "online marketplaces",
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "rapid growth",
        "electronic transactions",
        "communi- cations",
        "dramatic speed",
        "daily lives",
        "UK Parliament",
        "recent era",
        "money laundering",
        "computer intrusion",
        "illegal items",
        "fictive items",
        "typical approach",
        "individual transactions",
        "traditional approach",
        "corresponding buyer",
        "several hundreds",
        "similar number",
        "focal node",
        "twelve features",
        "four types",
        "classification performance",
        "Machine learning",
        "author(s",
        "statutory regulation",
        "copyright holder",
        "iveco mmons",
        "RESEARCH Kodate",
        "Full list",
        "malicious users",
        "fraudulent users",
        "normal users",
        "online consumer",
        "intended use",
        "permitted use",
        "doi.org",
        "orcid.org",
        "Fraud detection",
        "consumer marketplace",
        "egocentric networks",
        "author information",
        "user profiles",
        "fraud activity",
        "Introduction",
        "tandem",
        "cybercrimes",
        "billions",
        "dollars",
        "year",
        "security",
        "society",
        "McAfee",
        "system",
        "Anderson",
        "dimension",
        "ranges",
        "plagiarism",
        "Abstract",
        "Providers",
        "behavior",
        "texts",
        "background",
        "frauds",
        "seller",
        "edge",
        "up",
        "connectivity",
        "neighbors",
        "aim",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "original",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "naokimas",
        "buffalo",
        "4 Department",
        "Mathematics",
        "University",
        "USA",
        "creativecommons",
        "licenses",
        "crossmark",
        "dialog",
        "Page",
        "18Kodate",
        "13 million monthly active users",
        "credit card systems",
        "social security system",
        "various other fields",
        "online review forums",
        "large online consumer",
        "machine learning algorithms",
        "online social networks",
        "mobile phone network",
        "credit card transactions",
        "suspicious connectivity patterns",
        "online auction system",
        "fraud detec- tion",
        "fraud detection techniques",
        "online C2C marketplaces",
        "online auctions",
        "supervised learning",
        "telecommunica- tion",
        "transaction networks",
        "C2C) marketplace",
        "Standard practice",
        "transaction amount",
        "item category",
        "call duration",
        "call type",
        "geographical region",
        "transaction history",
        "advanced fraudsters",
        "Google LLC",
        "particular words",
        "anomalous behavior",
        "alternative way",
        "graph-theoretic quantities",
        "Dreżewski",
        "car- booking",
        "Van Vlasselaer",
        "cryptocurrency transaction",
        "accom- plices",
        "bipartite cores",
        "belief propagation",
        "133 billion yen",
        "1.2 billion USD",
        "quarter year",
        "previous studies",
        "anomalous users",
        "fraudulent samples",
        "health-care data",
        "empirical data",
        "data set",
        "statistical methods",
        "transaction frauds",
        "reputation frauds",
        "statistical classifier",
        "one jargon",
        "Exemplar features",
        "Computational",
        "decades",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "case",
        "day",
        "week",
        "address",
        "number",
        "calls",
        "age",
        "gender",
        "Akoglu",
        "eyes",
        "administrators",
        "authorities",
        "signature",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "example",
        "authority",
        "drug",
        "idea",
        "nodes",
        "goods",
        "scores",
        "expectation",
        "insurance",
        "Šubelj",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "advertising",
        "Ferrara",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Monamo",
        "reputations",
        "authors",
        "eBay",
        "Mercari",
        "Japan",
        "Many prior network-based fraud detection algorithms",
        "online C2C marketplace service",
        "one problematic sell",
        "local clustering coefficient",
        "local infor- mation",
        "two fraudulent users",
        "two normal users",
        "two indices",
        "sell probability",
        "local indices",
        "two users",
        "local features",
        "connected components",
        "Yan- chun",
        "commercial implementations",
        "various items",
        "United States",
        "Japanese market",
        "following types",
        "non-existing items",
        "medicinal supplies",
        "weighted network",
        "egocentric network",
        "eight indices",
        "trans- actions",
        "separate index",
        "three indices",
        "informa- tion",
        "users’ network",
        "fictive item",
        "node vi",
        "node strength",
        "methods Data",
        "normal transactions",
        "lematic transactions",
        "mean number",
        "focal user",
        "Fictive transactions",
        "directed edge",
        "networks",
        "global",
        "communities",
        "betweenness",
        "k-cores",
        "Bhat",
        "Savage",
        "Others",
        "degree",
        "triangles",
        "Materials",
        "July",
        "January",
        "addition",
        "underwear",
        "medicine",
        "weapon",
        "perspective",
        "morality",
        "hygiene",
        "law",
        "crime",
        "Table",
        "buyer",
        "Figure",
        "1a",
        "Fig.",
        "edges",
        "pairs",
        "information",
        "Barrat",
        "direction",
        "SPi",
        "local clustering coefficient Ci ranges",
        "Random forest classifier",
        "unweighted trian- gles",
        "other three indices",
        "one feedforward triangle",
        "one cyclic triangle",
        "local network",
        "one item",
        "triangle congregation",
        "other four",
        "k ini",
        "weighted version",
        "overlapping set",
        "three-node subnetworks",
        "work motifs",
        "natural interpretation",
        "three nodes",
        "unweighted triangles",
        "two triangles",
        "feedforward triangles",
        "cyclic triangles",
        "many users",
        "different directed",
        "particular neighbors",
        "Such neighbors",
        "bidirectional edges",
        "ki(ki",
        "Fig. 2c",
        "degree zero",
        "proportion",
        "kouti",
        "Examples",
        "vj",
        "contribution",
        "sini",
        "buys",
        "souti",
        "sells",
        "abundance",
        "undirected",
        "Newman",
        "increase",
        "community",
        "Radicchi",
        "Palla",
        "hypothesis",
        "extent",
        "concept",
        "mi",
        "Ciki",
        "Note",
        "Frequencies",
        "marketplace",
        "contrast",
        "index",
        "cycle",
        "CYPi",
        "CYi",
        "definition",
        "calculation",
        "weights",
        "Breiman",
        "The precision–recall (PR) curve",
        "random forest clas- sifier",
        "random forest classifier",
        "ensemble learning method",
        "classifica- tion performance",
        "receiver operating characteristic",
        "two performance measures",
        "true positive rate",
        "false positive rate",
        "piecewise linear manner",
        "Directed triangle patterns",
        "same test sample",
        "tive training samples",
        "ROC) curve",
        "ROC curve",
        "two sets",
        "three-node patterns",
        "directed triangles",
        "same order",
        "single-tree positive",
        "Cyclic triangle",
        "tive samples",
        "precision range",
        "test data",
        "positive probability",
        "multiple classifiers",
        "decision-tree classifiers",
        "best split",
        "reciprocal edges",
        "fraudulent user",
        "fraudulent types",
        "descending order",
        "horizontal axis",
        "vertical axis",
        "test samples",
        "training data",
        "fraudulent) samples",
        "terminal node",
        "negative probability",
        "candidate features",
        "Feedforward triangle",
        "decision trees",
        "300 trees",
        "Hastie",
        "scikit-learn",
        "Pedregosa",
        "overfitting",
        "basis",
        "replace",
        "ment",
        "fraction",
        "average",
        "count",
        "Five",
        "numbers",
        "figure",
        "2",
        "100 random forest classifiers",
        "accurate binary classifier",
        "random number generator",
        "ten different permutations",
        "ten permutations",
        "different types",
        "PR curves",
        "grid search",
        "10-fold cross-validation",
        "maximum depth",
        "max_depth parameter",
        "other hyperparameters",
        "default values",
        "parameter optimization",
        "sampling bias",
        "sampling scheme",
        "Descriptive statistics",
        "survival probability",
        "frac- tion",
        "one transaction",
        "clear distinction",
        "large value",
        "The AUC",
        "lent users",
        "mal users",
        "minimum number",
        "seed number",
        "single set",
        "AUC value",
        "permutation importance",
        "nal nodes",
        "scikit-learn version",
        "useful information",
        "degree distribution",
        "unweighted degree",
        "user type",
        "Fig. 3a",
        "Fig. 3c",
        "training set",
        "performance measure",
        "degree ki",
        "normal",
        "area",
        "good",
        "Strobl",
        "Altmann",
        "method",
        "decrease",
        "tree",
        "integers",
        "split",
        "max_features",
        "n_estimators",
        "Results",
        "specified",
        "classification",
        "difference",
        "many",
        "fact",
        "≥",
        "Normal Fictive Underwear Medicine Weapon",
        "unweighted sell probability",
        "different user types",
        "less frequent transactions",
        "one sell transaction",
        "Seed user type",
        "large fraction",
        "smaller value",
        "characteristic behavior",
        "first column",
        "exclusive buyer",
        "fraudulent type",
        "seed users",
        "sell transactions",
        "Fig. 4a",
        "Fig.  5a",
        "c Strength",
        "specific neighbor",
        "average number",
        "Total number",
        "Table 1 Properties",
        "conditional mean",
        "Degree",
        "ki",
        "majority",
        "result",
        "ence",
        "median",
        "FFi",
        "samples",
        "peak",
        "smallest possible weighted sell probability",
        "smallest possible out-degree",
        "corresponding frac- tions",
        "tinct frequency distributions",
        "Survival probability",
        "cycle probability",
        "Figure  5b",
        "dashed lines",
        "Figure 5b",
        "two aspects",
        "Figure  6b",
        "larger value",
        "clear difference",
        "descriptive statistics",
        "ous section",
        "horizontal line",
        "second feature",
        "Fig.  6a",
        "Fig. 8a",
        "relationships",
        "ini",
        "buyers",
        "one",
        "transactions",
        "neighbor",
        "results",
        "analysis",
        "tendency",
        "Classification",
        "12 features",
        "unweighted sell probability SPi",
        "Fig. 8 Cycle probability",
        "Fig. 7 Triangle congregation",
        "b Survival probability",
        "third feature",
        "real number",
        "fourth feature",
        "fifth feature",
        "sixth feature",
        "seventh feature",
        "eighth feature",
        "c Relationship",
        "WSPi"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.938228,
      "content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 707841,
      "metadata_storage_name": "s40467-015-0033-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDQ2Ny0wMTUtMDAzMy05LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2015-05-16T18:35:45Z",
      "keyphrases": [
        "Creative Commons Attribution License",
        "interesting, potentially useful patterns",
        "Ant colony optimization algorithm",
        "modern intelligent optimization methods",
        "current data analysis tools",
        "important practical significance",
        "Open Access article",
        "many successful examples",
        "machine learning methods",
        "many different specifiedmethods",
        "Uncertain inference rule",
        "RESEARCH Open Access",
        "Bayesian belief network",
        "large data sets",
        "financial data analysis",
        "data compartment analysis",
        "many different fields",
        "data mining techniques",
        "data mining algorithms",
        "mated data generation",
        "data mining methods",
        "data mining technology",
        "collection tools",
        "many methods",
        "inference rules",
        "many other",
        "Uncertainty Analysis",
        "neutral network",
        "Bayesian classification",
        "Extraction methods",
        "uncertain system",
        "tomany fields",
        "mining objects",
        "tical methods",
        "effective methods",
        "Data classification",
        "existing data",
        "data description",
        "data dependency",
        "data regression",
        "data aggregate",
        "data prediction",
        "training data",
        "Rules extraction",
        "classification rules",
        "Yun Sun",
        "Yuanguo Zhu",
        "recent years",
        "classification problems",
        "computer networks",
        "scientific research",
        "ness decision",
        "computational process",
        "overall goal",
        "standable structure",
        "biomedical research",
        "main jobs",
        "decision trees",
        "mathematical formula",
        "original work",
        "Nanjing University",
        "business management",
        "valuable information",
        "two applications",
        "science research",
        "unrestricted use",
        "Ling Chen",
        "Journal",
        "DOI",
        "Correspondence",
        "chinalsy",
        "School",
        "Abstract",
        "increasing",
        "attention",
        "paper",
        "effectiveness",
        "superiority",
        "Keywords",
        "Introduction",
        "databases",
        "finance",
        "E-commerce",
        "logistics",
        "result",
        "amount",
        "people",
        "basis",
        "difficulty",
        "depth",
        "deficiency",
        "concepts",
        "laws",
        "modes",
        "knowledge",
        "reality",
        "instance",
        "telecommunication",
        "retail",
        "study",
        "couple",
        "models",
        "functions",
        "characteristics",
        "categories",
        "output",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "Page",
        "variety",
        "approaches",
        "start",
        "addition",
        "mutation ant colony optimization algorithm",
        "original ant colony optimization algorithm",
        "early decision trees algorithm",
        "simulated annealing algorithm",
        "global search algorithm",
        "powerful mathematical tool",
        "uncertain differential equation",
        "mature technology methods",
        "Decision trees method",
        "fuzzy rules learning",
        "many mathematical tools",
        "massive data set",
        "data mining technique",
        "neutral network algorithm",
        "probability distri- bution",
        "product measure axiom",
        "decision tree method",
        "incremental tree structure",
        "intelligent optimization algorithms",
        "different network models",
        "random initial solutions",
        "Machine learning methods",
        "popular classification methods",
        "complex nonlinear relations",
        "classification machine learning",
        "many evolution algorithms",
        "Genetic algorithm",
        "optimization strategies",
        "bionic optimization",
        "numerical methods",
        "conceptual learning",
        "inductive learning",
        "ID3 method",
        "C4.5 method",
        "different types",
        "many cases",
        "spatial data",
        "probability theory",
        "normality axiom",
        "duality axiom",
        "subadditivity axiom",
        "pattern classification",
        "statistical principle",
        "tering problems",
        "core content",
        "simulation model",
        "complex system",
        "pattern recognition",
        "biological evolution",
        "genetic mechanism",
        "important role",
        "Mixed algorithms",
        "other algorithms",
        "slow convergence",
        "local optimums",
        "Hybrid genetic",
        "hybrid particle",
        "real world",
        "human being",
        "indetermi- nacy",
        "domain experts",
        "belief degree",
        "tainty theory",
        "Many researchers",
        "theoretical work",
        "uniqueness theorem",
        "practical problems",
        "number",
        "variants",
        "SLIQ",
        "clustering",
        "regression",
        "behavior",
        "ant-miner",
        "Herrera",
        "weakness",
        "reason",
        "Zhu",
        "ACOA",
        "MACO",
        "performance",
        "understanding",
        "indeterminacy",
        "samples",
        "situation",
        "choice",
        "event",
        "unlikely",
        "frequency",
        "view",
        "Liu",
        "Chen",
        "Applications",
        "lot",
        "existence",
        "stability",
        "Peng",
        "Yao",
        "two typical classification problems",
        "ant colony optimization algorithm",
        "uncertain optimal control model",
        "heuristic optimization approach",
        "ant transfer prob",
        "option pricing models",
        "expected value operator",
        "two extraction methods",
        "M feasible solutions",
        "following optimization problem",
        "uncertain inference rule",
        "pheromone evaporation rate",
        "same pheromone level",
        "uncertain inference controller",
        "optimal solution s",
        "one ant",
        "artificial ant",
        "tain controller",
        "uncertain systems",
        "uncertain sets",
        "membership functions",
        "five parts",
        "universal approximator",
        "reasonable tool",
        "inverted pendulum",
        "uncertainty theory",
        "mutation operation",
        "data mining",
        "uncertainty sets",
        "decision variable",
        "domain D",
        "candidate solution",
        "best solution",
        "pheromone communication",
        "pheromone levels",
        "initial pheromone",
        "pheromone trails",
        "many paths",
        "bad paths",
        "good paths",
        "next section",
        "basic concepts",
        "last section",
        "objective function",
        "constraint function",
        "good ability",
        "data set",
        "food sources",
        "real ants",
        "cial ants",
        "one path",
        "walking path",
        "stocks",
        "inputs",
        "outputs",
        "rule-base",
        "Gao",
        "robustness",
        "results",
        "Preliminary",
        "Dorigo",
        "nest",
        "time",
        "parameters",
        "iterations",
        "procedures",
        "Step",
        "probability",
        "sk",
        "5",
        "⎪⎪⎪",
        "first measure inversion formula",
        "following three axiom",
        "product uncertain measureM",
        "Mamdani inference rules",
        "Takagi-Sugeno inference rules",
        "triangular uncertain set",
        "regular membership function μ",
        "inversion formulas",
        "inference systems",
        "fuzzy inference",
        "nonempty set",
        "optimal solution",
        "current iteration",
        "uncertainty space",
        "real numbers",
        "expected value",
        "two integrals",
        "key points",
        "fuzzy systems",
        "CRI approach",
        "human knowledge",
        "set function",
        "Amembership functionμ",
        "fuzzy sets",
        "Mk{�k",
        "M{ξ � r",
        "M{B ⊂",
        "B.",
        "⊂ B",
        "algebra",
        "L.",
        "triplet",
        "Lk",
        "Definition",
        "collection",
        "Borel",
        "Example",
        "power",
        "ξn",
        "Bi",
        "sup",
        "Bc",
        "equations",
        "Remark",
        "mode",
        "Theorem",
        "x0",
        "fact",
        "dx",
        "2b",
        "antecedents",
        "consequents",
        "process",
        "consequences",
        "σ",
        "∈",
        "∑",
        "∞",
        "⎪",
        "γ",
        "1",
        "∫",
        "independent uncertain sets",
        "conditional uncertain set",
        "m crisp inputs",
        "crisp data",
        "crisp values",
        "Uncertain system",
        "uncertain antecedents",
        "uncertain consequents",
        "two concepts",
        "constant a",
        "theory",
        "following",
        "X1",
        "Xm",
        "im",
        "ck",
        "coefficients",
        "Eq.",
        "ηi",
        "constants",
        "k∑",
        "min",
        "μil",
        "experts",
        "Y1",
        "Y2",
        "Yn",
        "Yj",
        "βj",
        "ξ",
        "ν",
        "η∗",
        "⎪⎪⎪⎪",
        "1 μ",
        "special triangular uncertain set",
        "m input data",
        "dent uncertain sets",
        "Q uncertain sets",
        "R uncertain sets",
        "consequence uncertain sets",
        "uncertain inference rules",
        "k inference rules",
        "one inference rule",
        "uncertain consequence",
        "m inputs",
        "three cases",
        "two points",
        "symmetry property",
        "Problem formulation",
        "extraction model",
        "decision vector",
        "rule base",
        "n rules",
        "m antecedents",
        "variable xi",
        "k.",
        "βn",
        "μim",
        "bj",
        "ij",
        "loss",
        "generality",
        "above",
        "1 output",
        "ing",
        "αi",
        "γi",
        "2βi",
        "Proof",
        "Equation",
        "computation",
        "dy",
        "section",
        "example",
        "corresponding objective function values",
        "following optimization model",
        "candidate decision variable",
        "Q + 1 choices",
        "best feasible solution",
        "mean absolute error",
        "best rule base",
        "new candidate sequence",
        "m values",
        "absolute errors",
        "feasible solutionX0",
        "Uncertain systems",
        "mutated solution",
        "current solution",
        "tain sets",
        "five descriptions",
        "system outputs",
        "origin outputs",
        "C classes",
        "ith subinterval",
        "extraction method",
        "The procedures",
        "fixed parameter",
        "evaporation rate",
        "classification problem",
        "next node",
        "sequence xi1xi2",
        "mutated element",
        "step k",
        "left nodes",
        "Once ants",
        "Q∑",
        "4 antecedents",
        "input",
        "MAE",
        "1 P",
        "P∑",
        "integers",
        "nonsense",
        "subintervals",
        "xij",
        "Extractionmethod",
        "withmutations",
        "1 value",
        "R.",
        "steps",
        "walk",
        "Denote",
        "τi",
        "xik",
        "way",
        "termination",
        "condition",
        "ρg",
        "⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪",
        "inner cycle termination criterion",
        "M candidate solutions",
        "good robust property",
        "uncertain rule base",
        "feasible solu- tion",
        "local search operation",
        "Metropolis acceptance criterion",
        "Step 2 Ant movement",
        "new decision vector",
        "Simulated annealing algorithm",
        "feasible solution X0",
        "portfolio optimization",
        "optimization problem",
        "decision variables",
        "initial solution",
        "neighbor solution",
        "initial value",
        "anneal- ing",
        "controlled cooling",
        "sequence xip",
        "allM ants",
        "previous section",
        "Repeat Step",
        "1 steps",
        "values",
        "Xl.",
        "classification",
        "Kirkpatrick",
        "name",
        "inspiration",
        "metallurgy",
        "technique",
        "heating",
        "material",
        "size",
        "crystals",
        "defects",
        "temperature",
        "point",
        "neighborhood",
        "order",
        "xiq",
        "procedure",
        "original feasible solution",
        "search range",
        "current temperature",
        "F(X̂",
        "Set",
        "step",
        "rithm",
        "tk",
        "τ"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.719706,
      "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2424684,
      "metadata_storage_name": "s40064-015-1515-4.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDA2NC0wMTUtMTUxNS00LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Rik Das",
      "metadata_title": "Multi technique amalgamation for enhanced information identification with content based image data",
      "metadata_creation_date": "2015-11-26T05:08:05Z",
      "keyphrases": [
        "multi tech- nique feature extraction process",
        "content based image data Rik Das1",
        "Creative Commons Attribution 4.0 International License",
        "traditional text based annotation technique",
        "Dr. Camil Bulcke Path",
        "customary text based process",
        "Diverse low level features",
        "content based feature extraction",
        "content based image identification",
        "Creative Commons license",
        "robust feature vectors",
        "Multi technique amalgamation",
        "low cost storage",
        "digital photo-capture devices",
        "four public datasets",
        "test Open Access",
        "P.O. Box",
        "digital image acquisition",
        "huge image repositories",
        "gigantic image datasets",
        "image capturing devices",
        "new research challenges",
        "OT Scene) Dataset",
        "three different techniques",
        "exist- ing techniques",
        "original author(s",
        "Semantically analogous images",
        "image identification technique",
        "enhanced information identification",
        "manual process",
        "image content",
        "Diverse applications",
        "image recognition",
        "image binarization",
        "image transform",
        "Image classification",
        "Image retrieval",
        "Raventós",
        "RESEARCH Das",
        "image information",
        "author information",
        "Corel Dataset",
        "Caltech Dataset",
        "Conventional techniques",
        "art techniques",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Recent years",
        "computer power",
        "accessible internet",
        "Efficient indexing",
        "computer vision",
        "machine learning",
        "Automatic derivation",
        "severe limitations",
        "aforesaid limitations",
        "resourceful foundation",
        "social media",
        "morphological operator",
        "Oliva Torralba",
        "recognition rate",
        "Classification result",
        "average increase",
        "retrieval result",
        "Slant transform",
        "unrestricted use",
        "appropriate credit",
        "Xavier Institute",
        "Social Service",
        "Purulia Road",
        "Full list",
        "effective alternative",
        "meaningful information",
        "Information Technology",
        "Wang Dataset",
        "decision fusion",
        "Background",
        "ubiquity",
        "mass",
        "popularity",
        "Madireddy",
        "Walia",
        "words",
        "mapping",
        "perception",
        "vocabulary",
        "person",
        "nature",
        "Abstract",
        "proliferation",
        "areas",
        "biomedicine",
        "military",
        "commerce",
        "education",
        "means",
        "success",
        "paper",
        "Precision",
        "state",
        "Otsu",
        "threshold",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "SpringerPlus",
        "DOI",
        "Correspondence",
        "1 Department",
        "Ranchi",
        "Jharkhand",
        "India",
        "end",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "26Das",
        "competence",
        "color",
        "content based image recog",
        "binarization based feature extraction",
        "content based image classification",
        "feature extraction Feature extraction",
        "Z score normalization",
        "orthogonal unitary matrices",
        "Appropriate threshold selection",
        "fusion based method",
        "single feature extrac",
        "fractional energy coefficient",
        "fifteen fractional coefficients",
        "high dimensional data",
        "main contribu- tion",
        "multi-technique feature extraction",
        "efficient feature extraction",
        "feature extrac- tion",
        "efficient image binarization",
        "feature vector dimension",
        "image trans- formation",
        "multi technique fusion",
        "image binariza- tion",
        "Image binarization techniques",
        "feature vectors",
        "feature spaces",
        "image data",
        "energy spectrum",
        "subband coefficients",
        "novel techniques",
        "Diverse techniques",
        "image identification",
        "image elements",
        "image patterns",
        "seven image",
        "recognition decision",
        "hybrid architecture",
        "fusion architecture",
        "Statistical validation",
        "trans- forms",
        "morphological operators",
        "archi- tecture",
        "four subsections",
        "earlier works",
        "four topics",
        "basis functions",
        "one representation",
        "two aspects",
        "critical components",
        "compact structure",
        "cient storage",
        "radical reduction",
        "multiple scales",
        "tropic transform",
        "various factors",
        "uneven illumination",
        "research objectives",
        "research results",
        "transformation process",
        "aforesaid properties",
        "basis images",
        "Original images",
        "shape",
        "texture",
        "number",
        "features",
        "morphology",
        "Comparison",
        "retrieval",
        "correlation",
        "connection",
        "contemporary",
        "Change",
        "domain",
        "set",
        "series",
        "Annadurai",
        "Shanmugalakshmi",
        "advantages",
        "waveforms",
        "use",
        "analysis",
        "transmission",
        "Kekre",
        "Thepade",
        "subbands",
        "Prakash",
        "Luo",
        "execution",
        "inadequate",
        "Ramírez- Ortegón",
        "Gradient vector flow fields",
        "multi technique feature extraction",
        "fusion based image identification",
        "superior prediction accuracy",
        "feature level correlations",
        "image signature extraction",
        "feature extraction technique",
        "Popular contour-based descriptors",
        "Enhanced classification results",
        "curvature scale space",
        "feature selection technique",
        "region- based descriptors",
        "3 D color histogram",
        "diverse extraction techniques",
        "Two different categorization",
        "global threshold selection",
        "shape feature extraction",
        "multilevel mean threshold",
        "image binarization techniques",
        "local threshold selection",
        "retrieval process complexity",
        "feature vector",
        "identification rate",
        "Image Content",
        "different techniques",
        "shape descriptors",
        "semantic retrieval",
        "Local descriptors",
        "local decisions",
        "shape information",
        "region-based descriptors",
        "information fusion",
        "early fusion",
        "late fusion",
        "hybrid fusion",
        "intermediate fusion",
        "adverse effect",
        "Contemporary literatures",
        "unfavourable influences",
        "binarized images",
        "standard deviation",
        "gray values",
        "Commercial viability",
        "existing literatures",
        "ary lines",
        "Fourier descriptor",
        "chain codes",
        "complex shapes",
        "Information recognition",
        "Recent studies",
        "four classes",
        "higher dimensions",
        "joint model",
        "genetic algorithm",
        "optimum boundaries",
        "numerical intervals",
        "memory con",
        "filter responses",
        "distinct features",
        "concentrated features",
        "multiple features",
        "Fusion methodologies",
        "rate learner",
        "Gabor filters",
        "Color moments",
        "texture features",
        "contrast",
        "computation",
        "Valizadeh",
        "average",
        "advantage",
        "spread",
        "calculation",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Rojas",
        "Shaikh",
        "Use",
        "systems",
        "Flickner",
        "PicToSeek",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Emphasize",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "area",
        "object",
        "Kim",
        "single",
        "input",
        "sepa",
        "combiner",
        "scalability",
        "comparison",
        "mix",
        "Zhu",
        "Shyu",
        "sumption",
        "ElAlami",
        "back propagation neural net- work",
        "fuzzy set theoretic approach",
        "angular radial transform descriptor",
        "artificial neural network",
        "spatial structure descriptors",
        "spatial orientation tree",
        "lesser computational overhead",
        "neural network architecture",
        "correct semantic retrieval",
        "Higher retrieval results",
        "semantic retrieval results",
        "local edge bins",
        "non directional edge",
        "HSV color space",
        "significant point features",
        "Three different techniques",
        "color layout descriptor",
        "greater feature dimension",
        "content-based image retrieval",
        "tex- ture features",
        "edge histogram descriptor",
        "inter-class feature extraction",
        "Gabor texture descriptor",
        "fusion based classifier",
        "invariant color features",
        "invariant moments",
        "retrieval decisions",
        "retrieval performance",
        "retrieval rate",
        "retrieval purpose",
        "color histogram",
        "GIST descriptor",
        "horizontal edge",
        "Feature vectors",
        "popular feature",
        "32 feature maps",
        "feature values",
        "feature size",
        "color co",
        "color motif",
        "color moment",
        "occurrence matrix",
        "Recognition process",
        "image signatures",
        "Multi view",
        "Wavelet packets",
        "Eigen values",
        "sub repository",
        "main image",
        "right neighbourhood",
        "query image",
        "scan pattern",
        "equal weights",
        "Precision values",
        "individual techniques",
        "Six semantics",
        "sub- image",
        "tion techniques",
        "same size",
        "16 average value",
        "other hand",
        "morphological technique",
        "vertical edge",
        "shape features",
        "EHD) features",
        "edge images",
        "45° edge",
        "135° edge",
        "Hiremath",
        "Pujari",
        "Yue",
        "points",
        "similarity",
        "Banerjee",
        "Jalab",
        "increased",
        "Shen",
        "Wu",
        "authors",
        "Irtaza",
        "kind",
        "training",
        "response",
        "intra-class",
        "Rahimi",
        "Moghaddam",
        "CCM",
        "difference",
        "pixels",
        "DBPSP",
        "ANN",
        "Subrahmanyam",
        "MCMCM",
        "colour",
        "CMs",
        "sub-image",
        "unique",
        "Methods",
        "4 scales",
        "8 orientations",
        "Douze",
        "region",
        "Red Component Green Component Blue Component",
        "popular global threshold selection method",
        "local threshold selec- tion method",
        "image feature point extraction",
        "different gray level pixels",
        "four subsec- tions",
        "three color components",
        "necessary image information",
        "squared Euclidian distance",
        "document image binarzation",
        "Conventional BoW model",
        "alloca- tion",
        "feature extraction",
        "level histogram",
        "thresholding method",
        "parametric method",
        "optimal threshold",
        "words model",
        "information losses",
        "fifth subsection",
        "test images",
        "binari- zation",
        "dant details",
        "two classes",
        "foreground pixels",
        "background pixels",
        "class variance",
        "The separation",
        "Comprehensive investigation",
        "class probabilities",
        "Total variance",
        "two terms",
        "erate bag",
        "SIFT algorithm",
        "SIFT descriptors",
        "cluster members",
        "codebook generation",
        "final step",
        "huge computational",
        "BoW generation",
        "class means",
        "tering process",
        "descriptor dimension",
        "classification",
        "techniques",
        "methods",
        "fusion",
        "architecture",
        "following",
        "description",
        "datasets",
        "Fig.",
        "way",
        "combined",
        "sum",
        "variances",
        "Eq.",
        "Eqs.",
        "q1",
        "effect",
        "contributions",
        "Zhao",
        "size",
        "problem",
        "omissions",
        "stability",
        "clustering",
        "codewords",
        "overhead",
        "∑",
        "σ",
        "N*N feature vector Feature Vector Dimension Reduction",
        "Compute binary image maps",
        "N unitary slant matrix",
        "Blue (B) color component",
        "N × N matrix",
        "higher intensity feature vectors",
        "two dimensional slant transform",
        "N × 1 vector",
        "higher intensity group",
        "three different color",
        "discrete Fourier transform",
        "One dimensional transform",
        "lower intensity group",
        "Slant transform matrices",
        "frequency domain information",
        "local threshold value",
        "spatial domain information",
        "efficient algorithm design",
        "insignificant transform coefficients",
        "drastic reduction",
        "smaller dimension",
        "Energy compaction property",
        "unitary transform",
        "spatial information",
        "matrix multiplication",
        "two groups",
        "orthogonal transform",
        "inverse transform",
        "transform operation",
        "grey values",
        "two codewords",
        "large fraction",
        "faster execution",
        "tion operation",
        "computational overhead",
        "energy conservation",
        "signal energy",
        "significant role",
        "Partial Coefficients",
        "sequential transformations",
        "average energy",
        "test image",
        "image features",
        "monochrome image",
        "image quality",
        "image line",
        "components R",
        "tain operations",
        "real components",
        "pixel values",
        "average coding",
        "Transforms",
        "approach",
        "cluster",
        "mean",
        "codebook",
        "Tx",
        "Method",
        "End",
        "xhi",
        "cer",
        "capacity",
        "example",
        "characteristic",
        "Green",
        "8 bits",
        "1 bit",
        "images",
        "24–2 bits",
        "Pratt",
        "less",
        "column",
        "row",
        "forward",
        "Eqs",
        "The energy compaction property",
        "gray scale opening operation",
        "lower intensity feature vector",
        "higher intensity feature vector",
        "entire feature vector set",
        "feature vector extraction process",
        "shorter dis- tance",
        "feature vector formation",
        "highest classification result",
        "Blue color components",
        "top-hat transform technique",
        "feature vector database",
        "significant image information",
        "image transform End",
        "image similarity measures",
        "Higher similarity",
        "database images",
        "working process",
        "Similar process",
        "classification results",
        "slant transform",
        "complete set",
        "partial coefficient",
        "fractional coefficient",
        "small patch",
        "Human perception",
        "shape context",
        "point correspondences",
        "considerable contribution",
        "closing operations",
        "hat transformation",
        "bright peaks",
        "tophat operator",
        "code- book",
        "noteworthy image",
        "original image",
        "visual words",
        "series form",
        "one cluster",
        "codebook size",
        "peak detector",
        "Red, Green",
        "coefficients",
        "2. Peak",
        "algorithm",
        "query",
        "block",
        "12 elements",
        "dimension",
        "variant",
        "Sridhar",
        "Exit",
        "bag",
        "BoW",
        "methodology",
        "clusters",
        "Determination",
        "distance",
        "Dunham",
        "σ   Red Component Green Component Blue Component",
        "Three different distance measures",
        "forward Neural Network Classifier",
        "support vector machine",
        "nearest neigh- bors",
        "city block distance",
        "MSE) distance metric",
        "Canberra Distance measure",
        "fusion based classification",
        "first hidden layer",
        "input feature vectors",
        "mean squared error",
        "Data standardization technique",
        "back propagation technique",
        "supervised learning procedure",
        "multi layer perceptron",
        "database image T",
        "query image Q",
        "color component",
        "SVM) classifier",
        "classifier types",
        "network prediction",
        "Euclidian distance",
        "final distance",
        "classification decision",
        "Multilayer Perceptron",
        "input layer",
        "input features",
        "input units",
        "classified query",
        "clas- sifier",
        "following sections",
        "tophat transform",
        "morphological operation",
        "xloF.V.",
        "common range",
        "Top-Hat operator",
        "weighted sum",
        "appropriate output",
        "sification performance",
        "training tuple",
        "backward direction",
        "normalization process",
        "classified image",
        "higher values",
        "precision values",
        "stdev xhi",
        "qp background",
        "greater effect",
        "individual distances",
        "class majority",
        "similar images",
        "top 20 images",
        "target value",
        "stdev xlo",
        "VF xlo",
        "foreground",
        "xmean",
        "xhimeanxhi",
        "xlomeanxlo",
        "Qi",
        "attributes",
        "possibilities",
        "Dcityblock",
        "Deuclidian",
        "DMSE",
        "distn",
        "disti",
        "weights",
        "interest",
        "Ranking",
        "feed",
        "MLP",
        "Alsmadi",
        "optimization",
        "modifications",
        "The",
        "µ",
        "feature vector dataset",
        "Support vector machine",
        "original training data",
        "Optimal separating hyperplane",
        "two different classes",
        "bias node",
        "subsequent layer",
        "hidden layer",
        "Weighted output",
        "final layer",
        "output layer",
        "forward property",
        "higher dimension",
        "nonlinear mapping",
        "new dimension",
        "appropriate nonlinear",
        "class labels",
        "summation",
        "utes",
        "nodes",
        "half",
        "inputs",
        "processing"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.418702,
      "content": "\nSentiment analysis and the complex \nnatural language\nMuhammad Taimoor Khan1*, Mehr Durrani2, Armughan Ali2, Irum Inayat3, Shehzad Khalid1 and Kamran \nHabib Khan4\n\nIntroduction\nSentiment analysis (Pang and Lillian 2008) is a type of text classification that deals with \nsubjective statements. It is also known as opinion mining, since it processes opinions in \norder to learn about public perception. Sentiment analysis and opinion mining are the \nsame, and are used interchangeably throughout the document. It uses natural language \nprocessing (NLP) to collect and examine opinion or sentiment words. SA is explained \nas identifying the sentiments of people about a topic and its features (Pang and Lillian \n2008). The reason for the popularity of opinion mining is because people prefer to take \nadvice from others in order to invest sensibly. Determining subjective attitudes in big \nsocial data is a hotspot in the field of data mining and NLP (Hai et al. 2014).\n\nAbstract \n\nThere is huge amount of content produced online by amateur authors, covering a \nlarge variety of topics. Sentiment analysis (SA) extracts and aggregates users’ senti-\nments towards a target entity. Machine learning (ML) techniques are frequently used \nas the natural language data is in abundance and has definite patterns. ML techniques \nadapt to domain specific solution at high accuracy depending upon the feature set \nused. The lexicon-based techniques, using external dictionary, are independent of data \nto prevent overfitting but they miss context too in specialized domains. Corpus-based \nstatistical techniques require large data to stabilize. Complex network based tech-\nniques are highly resourceful, preserving order, proximity, context and relationships. \nRecent applications developed incorporate the platform specific structural information \ni.e. meta-data. New sub-domains are introduced as influence analysis, bias analysis, and \ndata leakage analysis. The nature of data is also evolving where transcribed customer-\nagent phone conversation are also used for sentiment analysis. This paper reviews \nsentiment analysis techniques and highlight the need to address natural language \nprocessing (NLP) specific open challenges. Without resolving the complex NLP chal-\nlenges, ML techniques cannot make considerable advancements. The open issues and \nchallenges in the area are discussed, stressing on the need of standard datasets and \nevaluation methodology. It also emphasized on the need of better language models \nthat could capture context and proximity.\n\nKeywords: Sentiment analysis, Machine learning, Sentiment orientation, Complex \nnetworks\n\nOpen Access\n\n© 2016 Khan et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nREVIEW\n\nKhan et al. Complex Adapt Syst Model  (2016) 4:2 \nDOI 10.1186/s40294-016-0016-9\n\n*Correspondence:   \ntaimoor.muhammad@gmail.\ncom \n1 Bahria University, Shangrilla \nRoad, Sector E-8, Islamabad, \nPakistan\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40294-016-0016-9&domain=pdf\n\n\nPage 2 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nManufacturers are also interested to know which features of their products are more \npopular in public, in order to make profitable business decisions. There is a huge reposi-\ntory of opinion content available at various online sources in the form of blogs, forums, \nsocial media, review websites etc. They are growing, with more opinionated content \npoured in continuously. It is, therefore, beyond the control of manual techniques to \nanalyze millions of reviews and to aggregate them towards a rapid and efficient deci-\nsion. Sentiment analysis techniques perform this task through automated processes with \nminimal or no user support. The online datasets may also contain objective statements, \nwhich do not contribute effectively in sentiment analysis. Such statements are filtered at \npre-processing.\n\nOpinion mining deals with identifying opinion patterns and presenting them in a \nway that is easy to understand. The outcome of sentiment analysis can be in the form \nof binary classification, such as categorizing opinions as recommended or not recom-\nmended. It can be considered as a multi-class classification problem on a given scale of \nlikeness. Cambria et al. (2013) used common-sense knowledge to improve the results of \nsentiment analysis. The results can be presented in the form of a short summary gen-\nerated from the overall analysis. Sentiment analysis has various sub streams including \nemotion analysis, trend analysis, and bias analysis etc. Its applications has outgrown \nfrom business to social, political and geographical domains. Sentiment analysis is \napplied to emails for gender identification through emotion analysis (Mohammad and \nYang 2011). Emotion is applied to fairy tales to draw interesting patterns (Mohammad \n2011). Considering text a complex network of words that are associated to each other \nwith sentiments, graph based analysis techniques are used for NLP tasks.\n\nNatural language processing\n\nOpinion mining requires NLP, to extract semantics of opinion words and sentences. \nHowever, NLP has open challenges that are too complex to be handled accurately till \ndate. Since sentiment analysis makes extensive use of NLP, it has this complex behav-\nior reflected. The assumptions in NLP for text categorization do not work with opinion \nmining, as they are different in nature. Documents having high frequency of matching \nwords may not necessarily possess same sentiment polarity. It is because, a fact in text \ncategorization could be either correct or incorrect, and is well known to all. Unlike facts, \na variety of opinions can be correct about the same product, due to its subjective nature. \nAnother difference is that, opinion mining is sensitive to individual words, where a sin-\ngle word like NOT may change the whole context. The open challenges are negations \nwithout using NOT word, sarcastic and comparative sentences etc. The later section has \na detailed discussion on NLP issues that affect sentiment analysis.\n\nThe subjective content from the online sources have simple, compound or complex \nsentences. Simple sentences possess single opinion about a product, while compound \nsentences have multiple opinions expressed together. Complex sentences have implicit \nmeaning and are hard to evaluate. Regular opinions pertain to a single entity only, while \ncomparative opinions have an object or some of its aspects discussed in comparison to \nanother object. Comparative opinions can either be objective or subjective. An example \nof a subjective sentence having comparison is “The sound effects of game X are much \nbetter than that of game Y” whereas an example of objective sentence with comparison is \n\n\n\nPage 3 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\n“Game X has twice as many control options as that of Game Y”. Opinion mining expects \na variety of sentence types, since people follow different writing styles in order to express \nthemselves in a better way.\n\nSentiment analysis\n\nThe machine learning (ML) based techniques are supervised, semi supervised or unsu-\npervised. The supervised techniques require labeled data, while the semi supervised \ntechniques need manual tuning from domain experts. The unsupervised techniques \nmake use of statistical analysis on large volume of data. ML techniques has a large fea-\nture set using Bag-of-words (BOW). Results are improved by pruning repetitive and \nlow quality features. The opinion words are extracted to identify the polarity of opinion \nexpressed for a feature. The performance of a classifier is measured through its effective-\nness at the cost of efficiency. Effectiveness is calculated as precision/recall and F-meas-\nure, which are measurements of relevance.\n\nSentiment analysis can also be considered as a complex network. It consists of nodes \nand edges joining them. Many complex systems from a variety of domains are repre-\nsented as network including environmental modeling (Niazi et al.  2010), business sys-\ntems (Aoyama 2002), wireless sensors, and ad-hoc networks (Niazi and Hussain 2009). \nNetworks are rich in information, having a range of local and global properties. Text cor-\npora can be used with words as nodes and edges representing the structural or seman-\ntic association between them. The adjacent nodes sharing a link are closely associated \nand directly affect each other through the weight of the link they share. Representing \ntext as complex network, various properties like centrality, degree distribution, com-\nponents, communities, paths etc. can be used to explore the data thoroughly. Through \nmulti-partite graphs, nodes can be distributed among various clusters with inter-cluster \nedges only. It separates different types of entities discussed in comparison. Entities are \nlinked to their respective aspects/features and then to the sentiments associated. The \nsentiments can be linked with the reasons shared in support of those sentiments.\n\nData sources\n\nOpinion mining has diverse subjective data sources that are available online. They cover \na large number of topics and are up-to-date with current issues. Introduction of Web2.0 \nin the last decade has enabled people to post their thoughts and opinions on a range of \ntopics. The data produced online is growing all the time produced by people from differ-\nent backgrounds (Katz et al. 2015). Opinion mining makes use of this data generated by \nmillions of users all over the world. According to Business Week survey in 2009, 70 % of \nthe people consult online reviews and ratings to make a purchase. Comscore/The Kelsey \ngroup in 2007 reported that 97 % of the people who made purchases based on online \nreviews, found them to be honest.\n\nThe user generated subjective content is of value to be assessed and summarized for \nprospective customers. These online data sources are in the form of blogs, reviews and \nsocial media websites. The popularity of blogging is on the rise, where people from dif-\nferent walks of life express their opinions about various entities and events and get com-\nments on them. At times, it leads to a form of discussion among the author and various \nusers commenting on them. A detailed analysis on blogging styles of authors, as they \n\n\n\nPage 4 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nfollow their own unique approaches for expressing their feelings is provided in (Chau \nand Xu 2007). Blogs contain opinions about various products, services, their features, \npackages and promotions. Most of the online studies on opinion extraction use blogs as \ndatasets (Qiang and Rob 2009) to perform detailed analysis.\n\nThere are professional review websites providing customers’ feedbacks, used for sen-\ntiment analysis. E-commerce websites allow customers to comment on their products. \nSocial media is another popular medium of sharing information among like-minded \npeople. Here, a variety of subjects are discussed where people express their opinions, \nbased on their own experience. Social media websites have a very complex struc-\nture for extracting information having user opinions. They allow users to express their \nviews through sharing articles and other media sources as an external link. Twitter, also \nreferred to as microblogging, has the problem of reviews being too short and at times \nmiss the context.\n\nThis review article is organized into the following divisions. Section 2 reviews the Sen-\ntiment analysis techniques and the NLP issues. Section 3 provides a discussion on the \nreview studied and Sect. 4 list the application areas for sentiment analysis. Section 5 has \nconcluded the study to important issues drawn from the study. Section 6 has distribu-\ntion of the work carried out by the authors.\n\nReview\nThe sentiment analysis techniques categorize reviews into positive and negative bins or \nmultiple degrees of it. The social data can be analyzed at three different levels i.e. user \ndata, relationship data and content (Tang et al.  2014). In survey (Guellil and Boukhalfa \n2015) these categories are further elaborated. Recommender systems are extended to \nsupport textual content using knowledge (Tang et al. 2013). In our previous work (Khan \nand Khalid 2015) sentiment analysis is highlighted to address health care problems from \nthe view point of a user. The issues faced in SA also depend on the data sources and \nnature of analysis required. An important aspect of social data analysis is the identifi-\ncation of sentiments and sentiment targets (Tuveri and Angioni 2014; Zhang and Liu \n2014). Opinion mining also consider the additional features of opinion holder and time. \nSentiment analysis techniques can be separated into three groups: supervised, semi-\nsupervised and unsupervised techniques.\n\nThe supervised techniques are the machine learning classifiers. They are more accu-\nrate, however, need to be trained on a relevant domain. The unsupervised statistical \ntechniques do not require training. They are efficient in dynamic environment but at the \ncost of accuracy. Sentiment analysis techniques analyze opinion datasets to generate a \ngeneral perception that people have about a product. The classification of sentiments in \na review document is performed through identifying and separating all the positive and \nnegative opinion words. Considering the strength of these words, along with their polar-\nity, helps in multi-class classification. Machine learning classifiers such as Naive-Bayes, \nk-nearest neighbor and centroid based classifier etc., are successfully used for this pur-\npose. Semantic orientation based techniques used for opinion mining are Lexicon based \nand statistical analysis. Lexicon based technique works with individual words while sta-\ntistical analysis incorporates words co-occurrence using point wise mutual information \n(PMI) and latent semantic analysis (LSA). Semi-supervised techniques start with a small \n\n\n\nPage 5 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nset of opinion words from the given domain, and expand on it. More opinion words are \nexplored by querying the starting seeds. The newly found words are queried again to find \nmore words until no new words are returned. Orientation of the opinion word form the \nbasis for classification. Other attributes used are frequency of occurrence, location and \nco-occurrence with other words. The taxonomy of these approaches is shown in Fig. 1.\n\nSentiment classification\n\nThese are the machine learning classifiers used for sentiment analysis. They can be \napplied to text documents at three levels for analysis. A document level approach, \nwhich studies the whole document as a single entity is appropriate for text categoriza-\ntion. However, document level approach is not viable for sentiment analysis with docu-\nments having multiple opinions. Therefore, sentiment analysis is performed extensively \nat sentence or word level. Word level analysis is also known as sentiment level analysis. \nML techniques suits sentiment analysis as the data is in abundance and there is obvious \npresence of patterns (Schouten and Frasincar 2015). The classifiers are trained on label \ndataset having samples representing all classes. A test dataset is used to evaluate the per-\nformance of the classifiers for the given task. Let the set of documents as {D = d1,…,dn}, \nand set of classes labeled as {C = c1,…,cn}, then the task is to classify document di in D \nwith a label ci in C. This task can be performed using supervised classifiers. The more \nfrequently used classifiers for sentiment analysis are discussed below.\n\nNaïve Bayes\n\nNaive Bayes (NB) classifier is extensively used for text classification. It learns from a \ntraining dataset of annotated feature vectors, with labels as positive and negative (in case \nof binary classification). The probability of a feature vector is calculated with each label \nusing the annotated training dataset. The feature vector is assigned a label that has high-\nest probability for it. If this information is preserved, it can be used to show confidence \nin a label for a feature vector. In further modifications of NB a fuzzy region is defined \nin which feature vectors hold both labels with a certain level of confidence. Text data \nnormally have high dimensional feature vectors. Therefore, the process of calculating \nprobability is repeated for each feature vector, and then all the probabilities contribute \ntowards the final decision. The feature set is represented as F = f1, f2…fm}, where prob-\nability of a document belonging to a class shown as:\n\nFig. 1 Taxonomy of expository literature on sentiment analysis\n\n\n\nPage 6 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nShows the probability of a document dj represented by its vector dj* belonging to a class \nci. It is the product of probabilities for all the features in the feature set. The document \nvector dj\n\n* is assigned to a class ci in order to maximize P\n(\n\nci\n\n∣\n\n∣\n\n∣\nd∗j\n\n)\n\n. The logarithm of prob-\nabilities are summed up to classify an opinion document. It is preferred over product of \nprobabilities to avoid underflow. It addresses the missing value problem as well. Slack \nvariables add smoothing effect against noisy data. Weights can also be assigned to fea-\ntures which define their contribution towards the classification. It is a biased approach, \nwhere prominent features are given high weights to play a major role in choose a senti-\nment label.\n\nNaive Bayes works on the assumption that all the sentences of a review document are \nopinion sentences. It also assumes that features of a document are independent of each \nother. Despite of this unrealistic assumption, Naïve Bayes is very successful and is used \nin various practical applications. The assumption of treating features as independent of \neach other makes Naive Bayes highly efficient (Dai et al. 2007). Although, Naive Bayes \nclassifier is simple, yet it is effective because of its robustness to irrelevant features. It \nperforms well in domains with many equally important features. It is considered to be \nmore reliable for text classification and sentiment analysis. The accuracy of the classifier \nimproves with pre-processing noise. It also used as transfer learning when trained on a \ndataset similar to the target dataset.\n\nNearest neighbor\n\nk-nearest neighbor classifier has been frequently used in literature for text classifica-\ntion. It considers the labels of k nearest neighbors to classify a test document. A special \ncase of the k-NN problem is typically referred to as classimbalance problem identified \nin (Yang and Liu 1999). Classes with more training data have higher influence to predict \nsame label for the new document. There are fewer chances of acquiring a class label if \nthat class has fewer training examples. (Li et al. 2003) catered this problem by using vari-\nable value of k for each class. Thus, the class having more training data will have higher \nvalue of k as compared to the one having few samples. This solution is helpful in online \nclassification, where there is time constraint on trying different values of k.\n\nA study on performance of k-NN using pre-processed dataset is conducted in (Shin \net al. 2006) claiming 10 % improvement when noise and outliers are filtered out. An opti-\nmum value is chosen as threshold to separate regular data from noise. Sentiment analy-\nsis is performed with a reduced set of feature vector in (Sreemathy and Balamurugan \n2012) to avoid the curse of dimensionality. Accuracy of the model improves as irrelevant \nfeatures were removed. Features are assigned weights to vary their contribution towards \ndecision making. Weights are extracted from probability of information in documents \nacross different categories. Tree-fast k-NN is introduced as fast kNN model (Soucy and \nMineau 2001). This tree based indexing of retrieval system improves the accuracy of \nk-NN in distance calculation. Its effective against large feature sets. The order of features \nand their thresholds are identified from within the training data. k-NN has promising \n\n(1)P(ci\n∣\n\n∣dj\n∗\n) =\n\np(ci)(\n∏m\n\ni=1 p(fi|ci) )\n\np(d∗j )\n\n\n\nPage 7 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nresults in sentiment analysis; however, it is more susceptible to noise and high dimen-\nsional feature set. Therefore, more of the work in k-NN for text classification has focused \non feature selection and reduction techniques as they are the driving factors of k-NN’s \nperformance.\n\nCentroid based\n\nCentroid based (CB) classifier calculates centroid vector or prototype vector for each \nclass in the training dataset. Centroid vector is the central point of the class and may not \nrepresent an actual training data. The distance of each test document is calculated with \nthe prototype vector of the class and is classified based on similarity with it. Its perfor-\nmance depends on the chosen centroid vectors. It is efficient since time and space com-\nplexities are proportional to the number of classes rather than training documents. To \ndouble the training data reverse of reviews are generated in (Xia et al. 2015) by invert-\ning the sentiment terms and their labels. Using both sets of training data with Mutual \nInformation (MI) the results were improved when only selected reviews were inverted. \nExternal dictionary WordNet is used to generate inverse for sentiment terms, however, \npseudo-antonyms can be generated internally using the corpus.\n\nms, however, pseudo-antonyms can be generated internally using the corpus. A variety \nof approaches have been used for CB classifier. Rocchio algorithm calculates centroid \nto represent feature space of documents (Ana and Arlindo 2007; Tan 2007a, b). Cen-\ntroid is computed through average of positive examples in (Han and Karypis 2000) and \nsum of positive cases i.e. the related training examples (Chuang et al. 2000). Normalized \nsum of positive vectors used in (Lertnattee and Theeramunkong2004), cosine similar-\nity between the test document and the Centroid of a class (Hidayet and Tunga 2012). \nCentroid is used with inverse of class similarity as well improving the accuracy close to \n100 % on the given dataset when characters are chosen as features instead of n-grams.\n\nCentroid evaluation is sensitive to noise in the training dataset which affects the over-\nall performance of the classifier. This shortfall is exposed when Centroid classifier is \napplied to a slightly different domain. The reason for this drawback is that some opinion \nwords are domain dependent. They have different polarity or strength of polarity when \nused in a different domain. Smoothing techniques have being proposed in (Tan 2007a, b;  \nLertnattee and Theeramunkong 2006; Guan 2009) that minimizes the effect of noise \nin the dataset. (Chizi et al. 2009) defined a weighting scheme giving higher weights to \nexplicit opinion words. Characters and special characters for feature selection are used \nin (Ozgur and Gungor 2009). The work in (Shankar and Karypis 2000; Tan et al. 2005) \nis focused on adjusting the value of centroid based with feedback looping, hypothesis \nmargin and weight-adjustment respectively. They try to rectify class Centroid, if it is not \ncalculated accurately. Centroid based classifier performs efficiently as it doesn’t consider \ntraining data each time to decide a test document.\n\nSupport vector machine\n\nSupport vector machine classifier is used for text classification in various studies. It finds \na separation among the data using the annotated training dataset. The margin of sep-\naration between classes, which is known as hyperplane, is used to classify the incom-\ning data. The hyperplane should give maximum separation between the classes. It is \n\n\n\nPage 8 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\napplicable even in the presence of high dimensional feature set representation. It classify \nbased o hyperplane among classes. Like centroid-based, SVM also consider the hyper-\nplane to classify a test document. (Brown et  al. 1997) has compared SVM with artifi-\ncial neural networks for text classification and has found it better. Since it has promising \nresults in text classification, it also performs well for opinion mining. They have also \nclaimed in (Brown et al. 1997) that SVM is better than Naive Bayes and decision trees \nclassification algorithms. However, SVM consumes more resources at the training \nstage. Although, it is efficient with large feature set, Feldman et al.(2011) has shown that \ndimensionality reduction in feature set further improves the performance of SVM. It \nexhibits linear complexity and can scale up to a large dataset.\n\nSVM has a limitation of over-reliance on selection of suitable kernel function. Kernel \nis calculated through Linear, Polynomial, Gaussian or sigmoid methods but they tend to \nbe domain specific. Kernel functions that perform well for one domain may not repeat it \nfor next. Its accuracy is also sensitive to number of training samples close to hyperplane. \nSlack variables are introduced to limit the impact of boundary samples by generalizing \nthe classifier, known as soft margin classification. They also help to avoid over-fitting the \ntraining data.\n\nUnsupervised techniques\n\nThe unsupervised sentiment analysis techniques do not require training data and rather \nrely on semantic orientation. They make use of lexicons to identify the positive or neg-\native semantics of opinion words. The meaning of the word, expressed by its use in a \ncontext is called lexicon. An online or off-line dictionary is consulted for this purpose. \nStatistical analysis techniques are also unsupervised, identifying the orientation of senti-\nment words through statistical evaluations. They require large volume of data for high \naccuracy.\n\nLanguages consists of lexicons that are the words used for a particular sense, and a \ngrammar that connect these lexicons. Part-of-speech rules are used to extract senti-\nment phrases from text document. Search engines are used to identify the orientation \nof sentiment words that are missing in the dictionary. Its polarity is identified through \nthe nearby words brought by search engines. They purely rely on external sources and \ntherefore cannot address the context. Lexicon based techniques perform well for general \ndomains while statistical techniques addresses the context and are useful in specialized \ndomains. The two types of approaches are discussed in detail.\n\nDictionary (Lexicon) based techniques\n\nLexicon based techniques extract opinion lexicons from the document and analyzes \nits orientation without the support of any training data. These techniques process the \nopinion words separately, ignoring the relationship between them. Lexicons refer to the \nsemantic orientation. Lexicons are independent of the source data and therefore it does \nnot fall for over-fitting. But context not addressed either in this approach (Katz et  al. \n2015; Cambria 2013). Search engines are used to find the meaning of unknown opinion \nlexicons. They are searched and the top N results are accepted to identify its orientation. \nThe semantics of lexicons can be categorized as positive or negative with weights rep-\nresenting their strength. This approach struggles with lexicons having domain specific \n\n\n\nPage 9 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\npolarity. For example, good has positive polarity in any type of domain but “heavy \nweight” has positive polarity for bike domain but negative for the domain of electronic \ndevices.\n\nIn its simplest form, sentiment words are split into positive and negative as binary \ndistribution. A more sophisticated approach has fuzzy lexicons, introducing a grey area \nbetween the two categories. These fuzzy lexicons exist in both the classes with a score \nassociated to it, representing the strength of each label. Various manual and semi-auto-\nmatic techniques can be used for building lexicons. Princeton University’s WordNet is \na popular lexicon source available for sentiment analysis. Dictionaries like WordNet, \nextracts synonyms and antonyms for the provided opinion words. Manual cleansing is \nemployed to rectify the lists generated for the unknown sentiment words. These opinion \nwords are used to classify a review as positive or negative.\n\nFixed syntactic patterns are also used for expressing opinions which are composed of \npart-of-speech (POS) tags. The basic idea of this technique is to identify the patterns in \nwhich words co-occur with each other and to exploit those patterns for understanding \nits semantic orientation. One example of such pattern is an adverb followed by an adjec-\ntive. A more sophisticated approach was proposed by (Mohammad and Yang 2011), \nwhich used a WordNet distance based method to determine the sentiment orientation. \nThe distance d(t1, t2) between terms t1 and t2 is the length of the shortest path that con-\nnects them in WordNet, as shown in Eq. 2. The semantic orientation (SO) of an adjective \nterm t is determined by its relative distance from two reference (or seed) terms good and \nbad. The polarity of opinion term t is resolved through eq.\n\nStatistics (Corpus) based techniques\n\nStatistical analysis of large corpus of text can also be used to determine the sentiment \norientation of words. Co-occurrence of words is evaluated without consulting any exter-\nnal support. Two methods are used for this purpose which are point wise mutual infor-\nmation (PMI) and latent semantic analysis (LSA). PMI method for co-occurrence is \ngiven as:\n\nwhere w1 and w2 refers to two words in a given sentence. The main concept behind PMI \nbased techniques is that the semantic orientation of a word has a tendency of being \nclosely related to that of its neighbors. Equation 3 gives the probability of words w1 and \nw2 to co-exist, based on the measure of degree of statistical dependence between the \ntwo. This approach is, however, implemented differently in LSA based techniques. In \nLSA, matrix factorization technique is used with singular value decomposition to dem-\nonstrate the statistical co-occurrence of words. More formally, this process can be speci-\nfied as:\n\n(2)SO(t) =\nd(t, bad)− d(t, good)\n\nd(bad, good)\n\n(3)p(w1,w2) =\np(w1,w2)\n\np(w1) p(w2)\n\n(4)LSA(w) = LSA(w, {+paradigms})− LSA(w, {−paradigms})\n\n\n\nPage 10 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nwhere a word w is passed to LSA with positive and negative paradigms. LSA based tech-\nniques develop a matrix having rows as words and columns as sentences or paragraphs. \nEach cell possesses a weight corresponding to the relation of the word in row with the \nsentence or paragraph in columns. This matrix is decomposed into three matrices using \nsingular value decomposition (SVD).\n\nComplex challenges\n\nOpinion mining is a relatively new area of research and there are open challenges that \nneed to be answered. Some of the challenges are common to opinion mining in general \nwhile others are related to their own sources and context depending upon the domain of \nthe dataset. These issues affect the performance of machine learning techniques, but it \nhas little control on them. Figure 2 gives NLP challenges faced in sentiment analysis, dis-\ntributing them into their logical groups. The groupings are based on the parsing level, at \nwhich these issues occur. The following sub section has detailed discussion on the NLP \nissues.\n\nDocument level\n\nDocument level NLP challenges are the ones that are faced at the document or review \nlevel. They deal in general with the review document or the reviewer style. It is common \nto find reviews that have the information about an object, given in an informal manner. \nCapitalization is over or under used. Spelling mistakes are ignored or words being short-\nened. It makes the analysis very difficult for the automatic techniques to identify features \nand associate them. The unknown words (shortened/miss spelled) are matched with \nsimilar words to identify the aspect or opinion words. Slang specific to a certain region \nare also occasionally used in reviews and discussions. Reviews having sarcastic expres-\nsions are the hardest to deal with. Even though they have the opinion words explicitly \nmen",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1582435,
      "metadata_storage_name": "s40294-016-0016-9.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDI5NC0wMTYtMDAxNi05LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Muhammad Taimoor Khan",
      "metadata_title": "Sentiment analysis and the complex natural language",
      "metadata_creation_date": "2016-02-02T06:54:52Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "customer- agent phone conversation",
        "Complex networks Open Access",
        "Complex Adapt Syst Model",
        "Creative Commons license",
        "users’ senti- ments",
        "domain specific solution",
        "profitable business decisions",
        "various online sources",
        "Complex network based",
        "specific structural information",
        "original author(s",
        "natural language processing",
        "specific open challenges",
        "natural language data",
        "data leakage analysis",
        "Muhammad Taimoor Khan1",
        "sentiment analysis techniques",
        "open issues",
        "author information",
        "language models",
        "influence analysis",
        "bias analysis",
        "ML) techniques",
        "ML techniques",
        "lexicon-based techniques",
        "statistical techniques",
        "sentiment words",
        "Sentiment orientation",
        "Mehr Durrani",
        "Armughan Ali",
        "Irum Inayat",
        "Shehzad Khalid1",
        "Habib Khan4",
        "text classification",
        "subjective statements",
        "subjective attitudes",
        "social data",
        "huge amount",
        "amateur authors",
        "large variety",
        "target entity",
        "Machine learning",
        "definite patterns",
        "high accuracy",
        "feature set",
        "external dictionary",
        "specialized domains",
        "large data",
        "Recent applications",
        "New sub-domains",
        "considerable advancements",
        "standard datasets",
        "evaluation methodology",
        "unrestricted use",
        "appropriate credit",
        "1 Bahria University",
        "Shangrilla Road",
        "Sector E",
        "Full list",
        "social media",
        "data mining",
        "opinionated content",
        "public perception",
        "opinion mining",
        "opinion content",
        "REVIEW Khan",
        "Kamran",
        "Introduction",
        "Pang",
        "Lillian",
        "type",
        "opinions",
        "order",
        "document",
        "NLP",
        "sentiments",
        "people",
        "topic",
        "features",
        "reason",
        "popularity",
        "advice",
        "others",
        "big",
        "hotspot",
        "field",
        "Hai",
        "Abstract",
        "abundance",
        "overfitting",
        "context",
        "Corpus-based",
        "proximity",
        "relationships",
        "platform",
        "meta-data",
        "nature",
        "paper",
        "need",
        "area",
        "better",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "DOI",
        "Correspondence",
        "Islamabad",
        "Pakistan",
        "end",
        "crossmark",
        "org",
        "Page",
        "19Khan",
        "Manufacturers",
        "products",
        "tory",
        "blogs",
        "forums",
        "websites",
        "graph based analysis techniques",
        "various sub streams",
        "different writing styles",
        "multi-class classification problem",
        "ML) based techniques",
        "Natural language processing",
        "many control options",
        "same sentiment polarity",
        "Sentiment analysis techniques",
        "binary classification",
        "complex network",
        "manual techniques",
        "supervised techniques",
        "overall analysis",
        "trend analysis",
        "statistical analysis",
        "complex sentences",
        "automated processes",
        "user support",
        "online datasets",
        "objective statements",
        "Such statements",
        "common-sense knowledge",
        "geographical domains",
        "gender identification",
        "fairy tales",
        "interesting patterns",
        "open challenges",
        "high frequency",
        "same product",
        "later section",
        "detailed discussion",
        "subjective content",
        "online sources",
        "single entity",
        "subjective sentence",
        "sound effects",
        "game X",
        "game Y",
        "objective sentence",
        "sentence types",
        "machine learning",
        "manual tuning",
        "domain experts",
        "large volume",
        "Opinion mining",
        "opinion patterns",
        "single opinion",
        "emotion analysis",
        "text categorization",
        "comparative sentences",
        "multiple opinions",
        "Regular opinions",
        "comparative opinions",
        "extensive use",
        "subjective nature",
        "gle word",
        "individual words",
        "NLP tasks",
        "NLP issues",
        "opinion words",
        "Simple sentences",
        "millions",
        "reviews",
        "rapid",
        "minimal",
        "way",
        "outcome",
        "form",
        "scale",
        "likeness",
        "Cambria",
        "results",
        "applications",
        "business",
        "political",
        "emails",
        "Mohammad",
        "Yang",
        "semantics",
        "date",
        "ior",
        "assumptions",
        "Documents",
        "matching",
        "fact",
        "variety",
        "difference",
        "negations",
        "NOT",
        "sarcastic",
        "compound",
        "implicit",
        "meaning",
        "aspects",
        "comparison",
        "example",
        "Bag",
        "BOW",
        "diverse subjective data sources",
        "seman- tic association",
        "Business Week survey",
        "Many complex systems",
        "complex struc- ture",
        "professional review websites",
        "other media sources",
        "social media websites",
        "The Kelsey group",
        "low quality features",
        "online data sources",
        "E-commerce websites",
        "online studies",
        "effective- ness",
        "Sentiment analysis",
        "environmental modeling",
        "wireless sensors",
        "global properties",
        "degree distribution",
        "multi-partite graphs",
        "different types",
        "large number",
        "current issues",
        "last decade",
        "ferent walks",
        "detailed analysis",
        "unique approaches",
        "popular medium",
        "various properties",
        "various clusters",
        "opinion extraction",
        "online reviews",
        "prospective customers",
        "customers’ feedbacks",
        "ad-hoc networks",
        "blogging styles",
        "external link",
        "various products",
        "adjacent nodes",
        "various entities",
        "user opinions",
        "Results",
        "repetitive",
        "polarity",
        "performance",
        "classifier",
        "cost",
        "efficiency",
        "Effectiveness",
        "precision/recall",
        "measurements",
        "relevance",
        "edges",
        "domains",
        "Niazi",
        "Aoyama",
        "Hussain",
        "information",
        "range",
        "local",
        "Text",
        "pora",
        "structural",
        "weight",
        "centrality",
        "ponents",
        "communities",
        "paths",
        "inter-cluster",
        "reasons",
        "support",
        "topics",
        "Web2.0",
        "thoughts",
        "backgrounds",
        "Katz",
        "users",
        "world",
        "ratings",
        "purchase",
        "Comscore",
        "value",
        "rise",
        "life",
        "events",
        "times",
        "discussion",
        "author",
        "feelings",
        "Chau",
        "Xu",
        "services",
        "packages",
        "promotions",
        "datasets",
        "Qiang",
        "Rob",
        "minded",
        "subjects",
        "experience",
        "articles",
        "Twitter",
        "point wise mutual information",
        "Semantic orientation based techniques",
        "centroid based classifier",
        "health care problems",
        "latent semantic analysis",
        "Lexicon based technique",
        "machine learning classifiers",
        "document level approach",
        "timent analysis techniques",
        "three different levels",
        "unsupervised statistical techniques",
        "text categoriza- tion",
        "Word level analysis",
        "sentiment level analysis",
        "social data analysis",
        "negative opinion words",
        "unsupervised techniques",
        "view point",
        "three levels",
        "sentiment analysis",
        "Semi-supervised techniques",
        "negative bins",
        "sentiment targets",
        "three groups",
        "opinion holder",
        "opinion datasets",
        "following divisions",
        "application areas",
        "multiple degrees",
        "relationship data",
        "Recommender systems",
        "data sources",
        "important aspect",
        "identifi- cation",
        "additional features",
        "dynamic environment",
        "general perception",
        "polar- ity",
        "k-nearest neighbor",
        "starting seeds",
        "Other attributes",
        "text documents",
        "docu- ments",
        "label dataset",
        "test dataset",
        "Sentiment classification",
        "review document",
        "new words",
        "other words",
        "important issues",
        "review article",
        "user data",
        "textual content",
        "previous work",
        "relevant domain",
        "multi-class classification",
        "Section 2",
        "Sect.",
        "Section 5",
        "study",
        "Section 6",
        "authors",
        "positive",
        "Tang",
        "survey",
        "Guellil",
        "Boukhalfa",
        "categories",
        "knowledge",
        "Khan",
        "Khalid",
        "SA",
        "Tuveri",
        "Angioni",
        "Zhang",
        "Liu",
        "rate",
        "training",
        "accuracy",
        "product",
        "strength",
        "Naive-Bayes",
        "pose",
        "occurrence",
        "PMI",
        "small",
        "More",
        "basis",
        "frequency",
        "location",
        "taxonomy",
        "approaches",
        "Fig.",
        "sentence",
        "presence",
        "patterns",
        "Schouten",
        "Frasincar",
        "classes",
        "formance",
        "task",
        "many equally important features",
        "high dimensional feature vectors",
        "various practical applications",
        "Naïve Bayes",
        "annotated feature vectors",
        "text classifica- tion",
        "k-nearest neighbor classifier",
        "fewer training examples",
        "senti- ment label",
        "k nearest neighbors",
        "missing value problem",
        "Naive Bayes classifier",
        "document vector dj",
        "high weights",
        "fewer chances",
        "Text data",
        "document dj",
        "training data",
        "fuzzy region",
        "final decision",
        "prob- abilities",
        "Slack variables",
        "smoothing effect",
        "noisy data",
        "fea- tures",
        "biased approach",
        "major role",
        "transfer learning",
        "classimbalance problem",
        "able value",
        "time constraint",
        "different values",
        "document di",
        "opinion document",
        "test document",
        "new document",
        "prominent features",
        "irrelevant features",
        "target dataset",
        "online classification",
        "processed dataset",
        "NB) classifier",
        "same label",
        "supervised classifiers",
        "expository literature",
        "opinion sentences",
        "pre-processing noise",
        "special case",
        "k-NN problem",
        "higher influence",
        "unrealistic assumption",
        "class ci",
        "class label",
        "k.",
        "C.",
        "labels",
        "probability",
        "confidence",
        "modifications",
        "level",
        "probabilities",
        "Taxonomy",
        "The",
        "logarithm",
        "underflow",
        "contribution",
        "Dai",
        "robustness",
        "samples",
        "solution",
        "Shin",
        "10 % improvement",
        "outliers",
        "opti",
        "External dictionary WordNet",
        "fast kNN model",
        "explicit opinion words",
        "related training examples",
        "sional feature set",
        "actual training data",
        "training data reverse",
        "large feature sets",
        "reduced set",
        "positive examples",
        "regular data",
        "feature vector",
        "feature selection",
        "decision making",
        "different categories",
        "retrieval system",
        "reduction techniques",
        "driving factors",
        "prototype vector",
        "central point",
        "Rocchio algorithm",
        "Cen- troid",
        "positive cases",
        "positive vectors",
        "different domain",
        "Smoothing techniques",
        "weighting scheme",
        "feedback looping",
        "hypothesis margin",
        "training dataset",
        "feature space",
        "sentiment terms",
        "training documents",
        "Centroid based",
        "centroid vector",
        "Centroid evaluation",
        "mum value",
        "distance calculation",
        "Mutual Information",
        "different polarity",
        "higher weights",
        "special characters",
        "Tree-fast k-NN",
        "Centroid classifier",
        "CB classifier",
        "class Centroid",
        "class similarity",
        "threshold",
        "noise",
        "Sreemathy",
        "Balamurugan",
        "curse",
        "dimensionality",
        "Accuracy",
        "Soucy",
        "Mineau",
        "indexing",
        "dj",
        "work",
        "plexities",
        "number",
        "Xia",
        "inverse",
        "pseudo-antonyms",
        "corpus",
        "Arlindo",
        "average",
        "Karypis",
        "sum",
        "Chuang",
        "Lertnattee",
        "Theeramunkong2004",
        "cosine",
        "Hidayet",
        "Tunga",
        "n-grams",
        "shortfall",
        "drawback",
        "Guan",
        "effect",
        "Chizi",
        "Ozgur",
        "Gungor",
        "Shankar",
        "∏",
        "high dimensional feature set representation",
        "decision trees classification algorithms",
        "Support vector machine classifier",
        "unsupervised sentiment analysis techniques",
        "large feature set",
        "cial neural networks",
        "Statistical analysis techniques",
        "suitable kernel function",
        "top N results",
        "incom- ing data",
        "soft margin classification",
        "Lexicon based techniques",
        "senti- ment words",
        "Centroid based classifier",
        "unknown opinion lexicons",
        "Unsupervised techniques",
        "ment phrases",
        "large dataset",
        "statistical evaluations",
        "various studies",
        "sep- aration",
        "Naive Bayes",
        "training stage",
        "dimensionality reduction",
        "sigmoid methods",
        "Kernel functions",
        "training samples",
        "boundary samples",
        "particular sense",
        "speech rules",
        "Search engines",
        "nearby words",
        "external sources",
        "two types",
        "source data",
        "text document",
        "maximum separation",
        "linear complexity",
        "one domain",
        "ative semantics",
        "general domains",
        "semantic orientation",
        "line dictionary",
        "hyperplane",
        "SVM",
        "Brown",
        "resources",
        "Feldman",
        "limitation",
        "over-reliance",
        "selection",
        "Polynomial",
        "Gaussian",
        "impact",
        "use",
        "online",
        "purpose",
        "Languages",
        "grammar",
        "detail",
        "relationship",
        "negative",
        "weights",
        "WordNet distance based method",
        "popular lexicon source",
        "singular value decomposition",
        "machine learning techniques",
        "Fixed syntactic patterns",
        "LSA based techniques",
        "matrix factorization technique",
        "unknown sentiment words",
        "Complex challenges",
        "relative distance",
        "PMI method",
        "Statistical analysis",
        "electronic devices",
        "simplest form",
        "grey area",
        "two categories",
        "Various manual",
        "Princeton University",
        "Manual cleansing",
        "POS) tags",
        "basic idea",
        "shortest path",
        "adjective term",
        "two reference",
        "nal support",
        "Two methods",
        "main concept",
        "statistical dependence",
        "three matrices",
        "new area",
        "little control",
        "sentiment orientation",
        "NLP challenges",
        "opinion term",
        "sophisticated approach",
        "fuzzy lexicons",
        "One example",
        "eq. Statistics",
        "large corpus",
        "statistical co-occurrence",
        "two words",
        "domain specific",
        "bike domain",
        "negative paradigms",
        "positive polarity",
        "good",
        "score",
        "label",
        "Dictionaries",
        "synonyms",
        "antonyms",
        "lists",
        "review",
        "speech",
        "understanding",
        "adverb",
        "t2",
        "length",
        "seed",
        "text",
        "mation",
        "w1",
        "w2",
        "tendency",
        "neighbors",
        "Equation",
        "measure",
        "degree",
        "process",
        "rows",
        "columns",
        "paragraphs",
        "cell",
        "relation",
        "SVD",
        "research",
        "general",
        "sources",
        "dataset",
        "issues",
        "Figure 2",
        "Document level NLP challenges",
        "following sub section",
        "parsing level",
        "review level",
        "logical groups",
        "reviewer style",
        "informal manner",
        "Spelling mistakes",
        "automatic techniques",
        "unknown words",
        "similar words",
        "groupings",
        "object",
        "Capitalization",
        "analysis",
        "aspect",
        "Slang",
        "region",
        "discussions"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 5.0393877,
      "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1373874,
      "metadata_storage_name": "s13673-019-0177-6.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY3My0wMTktMDE3Ny02LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Chunyong Yin ",
      "metadata_title": "Mobile marketing recommendation method based on user location feedback",
      "metadata_creation_date": "2019-04-05T10:16:31Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "current mobile marketing recommendation system",
        "Sequential behavior Open Access",
        "Mobile marketing recommendation method",
        "users’ timing preference characteristics",
        "Location-based mobile marketing recommendation",
        "potential mobile user preferences",
        "Creative Commons license",
        "convolutional neural network model",
        "users’ location feedback behavior",
        "user location feedback data",
        "mobile recommendation system",
        "traditional recommendation models",
        "user preference information",
        "creat iveco mmons",
        "users’ location-based behaviors",
        "mobile information overload",
        "user-product binary matrix",
        "original author(s",
        "different geographical locations",
        "shopping location information",
        "famous e-commerce platforms",
        "physical store products",
        "different time windows",
        "users’ location information",
        "Hum. Cent. Comput",
        "mobile network",
        "information recommendation",
        "different preferences",
        "mobile users",
        "convolutional model",
        "author information",
        "irrelevant information",
        "binary relationships",
        "different dimensions",
        "behavioral data",
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "recent years",
        "e-commerce industry",
        "one hand",
        "various products",
        "other hand",
        "large amount",
        "next consumer",
        "recommended methods",
        "third factor",
        "limited check",
        "timeli- ness",
        "hot spots",
        "accuracy rate",
        "recall rate",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Yin",
        "Inf. Sci.",
        "Communication Engineering",
        "Full list",
        "individual needs",
        "doi.org",
        "orcid.org",
        "experimental results",
        "Changsha University",
        "Introduction",
        "popularization",
        "Internet",
        "Alibaba",
        "zon",
        "home",
        "sellers",
        "goods",
        "costs",
        "wealth",
        "consumption",
        "case",
        "advantage",
        "terms",
        "Abstract",
        "attribute",
        "role",
        "paper",
        "LBCNN",
        "extractor",
        "classifier",
        "Keywords",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "jinwang",
        "csust",
        "2 School",
        "Computer",
        "Science",
        "Technology",
        "China",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "17Yin",
        "means",
        "problem",
        "implied feature-based cognitive feature collaborative filter- ing",
        "location-aware based generation probability model",
        "collaborative filtering methods",
        "biological neural networks",
        "Convolutional neural networks",
        "advanced abstract features",
        "loca- tion information",
        "accurate personalized recommendations",
        "small-scale training data",
        "detailed comparative analysis",
        "user informa- tion",
        "mobile marketing recommendation",
        "original input features",
        "business location information",
        "user preference model",
        "mobile recommendation research",
        "feature size",
        "deep learning algorithms",
        "mobile users’ preferences",
        "ing models",
        "deep model",
        "DL) model",
        "shallow model",
        "multimedia recommendations",
        "mobile environments",
        "accurate prediction",
        "local features",
        "overall features",
        "two-dimensional features",
        "user preferences",
        "location recommendation",
        "convolutional layer",
        "spatial information",
        "product information",
        "user information",
        "hot topic",
        "service resources",
        "different assumptions",
        "spatial activities",
        "location movement",
        "negative samples",
        "semantic content",
        "data size",
        "mation analysis",
        "long-term interest",
        "time stamps",
        "great achievements",
        "speech tasks",
        "novel field",
        "interventional optimization",
        "artificial intelligence",
        "great breakthroughs",
        "similar effects",
        "Weight sharing",
        "great advantages",
        "user needs",
        "positional relevance",
        "location relevance",
        "first layer",
        "previous layer",
        "convolution kernel",
        "pooling layer",
        "context information",
        "ventional methods",
        "user models",
        "many researches",
        "two layers",
        "network structure",
        "shortcomings",
        "Zhu",
        "retrograde",
        "approach",
        "Yin",
        "LA-LDA",
        "scoring",
        "items",
        "studies",
        "distance",
        "merchant",
        "area",
        "proximity",
        "importance",
        "gap",
        "core",
        "Lian",
        "impact",
        "author",
        "project",
        "Lee",
        "visual",
        "study",
        "aspects",
        "machine",
        "Experiments",
        "characteristics",
        "CNN",
        "difficulty",
        "number",
        "weights",
        "neuron",
        "Hum",
        "Cent",
        "Comput",
        "images",
        "location-based",
        "Location-based mobile marketing recommendation model",
        "Content‑based recommendation method",
        "convolutional neural network models",
        "Representative content-based recommendation systems",
        "general products recommendation system",
        "Traditional recommendation method",
        "traditional recommendation algorithm",
        "content-based recommendation algorithm",
        "similar Top-N products",
        "timing preference characteristics",
        "interest feature vector",
        "Content-based information filtering",
        "Experimental analysis” section",
        "item feature vector",
        "project feature vector",
        "other existing methods",
        "cosine similarity algorithm",
        "Related work” section",
        "user feature vector",
        "users’ timing preferences",
        "User description file",
        "feature models",
        "Content-based methods",
        "content filtering",
        "content characteristics",
        "other methods",
        "user-product information",
        "collaborative filtering",
        "feedback information",
        "hybrid methods",
        "historical content",
        "training sample",
        "next moment",
        "time window",
        "overall preferences",
        "Top-K sample",
        "four sections",
        "Necessary definitions",
        "specific implementation",
        "future progress",
        "current chapter",
        "three parts",
        "time series",
        "text documents",
        "accurate comparisons",
        "different texts",
        "lowing aspects",
        "Hum. Cent",
        "project profile",
        "user u",
        "similarity calculation",
        "sample features",
        "cast results",
        "effective application",
        "calculation formula",
        "two-class problem",
        "purchase behavior",
        "category",
        "order",
        "merchandise",
        "length",
        "test",
        "Remain",
        "sion",
        "strengths",
        "weaknesses",
        "plans",
        "establishment",
        "differences",
        "research",
        "threshold",
        "a.",
        "topics",
        "projects",
        "purchasing",
        "operation",
        "addition",
        "modulus",
        "Lops",
        "Gemmis",
        "Semeraro",
        "content filtering based recommendation method",
        "m * n matrix R",
        "traditional data mining method",
        "user historical behavior data",
        "traditional collaborative filtering method",
        "likely N objects",
        "user behavior data",
        "various draw- backs",
        "nearest neigh- bor",
        "conven- tional method",
        "association rule algorithm",
        "higher evaluation algorithms",
        "hybrid recommendation algorithm",
        "similar user sets",
        "mutual user relationships",
        "content-based recommendation method",
        "Hybrid recommendation method",
        "other similar users",
        "different recommendation algorithms",
        "The matrix  Umn",
        "content-based filtering",
        "other method",
        "hybrid method",
        "matrix decomposition",
        "different relationships",
        "One method",
        "new content",
        "evaluation standard",
        "association rules",
        "recommendation problem",
        "standard recommendation",
        "two algorithms",
        "different ways",
        "target user",
        "user neighbor",
        "cold-start issues",
        "same type",
        "same partition",
        "expert research",
        "three steps",
        "recommendation results",
        "ommended items",
        "normal methods",
        "supervised learning",
        "online stores",
        "informa- tion",
        "new users",
        "two types",
        "two specific",
        "compensa- tion",
        "different values",
        "common features",
        "rating sparsity",
        "erence information",
        "new projects",
        "mendations",
        "availability",
        "context",
        "party",
        "requirements",
        "merits",
        "quality",
        "memory",
        "following",
        "collection",
        "feedback",
        "range",
        "U11",
        "U1n",
        "U2n",
        "Um1",
        "classification",
        "survey",
        "Amazon",
        "personalized",
        "customer",
        "ratings",
        "techniques",
        "performance",
        "ally",
        "degree",
        "improvement",
        "complexity",
        "different timing behavior characteristics",
        "good recom- mendations",
        "relevant professional research",
        "T-time consumer behavior",
        "behavioral counting feature",
        "duplication counting feature",
        "feature statistics window",
        "historical behavior data",
        "timing behavior data",
        "user position feedback",
        "similar behavioral items",
        "collaborative filtering algorithm",
        "next purchase behavior",
        "feature statistics method",
        "user behavior record",
        "time series behavior",
        "timing recommendation model",
        "user interface layer",
        "U, V data",
        "high quality rules",
        "last behavior",
        "behavior count",
        "Different actions",
        "different times",
        "U data",
        "next section",
        "current window",
        "feature group",
        "product purchase",
        "recommendation process",
        "recommendation systems",
        "ready-made data",
        "data sets",
        "many years",
        "core idea",
        "rule algorithms",
        "total project",
        "disjoint sets",
        "association rule",
        "two criteria",
        "transac- tions",
        "key- word",
        "description layer",
        "main function",
        "strong dependence",
        "most systems",
        "time information",
        "time label",
        "Markov chain",
        "full use",
        "prediction problem",
        "time T",
        "three groups",
        "processing manner",
        "cumulative measure",
        "keyword layer",
        "user layer",
        "unknown items",
        "traditional algorithm",
        "preference features",
        "corresponding features",
        "location visit",
        "total number",
        "∩V",
        "V.",
        "∪ V",
        "business",
        "effect",
        "quantity",
        "focus",
        "transactions",
        "statement",
        "strength",
        "support",
        "confidence",
        "ratio",
        "Formula",
        "interest",
        "other",
        "TOP-N",
        "presentation",
        "attributes",
        "dependencies",
        "keywords",
        "resource",
        "users",
        "dataset",
        "explanation",
        "Construction",
        "Fig.",
        "example",
        "user1",
        "Table",
        "False",
        "behaviors",
        "Location‑based mobile marketing recommendation model",
        "Counting feature Mean feature Ratio feature",
        "location-based mobile marketing recom",
        "Table 1 Characteristic system diagram",
        "collaborative filtering calculation process",
        "False True User feature",
        "K user preference feature",
        "pooled feature vector",
        "evaluation crite- rion",
        "ior preference feature",
        "duplication count feature",
        "different feature maps",
        "repetitive behavioral data",
        "Specific implementa- tion",
        "pool- ing layer",
        "multi-window convolutional layer",
        "real-world work applications",
        "location visit behavior",
        "N time windows",
        "multi-window convolution layer",
        "user characteristics group",
        "products’ total visit",
        "user position features",
        "counting features",
        "Feature group",
        "True False",
        "feature expression",
        "product location",
        "behavioral count",
        "network structures",
        "input feature",
        "Model framework",
        "different lengths",
        "Product feature",
        "mean-type features",
        "output layer",
        "time axis",
        "input layer",
        "total behavior",
        "current research",
        "low difficulty",
        "mendation quality",
        "following sections",
        "important aspects",
        "next step",
        "evant definitions",
        "timing sensitivity",
        "timing behav",
        "four layers",
        "two-dimensional plane",
        "ture plane",
        "The model",
        "Specific information",
        "average number",
        "user-product behavior",
        "product preferences",
        "relevant definition",
        "K.",
        "Definition 1",
        "Definition 2",
        "visits",
        "activity",
        "popularity",
        "method",
        "usable",
        "achievement",
        "speed",
        "avgui",
        "Ij",
        "True/False",
        "sure",
        "correctness",
        "basis",
        "above",
        "analysis",
        "eigenvector",
        "commodity",
        "matrix",
        "new location-based marketing resources recommendation",
        "location-based mobile marketing resources",
        "M convolution kernels",
        "convolu- tion kernel",
        "gradient descent method",
        "convolution kernel w",
        "C × M matrix",
        "nonlinear activation function",
        "kth implicit class",
        "implicit semantic model",
        "maximum pooling operation",
        "latent factor vector",
        "training data set",
        "latent factor model",
        "traditional CNN parameters",
        "likelihood probability value",
        "i-th feature fi",
        "kth product",
        "convolutional kernel",
        "training method",
        "Dropout method",
        "CNN inputs",
        "i-th category",
        "loss function",
        "i-th sample",
        "Feature map",
        "pooled feature",
        "training results",
        "window length",
        "offset term",
        "maximum features",
        "C categories",
        "M-dimensional vector",
        "k-th offset",
        "real category",
        "two items",
        "same time",
        "same class",
        "pTu qi",
        "hidden classes",
        "Specific implementation",
        "two processes",
        "two parts",
        "top module",
        "historical data",
        "other module",
        "second pro",
        "traditional LFM",
        "L2-norm regularization",
        "smoothing problems",
        "training process",
        "dation process",
        "model parameters",
        "Probability distribution",
        "real number",
        "weight parameter",
        "many users",
        "user interest",
        "∑C",
        "j.",
        "framework",
        "ReLu",
        "Tanh",
        "Definition",
        "network",
        "fk",
        "bk",
        "teristic",
        "Zeiler",
        "fitting",
        "neurons",
        "behavior",
        "high",
        "Pool_feature",
        "log",
        "rui",
        "relationship",
        "outputs",
        "excessive",
        "σ",
        "θ",
        "existing authoritative standard training set",
        "Gradient Boosting Regression Tree Model",
        "mining users’ temporal behavior characteristics",
        "Linear Logistic Regression Classification Model",
        "offline training model phase",
        "location-based marketing resources",
        "convolutional neural network",
        "hyper parameter settings",
        "real-time recommendation stage",
        "LBCNN model structure",
        "Random Forest Model",
        "past behavior data",
        "matrix decomposition method",
        "network model parameters",
        "LFM training data",
        "user bias item",
        "item offset item",
        "regression analysis",
        "linear model",
        "classification models",
        "model training",
        "training section",
        "training features",
        "behavior information",
        "previous data",
        "language model",
        "same model",
        "real-time performance",
        "LFM results",
        "sion coefficient",
        "specification results",
        "next question",
        "two parameters",
        "intrinsic property",
        "positive samples",
        "negative sample",
        "first thing",
        "historical score",
        "implied tag",
        "Experimental analysis",
        "timing preferences",
        "same conditions",
        "Vector Machine",
        "Experimental tool",
        "sklearn kit",
        "L2 regular",
        "regularization term",
        "regularization coefficient",
        "user tag",
        "rating score",
        "User information",
        "kind",
        "thinking",
        "sparseness",
        "analyses",
        "solution",
        "premise",
        "mula",
        "calculation",
        "puk",
        "average",
        "qik",
        "public",
        "experience",
        "latter",
        "overfitting",
        "Description",
        "input",
        "problems",
        "output",
        "L1-norm",
        "part",
        "update",
        "background",
        "people",
        "products",
        "advance",
        "period",
        "advantages",
        "several",
        "LR",
        "Support",
        "SVM",
        "GBDT",
        "8 h",
        "λ",
        "∑",
        "lion users’ various behaviors",
        "mobile recommendation algorithm contest",
        "radial basis kernel function",
        "personalized recommendation model",
        "feature segmentation standard",
        "random feature ratio",
        "real business sce",
        "product category information",
        "data set",
        "Behavior types",
        "behavior record",
        "behavior time",
        "c. RF",
        "learning rate",
        "maximum depth",
        "Alibaba Group",
        "offline type",
        "RBF",
        "gamma",
        "trees",
        "entropy",
        "experiment",
        "1 month",
        "2,876,947 items",
        "clicks",
        "shopping",
        "carts",
        "purchases",
        "hour",
        "online",
        "nario",
        "subset"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.369608,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1344318,
      "metadata_storage_name": "s40537-019-0210-7.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDIxMC03LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Taiwo Kolajo ",
      "metadata_title": "Big data stream analysis: a systematic literature review",
      "metadata_creation_date": "2019-06-04T14:40:29Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "effective resource allocation strategy",
        "existing data mining tools",
        "big data stream tools",
        "data stream computing mode",
        "big data streaming tools",
        "big data streaming ana",
        "Big data stream analysis",
        "big data stream computing",
        "big data streams analysis",
        "big data batch computing",
        "Creative Commons license",
        "big data computing",
        "big data tool",
        "J Big Data",
        "several computational challenges",
        "process- ing requirements",
        "inherent dynamic characteristics",
        "Three major databases",
        "first search string",
        "standard benchmark dataset",
        "creat iveco mmons",
        "load balancing issues",
        "nificant research efforts",
        "technologies Open Access",
        "SURVEY PAPER Kolajo",
        "empirical analysis",
        "real-time analysis",
        "future computing",
        "data sources",
        "parallelization issues",
        "research questions",
        "literature review",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "two types",
        "huge amount",
        "methodical approach",
        "global view",
        "exclusion criteria",
        "preprocessing stage",
        "iterative jobs",
        "scalable frameworks",
        "growing size",
        "unrestricted use",
        "appropriate credit",
        "original author",
        "Information Sciences",
        "Covenant University",
        "Full list",
        "author information",
        "doi.org",
        "orcid.org",
        "information technology",
        "large volume",
        "great velocity",
        "systematic review",
        "initial 2295 papers",
        "Taiwo Kolajo",
        "47 papers",
        "Introduction",
        "Advances",
        "high-velocity",
        "ability",
        "nature",
        "terms",
        "variety",
        "veracity",
        "volatility",
        "value",
        "new",
        "trend",
        "Abstract",
        "fact",
        "number",
        "applications",
        "methods",
        "techniques",
        "rigorous",
        "comparisons",
        "Scopus",
        "ScienceDirect",
        "EBSCO",
        "journals",
        "conferences",
        "entities",
        "IEEE",
        "ACM",
        "SpringerLink",
        "Elsevier",
        "inclusion",
        "study",
        "privacy",
        "attention",
        "key",
        "features",
        "lytics",
        "conclusion",
        "algorithms",
        "complexity",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "changes",
        "Correspondence",
        "fulokoja",
        "1 Department",
        "Computer",
        "Ota",
        "Nigeria",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "Page",
        "30Kolajo",
        "scalable com- puting platforms",
        "big data streaming analytics",
        "big data stream analysis",
        "Big data batch processing",
        "Streaming processing frameworks",
        "stream processing paradigms",
        "Stream processing solutions",
        "data flow graph",
        "real-time application scenarios",
        "real-time, high volume",
        "time data analysis",
        "real-time data stream",
        "batch computing",
        "stream computing",
        "stream processor",
        "high-velocity flow",
        "incoming data",
        "data generating",
        "market data",
        "real-time sources",
        "changing conditions",
        "common understanding",
        "ben- efit",
        "scientific community",
        "key issues",
        "detailed evaluation",
        "massive amount",
        "high- velocity",
        "multiple sources",
        "low latency",
        "new sources",
        "location services",
        "mobile devices",
        "sensor pervasiveness",
        "fundamental assumption",
        "potential value",
        "parallel architectures",
        "decision-making process",
        "static questions",
        "continuous queries",
        "diverse sources",
        "sideration availability",
        "fault tolerance",
        "infinite tuple",
        "actionable results",
        "interconnected streams",
        "related work",
        "research works",
        "Research method",
        "crucial need",
        "real contrasts",
        "ming paradigm",
        "Discussion” section",
        "Result” section",
        "addition",
        "output",
        "low-latency",
        "seconds",
        "demand",
        "reason",
        "huge",
        "organisations",
        "businesses",
        "paper",
        "purpose",
        "overview",
        "findings",
        "implications",
        "practice",
        "update",
        "state",
        "areas",
        "challenges",
        "rest",
        "Background",
        "information",
        "Limitation",
        "Conclusion",
        "ubiquity",
        "Internet",
        "Things",
        "Sensors",
        "freshness",
        "Storm",
        "Kafka",
        "Spark",
        "Table",
        "motion",
        "essence",
        "fly",
        "scalability",
        "assimilation",
        "production",
        "operations",
        "Fig.",
        "Dimension Batch processing Streaming processing Input Data chunks",
        "Hardware Multiple CPUs Typical single limited amount",
        "big data stream computing environments",
        "big data streaming analysis",
        "domain Web mining",
        "streaming analytics system",
        "big data streams",
        "Data flow graph",
        "high data rates",
        "data processing node",
        "memory Storage Store",
        "new incoming data",
        "ble efficient operations",
        "typical example",
        "multiple rounds",
        "Such processing",
        "single pass",
        "new data",
        "missing data",
        "data normalization",
        "distributed system",
        "High fault-tolerance",
        "life-critical systems",
        "data tuples",
        "Data size",
        "new information",
        "new tuple",
        "new trends",
        "model predictions",
        "feature extraction",
        "external feeds",
        "non-trivial portion",
        "traffic monitoring",
        "sensor networks",
        "Key issues",
        "useful knowledge",
        "current happenings",
        "speedy manner",
        "organisa- tions",
        "load balancing",
        "privacy issues",
        "exponential growth",
        "computer resources",
        "research efforts",
        "effective resource",
        "cation strategy",
        "com- plexity",
        "small number",
        "different datasets",
        "component failure",
        "time-sensitive processes",
        "security threats",
        "data Time",
        "latest tuples",
        "analytic applications",
        "sliding window",
        "milliseconds Applications",
        "logical container",
        "main challenges",
        "Fault‑tolerance",
        "integration technique",
        "results",
        "operators",
        "models",
        "access",
        "idea",
        "duplicates",
        "parsing",
        "windows",
        "Table 1",
        "Comparison",
        "updates",
        "advance",
        "passes",
        "figure",
        "need",
        "order",
        "problems",
        "performance",
        "timeliness",
        "consistency",
        "heterogeneity",
        "incompleteness",
        "accuracy",
        "The",
        "way",
        "processors",
        "Moore",
        "law",
        "fore",
        "views",
        "interruption",
        "big data stream computing system",
        "organisational resource management specifi",
        "big data stream dataset",
        "Big data stream analytics",
        "good multiple instances replication",
        "big data analytics literature",
        "big data analytics methods",
        "good system structure",
        "distributing envi- ronment",
        "effective tech- niques",
        "Big data streams",
        "competent data presentation",
        "national Data Cooperation",
        "big data analysis",
        "data streams changes",
        "partial data streams",
        "previous research efforts",
        "big data streaming",
        "big data challenges",
        "systematic literature review",
        "High throughput Decision",
        "big threat",
        "streaming analytics",
        "streaming” analytics",
        "streaming data",
        "natural disaster",
        "continuous processing",
        "local views",
        "accessi- bility",
        "meaningful content",
        "Load balancing",
        "load shedding",
        "peak loads",
        "average load",
        "global centre",
        "entire information",
        "needs protection",
        "main objectives",
        "future observations",
        "processing algorithms",
        "stream-specific requirements",
        "various tools",
        "research focus",
        "business values",
        "high consistency",
        "high accuracy",
        "main challenge",
        "communicating nodes",
        "individual privacy",
        "ent characteristics",
        "Related work",
        "The study",
        "reviews",
        "fraud",
        "tures",
        "platforms",
        "architecture",
        "minimal",
        "latency",
        "stability",
        "Heterogeneity",
        "semantics",
        "granularity",
        "correlate",
        "real-time",
        "diversity",
        "hierarchy",
        "resources",
        "variance",
        "result",
        "respect",
        "sub-graph",
        "replicas",
        "portion",
        "issue",
        "opportunities",
        "IDC",
        "half",
        "volume",
        "velocity",
        "variability",
        "consideration",
        "section",
        "technologies",
        "Authors",
        "definitions",
        "types",
        "technology",
        "cally",
        "four big data streaming tools",
        "big data stream analysis tools",
        "big data stream literature review",
        "authoritative, full-text scientific, technical",
        "data stream analysis reviews",
        "big data stream framework",
        "big data stream processing",
        "big data stream algorithms",
        "big data stream analytics",
        "three standard database indexes",
        "big data stream technologies",
        "big data analytics",
        "machine learning algorithms",
        "full-text data- bases",
        "large data- base",
        "smart intuitive functionality",
        "electronic journal service",
        "big data technologies",
        "academic journal articles",
        "leading information solution",
        "systematic mapping method",
        "following research questions",
        "good search string",
        "Data sources",
        "four categories",
        "peer-reviewed literature",
        "citation database",
        "standard databases",
        "information professionals",
        "bibliographic database",
        "empirical research",
        "same vein",
        "particular focus",
        "anomaly detection",
        "tech- niques",
        "clear explanation",
        "Relevant publications",
        "Science Direct",
        "hensive overview",
        "research output",
        "social sciences",
        "largest abstract",
        "open access",
        "health publications",
        "14 million publications",
        "Life Sciences",
        "Physical Sciences",
        "Health Sciences",
        "wide range",
        "academic researchers",
        "application areas",
        "healthcare profes",
        "evaluation techniques",
        "peer-reviewed journals",
        "3800 journals",
        "status",
        "survey",
        "scope",
        "comprehensive",
        "differences",
        "concept",
        "capabilities",
        "limitations",
        "strengths",
        "benchmarks",
        "population",
        "son",
        "intervention",
        "outcome",
        "keywords",
        "searches",
        "EBSCOhost",
        "rich",
        "abstracts",
        "citations",
        "36,377 titles",
        "11,678 publishers",
        "world",
        "arts",
        "humanities",
        "students",
        "teachers",
        "35,000 books",
        "Engineering",
        "porate",
        "Third Search string refinement result",
        "free online professional network",
        "First search string result",
        "Second search string result",
        "Scopus ScienceDirect EBSCOhost Total",
        "major academic publishers",
        "nine (9) search strings",
        "high impact journals",
        "111 seemingly relevant papers",
        "Inclusion criteria Papers",
        "Table 5 Final Selection",
        "Further refinement",
        "Data retrieval",
        "easy analysis",
        "high-quality e-books",
        "100 million publications",
        "secondary source",
        "rich databases",
        "Boolean ‘OR",
        "2295 arti- cles",
        "three databases",
        "computer science",
        "subject domain",
        "quick overview",
        "Microsoft Excel",
        "three categories",
        "black colour",
        "similar investigations",
        "following categories",
        "primary study",
        "source language",
        "relevant” papers",
        "11 million researchers",
        "year range",
        "total number",
        "peer-reviewed conferences",
        "recent papers",
        "16,711 journals",
        "Table 2",
        "Table 3",
        "Table 4",
        "1989 papers",
        "315 papers",
        "111 papers",
        "45 papers",
        "18 papers",
        "magazine",
        "titles",
        "60,000 audiobooks",
        "iv.",
        "ResearchGate4",
        "scientists",
        "collaborators",
        "authors",
        "subscription",
        "set",
        "reseaarchgate",
        "sources",
        "interest",
        "stage",
        "PDF",
        "introduction",
        "green",
        "red",
        "colours",
        "end",
        "workshops",
        "technical",
        "symposium",
        "case",
        "part",
        "English",
        "contributions",
        "900,000",
        "1500",
        "alternative big data streaming solutions",
        "effective data management decisions",
        "spike load profile platform",
        "Big data stream platforms",
        "Streaming data sources",
        "many NoSQL databases",
        "specific application interfaces",
        "data loading procedure",
        "enterprise technology vendors",
        "open source community",
        "open source solutions",
        "Workload profile",
        "proprietary solutions",
        "stream applications",
        "high-velocity data",
        "data stores",
        "recent technology",
        "Such platforms",
        "platform distribution",
        "Data access",
        "Several tools",
        "sary tools",
        "single flow",
        "growing demand",
        "pro- jection",
        "different structures",
        "different ways",
        "CAP theorem",
        "consistent loads",
        "consistent flows",
        "web-based services",
        "soft- ware",
        "critical functions",
        "Latency requirement",
        "minimal delay",
        "key-value stores",
        "memory solution",
        "licensing issues",
        "limited maturity",
        "developer communities",
        "modification challenges",
        "large datasets",
        "large scale",
        "network partition",
        "service deployment",
        "service cloud",
        "premise approach",
        "easy integration",
        "consistency requirement",
        "careful selection",
        "serialization technologies",
        "execution",
        "functionalities",
        "response",
        "factors",
        "Shape",
        "capturing",
        "storing",
        "rep",
        "instance",
        "room",
        "flexibility",
        "storage",
        "users",
        "Availability",
        "presence",
        "break",
        "scenario",
        "requests",
        "Infrastructure",
        "option",
        "processing",
        "predictable",
        "workloads",
        "spikes",
        "combination",
        "go",
        "Tables",
        "pricing",
        "innovation",
        "development",
        "lack",
        "support",
        "outdating",
        "problem",
        "big data stream analysis Tools",
        "Multinomial latent dirichlet allocation",
        "Elastic streaming processing engine",
        "real-time social media data",
        "Microsoft azure stream analytics",
        "old-based stream clustering approaches",
        "Incremental algorithm threshold setting",
        "Table 6 Open source tools",
        "social media analysis",
        "social media streams",
        "high dimensional data",
        "Markov Random Field",
        "Sentiment brand monitoring",
        "IBM InfoSphere streams",
        "Density-based clustering algorithm",
        "Voltage clustering algorithm",
        "maximum similarity threshold",
        "little research efforts",
        "incremental clustering approaches",
        "Online Spherical K-means",
        "Incremental approaches",
        "Splunk stream",
        "incoming stream",
        "data grouping",
        "Proprietary tools",
        "WSO2 analytics",
        "Microsoft StreamInsight",
        "clustering algorithms",
        "Spark streaming",
        "Complete Clustering",
        "hierarchical clustering",
        "online clustering",
        "Research Question",
        "true costs",
        "Apache storm",
        "Apache Samza",
        "Apache Aurora",
        "Apache Kylin",
        "dynamic nature",
        "desirable number",
        "prior knowledge",
        "scalable graph",
        "limited space",
        "apriori number",
        "Google MillWheel",
        "TIBCO StreamBase",
        "Kyvos insights",
        "Lambda architecture",
        "Much work",
        "fragmentation issues",
        "tive approach",
        "static values",
        "metric learning",
        "partitioning algorithms",
        "technology Article",
        "balanced partitioning",
        "time window",
        "Condensed Clusters",
        "existing clusters",
        "Threshold-based techniques",
        "Table 8 Methods",
        "Table 7",
        "understanding",
        "benefits",
        "BlockMon",
        "NoSQL",
        "Photon",
        "MavEStream",
        "EsperTech",
        "Redis",
        "C-SPARQL",
        "SAMOA",
        "CQELS",
        "ETALIS",
        "XSEQ",
        "k-median",
        "k-medoid",
        "expectation-maximization",
        "tendency",
        "decisions",
        "DenStream",
        "OpticStream",
        "Exclusive",
        "outliers",
        "HDDStream",
        "PreDeCon-Stream",
        "PKS-Stream",
        "memory",
        "face",
        "CodeBlue",
        "Anodot",
        "Cloudet",
        "Numenta",
        "Artemis",
        "Striim",
        "AtScale",
        "efficiency",
        "SPADE",
        "LSML",
        "KTS",
        "Dynamic prime-number based security verification",
        "User profile vector update algorithm",
        "Incremental MI outlier detection algorithm",
        "Singular spectrum matrix completion",
        "Temporal fuzzy concept analysis",
        "Tag assignment stream clustering",
        "QRS detection algorithm",
        "cloud computing algorithm",
        "Parallel K-means clustering",
        "Locality sensitive hashing",
        "Forward chaining rule",
        "Continuous query processing",
        "Multi-query optimization strategy",
        "Outlier method",
        "Density cognition",
        "Inc I-MLOF",
        "Adaptive windowing",
        "online ensemble",
        "Nearest neighbour",
        "Markov chains",
        "LSH",
        "TASC",
        "StreamMap",
        "CluStream",
        "HPClustering",
        "D-Stream",
        "DCStream",
        "P-Stream",
        "ADStream",
        "CQR",
        "FPSPAN-growth",
        "OMCA",
        "MQOS",
        "automata",
        "VPA",
        "AWOE",
        "K-anonymity",
        "closeness",
        "ECM-sketch",
        "Block-QuickSort-AdjacentJobMatch",
        "Block-QuickSort-OverlapReplicate"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.029126,
      "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like precision i",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 514849,
      "metadata_storage_name": "s13173-013-0117-7.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzE3My0wMTMtMDExNy03LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": null,
      "metadata_title": null,
      "metadata_creation_date": "2013-10-11T02:33:25Z",
      "keyphrases": [
        "São José dos Campos",
        "J Braz Comput Soc",
        "Ciências Matemáticas",
        "The Brazilian Computer Society",
        "São Paulo",
        "São Carlos",
        "Ciência e",
        "Paulo Henrique Pisani",
        "Ana Carolina Lorena",
        "typ- ing rhythm",
        "P. H. Pisani",
        "A. C. Lorena",
        "behavioral biometric technology",
        "effective security mechanism",
        "Behavioral intrusion detection",
        "formal review protocol",
        "intrusion detection systems",
        "user profil- ing",
        "secure authentication methods",
        "systematic review method",
        "Computação",
        "behavioral analysis",
        "communication systems",
        "security problems",
        "sophisticated methods",
        "The process",
        "Systematic reviews",
        "keystroke dynamics",
        "Abstract Computing",
        "data exposure",
        "ric technologies",
        "possible technologies",
        "future researches",
        "rigorous procedure",
        "ficial intelligence",
        "performance measures",
        "mark datasets",
        "digital identities",
        "greater worries",
        "information exposure",
        "several activities",
        "online banking",
        "new momentum",
        "personal information",
        "system initialization",
        "several aspects",
        "regular behavior",
        "great variety",
        "rigorous method",
        "initial authentication",
        "possible way",
        "Instituto de",
        "Universidade Federal",
        "wider dissemination",
        "identity theft",
        "imate user",
        "potential intrusions",
        "SURVEY PAPER",
        "user authentication",
        "DOI",
        "life",
        "issue",
        "work",
        "users",
        "order",
        "area",
        "definition",
        "results",
        "state",
        "art",
        "sifiers",
        "features",
        "Keywords",
        "Biometrics",
        "ICMC",
        "USP",
        "mail",
        "phpisani",
        "Tecnologia",
        "ICT",
        "UNIFESP",
        "aclorena",
        "1 Introduction",
        "increased",
        "net",
        "commerce",
        "term",
        "crime",
        "someone",
        "scenario",
        "case",
        "example",
        "intruders",
        "session",
        "ways",
        "concept",
        "fact",
        "models",
        "profile",
        "identification",
        "deviations",
        "18",
        "24",
        "10",
        "S N User profile Store profile",
        "regular typing rhythm",
        "possible devia- tions",
        "keystroke dynam- ics",
        "system audit logs",
        "command line use",
        "user behavior pattern",
        "systematic bibliographic reviews",
        "Behavioural intrusion detection",
        "two major steps",
        "intrusion detection system",
        "possible incidents",
        "behaviour pattern",
        "anomaly detection",
        "user profiling",
        "same user",
        "systematic procedure",
        "behavioural IDS",
        "other researchers",
        "other areas",
        "software engineering",
        "artificial intelligence",
        "remaining sections",
        "basic concepts",
        "security policies",
        "acceptable use",
        "security practices",
        "mod- els",
        "basic flow",
        "key issue",
        "similar values",
        "keyboard input",
        "user behaviour",
        "User Training",
        "review method",
        "normal behaviour",
        "valuable information",
        "information security",
        "Training phase",
        "explicit protocols",
        "new concept",
        "Application usage",
        "same time",
        "Preliminary results",
        "An IDS",
        "name",
        "formal",
        "conduction",
        "way",
        "validation",
        "incidence",
        "bias",
        "problem",
        "medicine",
        "number",
        "benefits",
        "computing",
        "paper",
        "tributes",
        "aim",
        "Sect.",
        "process",
        "conclusions",
        "2 Background",
        "events",
        "computer",
        "signals",
        "violations",
        "threats",
        "analysis",
        "type",
        "alarms",
        "Recognition",
        "Fig.",
        "Figure",
        "matching",
        "aspects",
        "questions",
        "group",
        "monitoring",
        "systems audit logs",
        "lower error rates",
        "support vector machines",
        "Two distinctive processes",
        "machine learning algorithms",
        "Systematic literature review",
        "two ways",
        "several algorithms",
        "other aspects",
        "continuous authentication",
        "mail behav",
        "less effort",
        "first process",
        "second process",
        "neural networks",
        "less susceptibility",
        "Software Engineering",
        "major phases",
        "first phase",
        "second phase",
        "secondary study",
        "third phase",
        "final report",
        "check- ing",
        "specific knowledge",
        "important part",
        "primary studies",
        "eval- uation",
        "essential part",
        "other researches",
        "3 Systematic review",
        "Protocol evaluation",
        "systematic reviews",
        "dynamic text",
        "Static text",
        "bibliographic reviews",
        "review protocol",
        "feature classification",
        "classifica- tion",
        "pro- tocol",
        "three phases",
        "cific topic",
        "following steps",
        "research questions",
        "feature extraction",
        "formal way",
        "review need",
        "typing features",
        "specific user",
        "search strategies",
        "rhythms",
        "model",
        "technology",
        "iour",
        "expressions",
        "password",
        "recognition",
        "terms",
        "instance",
        "addition",
        "completion",
        "Medicine",
        "Computing",
        "application",
        "planning",
        "presentation",
        "information",
        "references",
        "items",
        "Identification",
        "goal",
        "existence",
        "investigation",
        "requirements",
        "Commissioning",
        "cases",
        "lack",
        "time",
        "Specification",
        "Development",
        "selection",
        "576 J Braz Comput Soc",
        "greatest possible number",
        "newer review article",
        "systematic review process",
        "traditional reviews",
        "inclusion/exclusion criteria",
        "Data synthesis",
        "quantitative aspects",
        "academic journals",
        "web sites",
        "two researchers",
        "next sections",
        "intrusion detection",
        "Classification algorithms",
        "Performance measures",
        "Benchmarking datasets",
        "comparative experiments",
        "respective sub-questions",
        "typing data",
        "Reference search",
        "search expressions",
        "planning phase",
        "Quality evaluation",
        "research question",
        "diverse aims",
        "Information extraction",
        "dissemination mechanisms",
        "explicit definition",
        "More details",
        "Extracted features",
        "reference databases",
        "conduction phase",
        "previous work",
        "Report evaluation",
        "summary results",
        "rigour",
        "Selection",
        "use",
        "importance",
        "study",
        "support",
        "forms",
        "step",
        "qualitative",
        "meta-analysis",
        "mulation",
        "publishing",
        "conferences",
        "experts",
        "publication",
        "principles",
        "phases",
        "topic",
        "Advantages",
        "Internet",
        "English",
        "authors",
        "part",
        "need",
        "3.2",
        "1.1",
        "logical connective O R.",
        "logic connective AN D",
        "personal- ization systems",
        "authentica- tion systems",
        "behaviour intrusion detection",
        "behavior intrusion detection",
        "ACM Digital Library",
        "biometric intrusion detection",
        "behavioural intrusion detection",
        "behavioral intrusion detection",
        "basic search expression",
        "following exclusion criteria",
        "keystroke biometric(s",
        "recommender systems",
        "1.3 Selection criteria",
        "keypress biometric",
        "behavioral IDS",
        "behavioural biometrics",
        "keystroke pattern",
        "keystroke authentication",
        "behavioral biometrics",
        "many users",
        "typing pattern",
        "uous authentication",
        "same topic",
        "personalized information",
        "ized content",
        "content delivery",
        "recommendation system",
        "ommendations system",
        "information retrieval",
        "N OT",
        "several data-bases",
        "computing area",
        "data- base",
        "Appendix A",
        "next steps",
        "one page",
        "keystroke biometrics",
        "typing dynamics",
        "web search",
        "keystroke analysis",
        "unrelated topics",
        "IEEE Xplore",
        "Science Direct",
        "last part",
        "systematic review",
        "various terms",
        "exclusion terms",
        "performance comparisons",
        "1.2 References search",
        "OR ‘‘personalization",
        "rithms",
        "measures",
        "datasets",
        "classifier",
        "tests",
        "list",
        "papers",
        "completeness",
        "database",
        "conjunction",
        "reason",
        "differences",
        "syntax",
        "org",
        "ieeexplore",
        "sciencedirect",
        "Scopus",
        "inclusion",
        "Publications",
        "requirement",
        "posters",
        "presentations",
        "cts",
        "editorials",
        "texts",
        "magazines",
        "newspaper",
        "578 J Braz Comput Soc",
        "enough informa- tion",
        "use- ful features",
        "high- light references",
        "complete version",
        "unnecessary work",
        "different versions",
        "restricted access",
        "The value",
        "possible scores",
        "research criteria",
        "feature vector",
        "previous researches",
        "researc- hes",
        "exclusion criteria",
        "static text",
        "first step",
        "data- bases",
        "several papers",
        "quality criteria",
        "Basic information",
        "Best performance",
        "duplicate publications",
        "same study",
        "higher values",
        "algorithm parameters",
        "experimental tests",
        "design step",
        "analysis step",
        "conclusion step",
        "test dataset",
        "first publications",
        "conduction step",
        "quality score",
        "nine items",
        "several items",
        "performance tests",
        "criterion",
        "services",
        "accessible",
        "sum",
        "goals",
        "beginning",
        "advantages",
        "approaches",
        "limitations",
        "problems",
        "acm",
        "isiknowledge",
        "scopus",
        "Part",
        "title",
        "year",
        "Type",
        "device",
        "PC",
        "measure",
        "Number",
        "Algorithms",
        "verification",
        "Observations",
        "line",
        "Table",
        "18/February",
        "tool",
        "mendeley",
        "series",
        "duplicates",
        "organization",
        "category",
        "4",
        "2.1",
        "references Database Number",
        "step Step Number",
        "PDF files",
        "fast reading",
        "secondary studies",
        "200 refer- ences",
        "quality assessment",
        "several texts",
        "direct way",
        "pub- lications",
        "descending order",
        "highest scores",
        "higher scores",
        "greater efforts",
        "average score",
        "high number",
        "exclusion criterion",
        "search databases",
        "plete text",
        "full text",
        "duplicate references",
        "remaining references",
        "eligible references",
        "information extraction",
        "Table 2 Number",
        "Table 1",
        "associations",
        "entries",
        "Web",
        "Gaines",
        "centralization",
        "intersection",
        "removal",
        "university",
        "Conse",
        "reviews",
        "surveys",
        "tasks",
        "spreadsheet",
        "Total",
        "elimination",
        "detail",
        "tables",
        "algorithms",
        "performance",
        "abstract",
        "observation",
        "one",
        "ally",
        "purpose",
        "equals",
        "focus",
        "rel",
        "graph",
        "different",
        "4.2.2",
        "2.3",
        "580 J Braz Comput Soc",
        "approximate normal distribution",
        "tional research efforts",
        "highest quality score",
        "publi- cations",
        "scientific community",
        "research sub-questions",
        "performance evaluation",
        "benchmarking datasets",
        "access card",
        "military purposes",
        "simple login",
        "unauthorized access",
        "card ownership",
        "recent years",
        "greatest growth",
        "same institution",
        "biometric technologies",
        "behavioral features",
        "biometric technology",
        "specific devices",
        "biometric data",
        "growth trend",
        "voice recognition",
        "primary method",
        "One way",
        "additional cost",
        "authentication factors",
        "several users",
        "unauthorized user",
        "security context",
        "Publication Year",
        "widespread use",
        "user identity",
        "zero",
        "scores",
        "maximum",
        "aspect",
        "field",
        "atten",
        "Both",
        "5 Results",
        "section",
        "disadvan",
        "credentials",
        "token",
        "biometrics",
        "gerprint",
        "iris",
        "combination",
        "secrecy",
        "weaknesses",
        "authorization",
        "words",
        "birthday",
        "scenarios",
        "expansion",
        "gally",
        "rate",
        "USA",
        "losses",
        "world",
        "insiders",
        "victim",
        "attack",
        "science",
        "methods",
        "determination",
        "iological",
        "hardware",
        "contrast",
        "other",
        "capture",
        "level",
        "transparency",
        "false alarm rates",
        "nature recognition biometrics",
        "specific operations",
        "stroke dynamics",
        "Recognition precision",
        "same environment",
        "nition performance",
        "user identification",
        "legiti- mate",
        "legitimate user",
        "trol systems",
        "authentication",
        "factor",
        "acceptance",
        "presence",
        "keyboards",
        "teristics",
        "pen",
        "intruder",
        "standards",
        "access",
        "European"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 4.023614,
      "content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n� The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning � Covariate shift �\nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg � X � Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg � X � Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ�Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ�Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ�Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n� min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp �cjjxtr � x\nðiÞ\nte jj22\n\n� �\n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð�cjjxtr � x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð�cjjxtr � x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k � n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð�cjjxtar � xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr � xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n� �� �\n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� y\n\nðiÞ\ntr � f x\n\nðiÞ\ntr\n\n� �� �2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n� �\n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x� CÞ or maxð0;C � xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods sho",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 456223,
      "metadata_storage_name": "s40708-015-0017-z.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDcwOC0wMTUtMDAxNy16LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Zengda Guan",
      "metadata_title": "Local regression transfer learning with applications to users’ psychological characteristics prediction",
      "metadata_creation_date": "2015-08-12T10:56:23Z",
      "keyphrases": [
        "China A. Li Black Dog Institute",
        "weighted risk regression model",
        "Z. Guan Business School",
        "cn T. Zhu Institute",
        "Local regression transfer learning",
        "cn A. Li",
        "Adaptive parameter-setting method",
        "New South Wales",
        "Local transfer learning",
        "clustering reweighting methods",
        "Shandong Jianzhu University",
        "individual psychological characteristics",
        "online user experience",
        "transfer learning methods",
        "personal psychological characteristics",
        "independent test dataset",
        "labelled training dataset",
        "psychological characteristics prediction",
        "Beijing Forestry University",
        "Zengda Guan",
        "Ang Li",
        "Tingshao Zhu",
        "personal preference",
        "online services",
        "web user",
        "The Author",
        "open access",
        "Recent studies",
        "training instance",
        "different genders",
        "ferent districts",
        "� Covariate shift",
        "recent decades",
        "important role",
        "human lives",
        "consumer preference",
        "consistent traits",
        "changeable status",
        "key factors",
        "one type",
        "human behaviour",
        "web behaviour",
        "Many studies",
        "Most studies",
        "many cases",
        "Chinese Academy",
        "Computing Technology",
        "Brain Informatics",
        "Previous studies",
        "little attention",
        "innovative approach",
        "learning models",
        "tional models",
        "generalization capability",
        "demographic variation",
        "identical distribution",
        "users’ personality",
        "behaviours",
        "applications",
        "article",
        "Springerlink",
        "Abstract",
        "differences",
        "problem",
        "bour",
        "importance",
        "situation",
        "labels",
        "experiments",
        "depression",
        "results",
        "Keywords",
        "1 Introduction",
        "people",
        "time",
        "Internet",
        "help",
        "assumption",
        "Jinan",
        "mail",
        "guanzengda",
        "sdjzu",
        "Department",
        "Psychology",
        "Sydney",
        "Australia",
        "blackdog",
        "Sciences",
        "tszhu",
        "DOI",
        "crossmark",
        "org",
        "low",
        "performance",
        "paper",
        "3",
        "14",
        "new local regression transfer learning methods",
        "Most existing covariate shift methods",
        "local transfer learning methods",
        "bility distribution PteðX",
        "local learning views",
        "adaptive k-NN methods",
        "ytrÞ�Pte lðxtr",
        "weighted risk model",
        "PteðxtrÞ PtrðxtrÞ",
        "general solution framework",
        "local neighbour region",
        "fðxð1Þte",
        "training-test k-NN method",
        "pre- diction functions",
        "covariate shift problems",
        "empirical risk form",
        "training data instances",
        "test dataset label",
        "test dataset model",
        "Þg � X � Y",
        "¼ fðxð1Þtr",
        "ytrÞ lðxtr",
        "test dataset population",
        "psychological characteristics labels",
        "learning model",
        "regression form",
        "local region",
        "¼ PteðXÞ",
        "prediction functions",
        "risk function",
        "expected risk",
        "PtrðXÞ",
        "simple covariate",
        "ðxðntrÞ",
        "data points",
        "parameter-setting method",
        "¼ PteðyjxÞ",
        "prediction accuracy",
        "training dataset",
        "entire dataset",
        "input dataset",
        "PtrðyjxÞ",
        "ðxðnteÞ",
        "resampling weight",
        "computation cost",
        "continual values",
        "unknown parameter",
        "exper- iments",
        "experiment results",
        "last section",
        "different distributions",
        "key point",
        "loss function",
        "ntr Xntr",
        "yÞ�Ptr",
        "probability distribution",
        "probability density",
        "previous work",
        "Y.",
        "purpose",
        "researches",
        "procedure",
        "addition",
        "parameters",
        "domains",
        "dictions",
        "rest",
        "Sect.",
        "background",
        "Ztr",
        "Zte",
        "inputs",
        "process",
        "hÞ",
        "datasets",
        "ratio",
        "2",
        "Gaussian kernel basis functions",
        "kernel mean matching method",
        "many machine learning tasks",
        "adaptive K-NN reweighting method",
        "training-test K-NN reweighting method",
        "weighted machine learning model",
        "complete covariate shift process",
        "many density estimation methods",
        "Gaussian kernel functions",
        "2.3.1 K-NN reweighting method",
        "Kullback–Leibler divergence",
        "reproducing-kernel Hilbert space",
        "comparative advan- tage",
        "density estima- tion",
        "nearest-neighbour approximation method",
        "Local machine learning",
        "local classification method",
        "k-nearest test set",
        "many neighbour points",
        "k-nearest-neighbour reweighting methods",
        "less neighbour points",
        "squares importance biases",
        "ratio PteðxtrÞ",
        "kernel methods",
        "learning process",
        "k-NN methods",
        "many kinds",
        "nearest-neighbour methods",
        "local learning",
        "density similarity",
        "transfer learning",
        "neighbour region",
        "histogram methods",
        "clustering methods",
        "PtrðxtrÞ",
        "prediction model",
        "146 Z. Guan",
        "significant effect",
        "one hand",
        "little relationship",
        "target point",
        "unnecessary interferences",
        "similar property",
        "small region",
        "big region",
        "other hand",
        "strong noise",
        "small proportion",
        "given point",
        "PðxÞ",
        "region volume",
        "one volume",
        "two stages",
        "test dataset",
        "first stage",
        "impor- tance",
        "origin form",
        "test points",
        "long-distance points",
        "target data",
        "data point",
        "training distributions",
        "nearest neighbours",
        "training instances",
        "training point",
        "training data",
        "input densities",
        "common situations",
        "total number",
        "errors",
        "researchers",
        "forms",
        "formula",
        "Sugiyama",
        "series",
        "Kanamori",
        "Huang",
        "KMM",
        "Dai",
        "Pardoe",
        "list",
        "boosting",
        "rithms",
        "size",
        "effects",
        "way",
        "nV",
        "ktr",
        "kte",
        "Loog",
        "work",
        "formance",
        "neighbourhood",
        "fact",
        "knowledge",
        "extension",
        "adaptation",
        "Clustering-based",
        "view",
        "ð3Þ",
        "clustering-based reweighting meth- ods",
        "3.2 Training-test K-NN reweighting method",
        "Cross validation tech- nique",
        "Gaussian kernel function ratio",
        "empirical parameter estimation way",
        "Clustering-based reweighting method",
        "nearest-neighbour reweighting method",
        "users’ psychological characteristics",
        "other big values",
        "nearest test set",
        "other test points",
        "covariate shift methods",
        "labelled test data",
        "training data points",
        "training data xtr",
        "k-NN method",
        "training data part",
        "togram method",
        "empirical way",
        "reweighting formula",
        "Gaussian kernels",
        "validation technique",
        "big bias",
        "nearest-neighbour point",
        "WeigðxtrÞ",
        "exponential term",
        "exponential law",
        "appropriate neighbour",
        "computation time",
        "comparable probability",
        "ðjÞ tr",
        "two parts",
        "important issue",
        "smaller value",
        "similar probability",
        "expð�cjjxtar",
        "small value",
        "2nneig Pnneig",
        "clustering algorithm",
        "test neighbours",
        "validation dataset",
        "k value",
        "mine k",
        "k � n",
        "gauðnneig",
        "density distance",
        "population size",
        "ktr Pktr",
        "k.",
        "¼ ktr",
        "Xk",
        "�cjjxtr",
        "bandwidth",
        "effect",
        "denominator",
        "c.",
        "Enas",
        "Choi",
        "threshold",
        "reason",
        "1 jjxtr",
        "kind",
        "histograms",
        "ð4Þ",
        "ð5Þ",
        "147",
        "jClustrðxðiÞtr Þj",
        "The Big Five personality model",
        "accurate density-region division methods",
        "lðyðiÞtr",
        "Big Five personality framework",
        "jClusteðxðiÞtr",
        "adaptive stepwise regression method",
        "WeigðxðiÞtr",
        "ðxðiÞte",
        "local transfer methods",
        "small enough region",
        "approximate computation way",
        "local reweighting methods",
        "2.4 Weighted regression model",
        "weighted learning model",
        "web users’ depression",
        "big influence",
        "five dimensions",
        "hjðxÞ",
        "weighted model",
        "C � xÞ",
        "regression function",
        "human personality",
        "histogram method",
        "PteðxtrÞ",
        "high-dimensional difficulty",
        "high-dimensional situation",
        "previous stage",
        "real value",
        "prediction value",
        "sion splines",
        "constant denoted",
        "hinge function",
        "total steps",
        "optimal performance",
        "cients bj.",
        "logical characteristics",
        "three parts",
        "148 Z. Guan",
        "different districts",
        "unpleasant emotions",
        "intellectual curiosity",
        "CES-T scale",
        "many data",
        "one histogram",
        "same cluster",
        "Clustering method",
        "probability estimation",
        "lowing formula",
        "one cluster",
        "different weight",
        "importance weighting",
        "tr ÞÞ",
        "� CÞ",
        "instances",
        "detail",
        "number",
        "idea",
        "equal",
        "risk",
        "Xntr",
        "bias",
        "MARS",
        "Xm",
        "bjhj",
        "product",
        "two",
        "respec",
        "3 Experiments",
        "psychology",
        "agreeableness",
        "conscientiousness",
        "extraversion",
        "neuroticism",
        "openness",
        "tendency",
        "degree",
        "creativity",
        "preference",
        "novelty",
        "ð6Þ",
        "ð7Þ",
        "famous microblog service providers",
        "male users’ depression degree",
        "many feature dimensions",
        "mogorov–Smirnov test",
        "503 million registered users",
        "high dimension curse",
        "Big Five dimensions",
        "online self-report questionnaire",
        "users’ web behaviours",
        "time series data",
        "additional linguistic features",
        "Guangdong users’ personality",
        "istration time",
        "test set",
        "T test",
        "two-sample test",
        "different users",
        "female users",
        "important factor",
        "training set",
        "3.1 Experiment setup",
        "ing personality",
        "depression scales",
        "digital records",
        "pression behaviours",
        "screen name",
        "facial picture",
        "privacy settings",
        "individual privacy",
        "vate messages",
        "interpersonal behaviours",
        "social interaction",
        "punctuation marks",
        "perceptual processing",
        "stepwisefit method",
        "Matlab toolbox",
        "gender-personality experiment",
        "district-personality experiment",
        "depression experiment",
        "same distribution",
        "Gaussian distribution",
        "unknown distribution",
        "Weibo users",
        "five categories",
        "online behaviours",
        "behavioural data",
        "Sina Weibo",
        "personal image",
        "personal page",
        "sentiment words",
        "cognitive words",
        "Guangdong dataset",
        "female dataset",
        "personality experiments",
        "sion experiments",
        "dynamic features",
        "845 features",
        "20 features",
        "relationship",
        "personality/depression",
        "example",
        "suitability",
        "difference",
        "China",
        "research",
        "consent",
        "prediction",
        "May",
        "August",
        "562 participants",
        "profiles",
        "demographics",
        "expression",
        "self-statement",
        "concern",
        "comments",
        "strangers",
        "outcomes",
        "friends",
        "microblogs",
        "period",
        "apps",
        "June",
        "1000 participants",
        "characters",
        "numerals",
        "pronouns",
        "relevant",
        "Both",
        "22",
        "other local trans- fer learning methods",
        "adaptive k-NN transfer learning methods",
        "test k-NN transfer learning method",
        "other transfer learning methods",
        "famous transfer learning method",
        "regression transfer learning methods",
        "adaptive k-NN reweighting method",
        "Local kNN reweighting methods",
        "transfer learning way",
        "mean square error",
        "Kol- mogorov–Smirnov",
        "weighted risk form",
        "global transfer method",
        "Kolmogorov–Smirnov test",
        "clustering-based reweighting method",
        "single-dimension T test",
        "fixed k value",
        "other k-NN",
        "KMM reweighting method",
        "male users’ personality",
        "global k-NN",
        "regression software",
        "risk models",
        "traditional method",
        "baseline method",
        "training- test",
        "test results",
        "open source",
        "female datasets",
        "situa- tions",
        "trait A",
        "total size",
        "perfor- mances",
        "TTkNN method",
        "5 personality dimensions",
        "MARS method",
        "GkNN method",
        "best results",
        "c value",
        "result comparisons",
        "distribution divergence",
        "comparable performance",
        "Clust denotes",
        "parameter values",
        "C.",
        "4 dimensions",
        "MARS.",
        "neighbours",
        "Matlab/Octave",
        "rtu",
        "lv",
        "jekabsons",
        "tables",
        "figures",
        "nation",
        "AkNN1",
        "AkNN2",
        "MSE",
        "genders",
        "task",
        "2 features",
        "1 features",
        "probability",
        "confidence",
        "situations",
        "others",
        "advantage",
        "impact",
        "Fig.",
        "noise",
        "149",
        "25",
        "test results Condition A C E N O",
        "Local regression transfer learning results",
        "other local transfer learning methods",
        "stable density estimation method",
        "male users’ depression level",
        "k-NN transfer methods",
        "regression transfer methods",
        "clustering transfer methods",
        "other two traits",
        "least k value",
        "five personality traits",
        "other districts",
        "clustering method",
        "five traits",
        "The method",
        "best method",
        "0, outlier point",
        "strong influence",
        "optimization result",
        "three traits",
        "M S",
        "150 Z. Guan",
        "cs.rtu",
        "regular rule",
        "person- ality",
        "mogorov–Smirnov",
        "one trait",
        "3 feature dimensions",
        "5 feature dimensions",
        "different-gender datasets",
        "different-district datasets",
        "best performance",
        "optimal value",
        "Weibo data",
        "gender experiment",
        "most situations",
        "cluster number",
        "clusters increases",
        "prediction performance",
        "5 traits",
        "exception",
        "shake-up",
        "factor",
        "GkNN",
        "constant",
        "diversity",
        "Guangdong",
        "province",
        "model",
        "total",
        "95% confidence",
        "performances",
        "Table 2",
        "local k-NN family transfer learning methods",
        "global k-NN transfer learning method",
        "predic- tion bias",
        "different- distribution datasets",
        "appropriate k value",
        "transfer method",
        "test data",
        "different-distribution feature",
        "subtle nature",
        "density estimation",
        "training",
        "experiment",
        "Table",
        "result",
        "non",
        "Clust",
        "other",
        "3.5 Discussion",
        "conclusion"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.5911174,
      "content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker  J Big Data            (2019) 6:95  \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence:   \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker  J Big Data            (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n\n\nPage 3 of 25Sarker  J Big Data            (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker  J Big Data            (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker  J Big Data            (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker  J Big Data            (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker  J Big Data            (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker  J Big Data            (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1  = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker  J Big Data            (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker  J Big Data            (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsivel",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1660534,
      "metadata_storage_name": "s40537-019-0258-4.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDI1OC00LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Iqbal H. Sarker ",
      "metadata_title": "Context-aware rule learning from smartphone data: survey, challenges and future directions",
      "metadata_creation_date": "2019-10-30T14:24:16Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "intelligent decision making strategies",
        "advanced data analytical techniques",
        "25Sarker  J Big Data",
        "Context‑aware rule learning",
        "Intelligent systems Open Access",
        "machine learning based techniques",
        "Creative Commons license",
        "corresponding context-aware rule learning",
        "Iqbal H. Sarker",
        "User behavior modeling",
        "creat iveco mmons",
        "intelligent context-aware applications",
        "raw contextual data",
        "corresponding search interests",
        "data-driven context-aware systems",
        "phone data analytics",
        "original author(s",
        "corresponding data processing",
        "multifunctional cell phones",
        "users’ surrounding environment",
        "smartphone data",
        "Data science",
        "Predictive analytics",
        "context-aware rules",
        "Context-aware computing",
        "author information",
        "future directions",
        "recent days",
        "daily life",
        "important IoT",
        "next generation",
        "wireless connectivity",
        "work coverage",
        "developed countries",
        "recent statistics",
        "Google Trends",
        "users’ interest",
        "other platforms",
        "Desktop Computer",
        "Laptop Computer",
        "current world",
        "recent developments",
        "behavioral activities",
        "wide attention",
        "high precision",
        "traditional approaches",
        "efficient results",
        "previous work",
        "rule-based automated",
        "IoT services",
        "unrestricted use",
        "appropriate credit",
        "doi.org",
        "1 Swinburne University",
        "Full list",
        "Tablet Computer",
        "last 5 years",
        "timestamp information",
        "particular date",
        "highest point",
        "personal devices",
        "Things) devices",
        "essential part",
        "world population",
        "Mobile Phones",
        "particular term",
        "SURVEY PAPER",
        "peak popularity",
        "challenges",
        "Introduction",
        "smartphones",
        "individuals",
        "Internet",
        "capabilities",
        "number",
        "Fig.",
        "Abstract",
        "ogy",
        "academia",
        "industry",
        "order",
        "set",
        "key",
        "contexts",
        "comparison",
        "effective",
        "article",
        "area",
        "discussion",
        "Clustering",
        "Classification",
        "Association",
        "Personalization",
        "Time-series",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "msarker",
        "Melbourne",
        "Australia",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "Figure",
        "x-axis",
        "range",
        "chart",
        "instance",
        "value",
        "maximum",
        "y-axis",
        "minimum",
        "context-aware mobile notification management system",
        "other broad application areas",
        "corresponding rule-based prediction model",
        "different context-aware test cases",
        "data-driven intelligent context-aware systems",
        "optimal time-based context-aware rules",
        "rule-based intelligent context-aware systems",
        "smartphone data analytic solutions",
        "context-aware smart city services",
        "users’ behavioral activity patterns",
        "intelligent data-driven decisions",
        "THEN” logical structure",
        "corresponding data clustering",
        "corresponding behavioral activities",
        "Users’ interest trends",
        "intelligent eHealth services",
        "aware behavioral rules",
        "relevant context-aware rules",
        "relevant contextual data",
        "contextual data patterns",
        "contextual data clustering",
        "machine learning researchers",
        "machine learning techniques",
        "different surrounding contexts",
        "machine learning rules",
        "relevant contextual information",
        "data mining techniques",
        "advanced data analysis",
        "rule-based classification",
        "raw data",
        "different contexts",
        "mobile notifications",
        "context-aware  software",
        "hidden patterns",
        "data science",
        "time-series data",
        "advanced features",
        "mental learning",
        "personalized services",
        "rity services",
        "deeper analysis",
        "association analysis",
        "detailed analysis",
        "daily basis",
        "great source",
        "useful information",
        "two parts",
        "temporal context",
        "tial context",
        "social contexts",
        "family members",
        "specific exceptions",
        "particular domain",
        "prac- titioners",
        "many reasons",
        "various purposes",
        "particular timestamp",
        "numeric values",
        "world-wide popularity",
        "ture optimization",
        "dynamic updating",
        "The reason",
        "huge amount",
        "unknown correlations",
        "diverse behaviors",
        "quent part",
        "individuals’ usage",
        "phone user",
        "devices",
        "result",
        "antecedent",
        "others",
        "example",
        "Alice",
        "work",
        "evening",
        "general",
        "preferences",
        "addition",
        "cable",
        "study",
        "applications",
        "building",
        "Several",
        "selection",
        "aim",
        "machine learning based context-aware rule learning framework",
        "Suggested machine learning based framework",
        "Context-aware rule based applications",
        "Context-aware rule learning strategies",
        "smartphone data analyt- ics",
        "real world applications",
        "rule discovery techniques",
        "decision making capability",
        "rule-based intelligent systems",
        "Human Computer Interaction",
        "smartphone data analytics",
        "corresponding context-aware rules",
        "contextual smartphone data",
        "Pervasive Computing area",
        "incremental learning",
        "context-aware computing",
        "updation techniques",
        "smrtphone data",
        "uitous Computing",
        "several perspectives",
        "time-series modeling",
        "various layers",
        "various perspectives",
        "different meanings",
        "different purposes",
        "numerous areas",
        "Computer-Supported Collaborative",
        "Ambient Intelligence",
        "recent works",
        "broader collection",
        "social aspects",
        "little work",
        "earlier work",
        "brief discussion",
        "background information",
        "main characteristics",
        "existing research",
        "brief survey",
        "first article",
        "multi-dimensional contexts",
        "sion” section",
        "profit",
        "analysis",
        "discretization",
        "issues",
        "contributions",
        "paper",
        "shortcomings",
        "role",
        "knowledge",
        "remainder",
        "variety",
        "notion",
        "mobile",
        "Ubiquitous",
        "context-awareness",
        "location",
        "people",
        "objects",
        "factors",
        "physical",
        "entity",
        "activities",
        "users",
        "definitions",
        "categories",
        "daily life activities",
        "temporal informa- tion",
        "ubiquitous computing community",
        "ubiquitous computing area",
        "Contextual smartphone data",
        "user diverse activities",
        "rent social situation",
        "data source",
        "smartphone usage",
        "temporal information",
        "defi- nitions",
        "dis- cussion",
        "Several studies",
        "surrounding people",
        "Other definitions",
        "other hand",
        "current situation",
        "nearby people",
        "available processors",
        "network capacity",
        "noise level",
        "lighting level",
        "day situations",
        "cellular phones",
        "computing environment",
        "location information",
        "locational information",
        "environmental information",
        "surrounding information",
        "changing environment",
        "physical environment",
        "other researchers",
        "important aspects",
        "alternative view",
        "con- text",
        "social context",
        "following definition",
        "user location",
        "user input",
        "relevant contexts",
        "spatial context",
        "user environment",
        "pervasive",
        "section",
        "scope",
        "different",
        "perspectives",
        "Schilit",
        "Brown",
        "temperature",
        "Simi",
        "identity",
        "account",
        "Ryan",
        "synonyms",
        "computer",
        "Franklin",
        "Ward",
        "state",
        "Hull",
        "entire",
        "settings",
        "Rodden",
        "resources",
        "connectivity",
        "display",
        "costs",
        "examples",
        "Dey",
        "survey",
        "person",
        "place",
        "interaction",
        "influence",
        "decisions",
        "everything",
        "lives",
        "Mobile",
        "personalized mobile game recommendation system",
        "vidual mobile phone user behavior",
        "mobile notification management systems",
        "relevant contextual informa- tion",
        "related contextual infor- mation",
        "various smart phone applications",
        "individual mobile phone users",
        "personalized context-aware recommendation",
        "user mobile web navigation",
        "smart mobile applications",
        "smart mobile phone",
        "phone usage behavior",
        "standardized communications rules",
        "International Telecommunication Union",
        "sive commercial industry",
        "accurate context-aware predictions",
        "text communication service",
        "contextual behavioral patterns",
        "automatic spam filtering",
        "smartphone notification logs",
        "phone call activities",
        "short text messages",
        "contextual usage patterns",
        "short message service",
        "spam text messages",
        "contextual patterns",
        "unique phone",
        "individual users",
        "User navigation",
        "short messages",
        "game log",
        "voice communication",
        "major activities",
        "spam messages",
        "various types",
        "various kinds",
        "various real",
        "web log",
        "web searching",
        "particular user",
        "meta data",
        "log data",
        "outgoing calls",
        "incoming calls",
        "other dimensions",
        "day situation",
        "social relationship",
        "numerous growth",
        "dramatic increasing",
        "good time",
        "bad time",
        "rapid development",
        "smartphone apps",
        "Such logs",
        "social networks",
        "predictive suggestions",
        "portal structure",
        "computing capabilities",
        "life-logging device",
        "technical sense",
        "context data",
        "tual information",
        "context source",
        "SMS log",
        "real world",
        "life purposes",
        "contact number",
        "ability",
        "caller",
        "callee",
        "exchange",
        "protocols",
        "81 billion",
        "task",
        "Multimedia",
        "Facebook",
        "Gmail",
        "Youtube",
        "Skype",
        "notifications",
        "events",
        "news",
        "reminders",
        "alerts",
        "games",
        "promotional",
        "emails",
        "Twitter",
        "LinkedIN",
        "WhatsApp",
        "Viver",
        "intelligent",
        "entertainment",
        "chat",
        "misc",
        "TV",
        "netting",
        "travel",
        "sport",
        "banking",
        "needs",
        "action",
        "adventure",
        "puzzle",
        "RPG",
        "strategy",
        "ubiquity",
        "opportunity",
        "memories",
        "Context‑aware rule learning strategies",
        "Time interval type Number",
        "temporal context based rules",
        "existing time-series segmentation approaches",
        "time‑series smartphone data",
        "mobile phone data",
        "Google play API",
        "geographical loca- tion",
        "context-aware rule learning",
        "segment details References",
        "personalized log data",
        "user phone calls",
        "smart mobile phones",
        "life log data",
        "mobile user behavior",
        "behavioral data clusters",
        "time-based behavior modeling",
        "A time series",
        "related contextual information",
        "static time segments",
        "existing rule",
        "existing strategies",
        "important context",
        "contextual rule",
        "learning rules",
        "IoT data",
        "data points",
        "temporal, spatial",
        "behavioral rules",
        "exact time",
        "user experience",
        "surrounding environment",
        "core components",
        "life- logs",
        "SMS headers",
        "App use",
        "physical activities",
        "Bluetooth devices",
        "open challenges",
        "diverse activities",
        "effective modeling",
        "open problem",
        "Various types",
        "different applications",
        "remaining hours",
        "late morning",
        "Equal 3 Morning",
        "Equal 4 Morning",
        "Unequal 5 Morning",
        "Equal 4 Night",
        "6:00 a",
        "sensors",
        "content",
        "sapp",
        "WiFi",
        "proximity",
        "opportunities",
        "future",
        "directions",
        "discovery",
        "behaviors",
        "sequence",
        "research",
        "Table",
        "afternoon",
        "Song",
        "Rawassizadeh",
        "Mukherji",
        "Bayir",
        "Paireekreng",
        "day",
        "Jayarajah",
        "Do",
        "Xu",
        "Mehrotra",
        "Zhu",
        "forenoon",
        "Oulasvirta",
        "Next",
        "Yu",
        "midnight",
        "0:00",
        "dynamic segmenta- tion technique",
        "high confidence temporal rules",
        "unequal interval based segmentation",
        "high confidence rules",
        "four different time segments",
        "various mobile applications",
        "dynamic segmen- tation",
        "four time slots",
        "mobile phone users",
        "individual smartphone users",
        "same interval length",
        "two broad categories",
        "users’ behavioral activities",
        "dynamic segmentation techniques",
        "Such static segmentation",
        "various time intervals",
        "unequal time interval",
        "time segments decreases",
        "temporal coverage",
        "different lengths",
        "temporal contexts",
        "arbitrary categories",
        "two types",
        "behavioral characteristics",
        "corresponding segmentation",
        "population behavior",
        "ioral evidence",
        "usage records",
        "seg- ments",
        "single parameter",
        "division result",
        "base period",
        "time period",
        "static generation",
        "meaningful results",
        "variable number",
        "one case",
        "N1 number",
        "N2 number",
        "Naboulsi",
        "Dashdorj",
        "Shin",
        "11:00 a",
        "S5",
        "S8",
        "Farrahi",
        "ii",
        "researchers",
        "periods",
        "1 h",
        "works",
        "approaches",
        "goal",
        "change",
        "patterns",
        "authors",
        "Tmax",
        "24-h",
        "BP",
        "5 min",
        "2:00",
        "5:00",
        "7:00",
        "9:00",
        "important machine learning tech- niques",
        "time- series data Lu",
        "Base technique Description References",
        "time-series log data Halvey",
        "data shape based approaches",
        "squared error-based clustering algorithm",
        "time-series data analysis",
        "time-series data Hartono",
        "time-series data Das",
        "similar K-medoids method",
        "cluster- ing algorithms",
        "mean value calculation",
        "temporal context Khail",
        "different initial points",
        "base time period",
        "dynamic time segments",
        "Various calendar schedules",
        "corresponding time boundaries",
        "user behavior patterns",
        "large time segments",
        "large value",
        "initial partitions",
        "different value",
        "time segmentation",
        "time interval",
        "different contexts",
        "predefined value",
        "clustering algorithms",
        "finest granularity",
        "segments decreases",
        "iable length",
        "particular event",
        "personal calendars",
        "multiple thresholds",
        "sliding window",
        "several applications",
        "specific application",
        "convergence centroids",
        "segments Ozer",
        "segmentation Do",
        "upper boundary",
        "particular segment",
        "genetic algorithm",
        "users’ behavior",
        "users’ preferences",
        "one user",
        "users’ behave",
        "Single parameter",
        "K-means algorithm",
        "predefined number",
        "individual",
        "assumptions",
        "problems",
        "sense",
        "clusters",
        "K.",
        "outliers",
        "30 min",
        "Karatzoglou",
        "small",
        "Phithakkitnukoon",
        "Dekel",
        "Zulkernain",
        "Seo",
        "Multi-thresholds",
        "lower",
        "Zhang",
        "Shokoohi",
        "Keogh",
        "Kandasamy",
        "characteristic",
        "reason",
        "1",
        "2:30",
        "single linkage agglomerative clustering algorithm",
        "bottom-up hierarchical data processing",
        "popular agglomerative clustering",
        "other characteristic mismatches",
        "device clustering method",
        "Existing hierarchical algorithms",
        "agglomerative methods",
        "complete linkage",
        "device methods",
        "other clusters",
        "time segments",
        "behavioral clusters",
        "nearest neighbor",
        "proximity matrix",
        "new cluster",
        "matrix value",
        "outlier problem",
        "presence",
        "medoid",
        "K-means",
        "size",
        "user",
        "practice",
        "simplest",
        "distance",
        "sivel"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.4478297,
      "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 2 of 19\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n�\ni K xi; x\n\n� �þ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2;…; ∂�N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n� �\n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK xi; x j\n\n� �� �\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 3 of 19\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 4 of 19\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 5 of 19\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 6 of 19\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 7 of 19\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 8 of 19\n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi�−R0∅i\n�−b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 9 of 19\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 10 of 19\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang ",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 2713106,
      "metadata_storage_name": "s13640-018-0373-8.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3MxMzY0MC0wMTgtMDM3My04LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Boping Zhang",
      "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform",
      "metadata_creation_date": "2018-11-23T11:51:48Z",
      "keyphrases": [
        "accurate feature point alignment model",
        "new online shopping mode",
        "Augmented reality virtual glasses",
        "external propaganda channels",
        "actual wearing effects",
        "Creative Commons Attribution",
        "lates physical information",
        "phys- ical information",
        "head pose estimation",
        "traditional online shopping",
        "virtual glasses model",
        "input device-monocular camera",
        "key research issue",
        "virtual world scene",
        "user online glasses",
        "RESEARCH Open Access",
        "1 Introduction Network virtual",
        "iOS mobile platform",
        "local face features",
        "virtual model",
        "feature points",
        "scale-invariant feature",
        "new way",
        "2018 Open Access",
        "research key",
        "local features",
        "virtual information",
        "Face information",
        "Information Engineer",
        "real world",
        "Virtual try",
        "favorite glasses",
        "different glasses",
        "user senses",
        "face detection",
        "human face",
        "iOS glasses",
        "Boping Zhang",
        "optimal purchase",
        "robust SIFT",
        "rapid achievement",
        "sponding images",
        "vector machine",
        "EURASIP Journal",
        "Video Processing",
        "The Author",
        "iOS platform",
        "AR glasses",
        "camera image",
        "quick try",
        "human senses",
        "AR principle",
        "Computer vision",
        "real time",
        "real-time interaction",
        "Xuchang University",
        "APP system",
        "descent method",
        "computer technology",
        "SVM classifier",
        "Abstract",
        "development",
        "commerce",
        "gap",
        "goods",
        "speed",
        "immersion",
        "SDM",
        "identification",
        "Keywords",
        "merchants",
        "consumers",
        "important",
        "part",
        "field",
        "experiential",
        "position",
        "angle",
        "corre",
        "screen",
        "sound",
        "taste",
        "touch",
        "space",
        "sensory",
        "perience",
        "faces",
        "bopingzhang",
        "yeah",
        "School",
        "Henan",
        "China",
        "article",
        "terms",
        "real-time head mo- tion tracking",
        "Two corresponding isosceles triangles",
        "General face recognition process",
        "original author(s",
        "charge-coupled device) camera",
        "Android mobile platform",
        "Creative Commons license",
        "real envir- onment",
        "foreign glasses sellers",
        "3D modeling approach",
        "2D image overlay",
        "2D image superposition",
        "AR technology design",
        "2D face images",
        "virtual optician system",
        "image processing technology",
        "real-time requirements",
        "2D images",
        "accurate tracking",
        "International License",
        "The superposition",
        "3D glasses",
        "glasses images",
        "network virtual",
        "virtual try",
        "virtual objects",
        "Virtual glasses",
        "unrestricted use",
        "appropriate credit",
        "Research status",
        "Smart Look",
        "wearing effect",
        "research hotspots",
        "research results",
        "four categories",
        "Three points",
        "fine transformation",
        "unrealistic deformation",
        "inter- action",
        "good results",
        "different defects",
        "depth research",
        "large extent",
        "new ideas",
        "false ratios",
        "important indicator",
        "feature extraction",
        "dimension reduc",
        "first step",
        "cific location",
        "Glasses companies",
        "glasses model",
        "AR-based glasses",
        "face features",
        "face area",
        "face needs",
        "matching recognition",
        "doi.org",
        "orcid.org",
        "shopping experience",
        "input sensor",
        "experience senses",
        "input image",
        "computer vision",
        "augmented reality",
        "video stream",
        "system performance",
        "online shopping",
        "based detection",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Zhang",
        "crossmark",
        "crossref",
        "dialog",
        "USA",
        "Camirror",
        "Ipoint",
        "Kisok",
        "Xview",
        "function",
        "Users",
        "domestic",
        "Meijing",
        "Kede",
        "Biyao",
        "present",
        "Huang",
        "eyes",
        "pose",
        "scale",
        "method",
        "realism",
        "Cheng",
        "monocular",
        "CCD",
        "marker",
        "kinds",
        "techniques",
        "advantages",
        "problems",
        "application",
        "precondition",
        "frame",
        "posture",
        "lumination",
        "occlusion",
        "omission",
        "people",
        "Fig.",
        "gion",
        "range",
        "The Gaussian kernel function Kðx",
        "three-layer feedforward neural network",
        "Face recognition process Zhang",
        "high-dimensional face feature vector",
        "Sigmoid kernel functions",
        "local binary patterns",
        "local structural content",
        "Common kernel func",
        "high-dimensional feature space",
        "spatial fre- quency",
        "machine learning method",
        "high-dimensional feature vector",
        "online matching models",
        "low-dimensional subspace information",
        "support vector machine",
        "optimal parameter vector",
        "Feature dimension reduction",
        "different scale spaces",
        "face feature identification",
        "sional face features",
        "face detection methods",
        "3.1 SVM-based face detection",
        "¼ sgn XN train",
        "low-dimensional input space",
        "robust SIFT algorithm",
        "classic SVM algorithm",
        "K xi",
        "Nsv X",
        "detection process",
        "vector dimension",
        "width function",
        "quadratic function",
        "discriminant function",
        "Common features",
        "space complexity",
        "3.2 Face recognition",
        "Matching recognition",
        "Feature extraction",
        "Gabo feature",
        "spatial position",
        "offline learning",
        "optimization parameter",
        "input vector",
        "face database",
        "face alignment",
        "matching strategy",
        "ginal features",
        "network output",
        "detection result",
        "∂�N train",
        "oriented gradient",
        "illumination changes",
        "small displacements",
        "ture changes",
        "brightness invariance",
        "direction selectivity",
        "previous links",
        "final decision",
        "virtual glasses",
        "Markov model",
        "statistical theory",
        "nonlinear mapping",
        "mal hyperplane",
        "largest interval",
        "inner product",
        "Mercer condition",
        "Optimization problem",
        "training sample",
        "face image",
        "video sequence",
        "description ability",
        "output layer",
        "Figure 3 shows",
        "tected image",
        "LBP",
        "HOG",
        "histogram",
        "Gabor",
        "insensitiveness",
        "overall",
        "Page",
        "time",
        "premise",
        "technology",
        "scholars",
        "HMM",
        "AdaBoost",
        "structure",
        "Equation",
        "polynomial",
        "zÞ",
        "Eq.",
        "ÞT",
        "jy",
        "XNtrain",
        "∂i",
        "j � �� � ð3Þ",
        "conditions",
        "rotation",
        "ð1Þ",
        "2σ",
        "∗",
        "Figure 4 shows flow block diagram",
        "stable image fea- tures",
        "other feature extraction functions",
        "higher positioning accuracy",
        "SVM network structure",
        "highest gradient value",
        "ian Laplace function",
        "potential fixed points",
        "variable scale parameter",
        "Establishing Gaussian pyramid",
        "SVM classifier Zhang",
        "Gaussian differential functions",
        "Spatial extreme detection",
        "gradient direction histogram",
        "stitute tower model",
        "two adjacent layers",
        "Key point matching",
        "image matching model",
        "scale space sequence",
        "scale space factor",
        "Gaussian difference pyramid",
        "DoG operator curve",
        "Gaussian difference function",
        "local extreme points",
        "② Position key points",
        "tower top image",
        "local feature",
        "Gaussian function",
        "feature construction",
        "candidate position",
        "feature vector",
        "Gauss-Laplacian curve",
        "local gradients",
        "Gaussian template",
        "Gaussian blur",
        "Laplacian operator",
        "ence space",
        "jacent points",
        "brightness changes",
        "sta- bility",
        "affine transformation",
        "angle change",
        "Basic principle",
        "multiple details",
        "image search",
        "stability degree",
        "main contour",
        "edge features",
        "larger scale",
        "blurred image",
        "human eye",
        "original image",
        "first layer",
        "new image",
        "second layer",
        "minimum dimension",
        "red line",
        "blue line",
        "same group",
        "SIFT algorithm",
        "generation process",
        "main direction",
        "formation process",
        "pixel point",
        "convolution operation",
        "different sizes",
        "scale normalization",
        "The relationship",
        "noise",
        "scales",
        "selection",
        "kind",
        "symbol",
        "target",
        "retina",
        "dimensions",
        "location",
        "down-sampling",
        "pyramids",
        "bottom",
        "number",
        "¼ log2",
        "qð",
        "logarithm",
        "maxima",
        "minima",
        "2G",
        "constant",
        "images",
        "3.2.1",
        "3.2.2",
        "σ",
        "ð6Þ",
        "∇",
        "Fig. 4 SIFT algorithm flow chart",
        "Terravic Re- search Corporation",
        "red intermediate detection point",
        "N+ 3-layer Gaussian pyramid",
        "N+ 2-layer DoG pyramid",
        "Fig. 5 Gaussian pyramid Zhang",
        "match- ing performance",
        "SIFT matching algorithm",
        "face infrared database",
        "feature point field",
        "20 infrared image sequences",
        "initial matching method",
        "N extreme points",
        "Face recognition experiment",
        "scale space value",
        "lower scale spaces",
        "Gaussian difference image",
        "Gradient histogram statistics",
        "7 Gaussian pyramid",
        "key point",
        "Ransac method",
        "group Zhang",
        "adjacent upper",
        "adjacent points",
        "correct matching",
        "gradient value",
        "successful matching",
        "matching process",
        "edge response",
        "independent characteristic",
        "direc- tion",
        "square column",
        "head rotation",
        "light-illuminated pictures",
        "mismatch rate",
        "correct matches",
        "mismatched points",
        "peak value",
        "matching accuracy",
        "gradient direction",
        "auxiliary direction",
        "contributive pixels",
        "Three pairs",
        "other words",
        "Error degree",
        "lower images",
        "total number",
        "26 points",
        "60 pairs",
        "120 images",
        "calculation",
        "surrounding",
        "case",
        "invariance",
        "perspective",
        "set",
        "tors",
        "descriptor",
        "probability",
        "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi",
        "neighborhood",
        "Comparison",
        "Gauss-Laplacian",
        "36 columns",
        "maximum",
        "robustness",
        "glasses",
        "hats",
        "Figure",
        "work",
        "classic",
        "grees",
        "2.5",
        "3.",
        "θ",
        "360°",
        "10°",
        "tracking registra- tion subsystem",
        "DoG space extreme point",
        "nonlinear feature extraction function",
        "many face alignment algorithms",
        "SDM-based face alignment process",
        "face feature point alignment",
        "ture point location",
        "feature vector descriptor",
        "main direction Zhang",
        "original data information",
        "initialization feature point",
        "head rotation angle",
        "n feature points",
        "① SIFT matching performance",
        "face feature points",
        "Harris matching method",
        "ginal face image",
        "face image detection",
        "3.3 Face alignment",
        "function approximation",
        "SIFT feature",
        "head posture",
        "real face",
        "face images",
        "matching re",
        "experimental data",
        "motion direction",
        "key points",
        "contour points",
        "estimation error",
        "average face",
        "number difference",
        "same number",
        "success rate",
        "overall trend",
        "inevitable errors",
        "good effect",
        "step size",
        "optimal position",
        "sonable translation",
        "shape factors",
        "optimal solution",
        "initial value",
        "training process",
        "perimental image",
        "Image normalization",
        "illumination transformation",
        "scaling transformation",
        "first sample",
        "sample size",
        "other variables",
        "Table 1 show",
        "experimental samples",
        "tween average",
        "SDM algorithm",
        "120 samples",
        "Figures",
        "sults",
        "hat",
        "light",
        "shade",
        "darkness",
        "matches",
        "finiteness",
        "positioning",
        "eyebrows",
        "nose",
        "mouth",
        "Zhu",
        "offset",
        "iteration",
        "current",
        "efficiency",
        "interference",
        "center",
        "x∗",
        "coordinates",
        "128 di",
        "mensions",
        "R128n",
        "operation",
        "Δx",
        "xk",
        "bk",
        "paths",
        "3.1",
        "∅",
        "3.3.3 SDM algorithm alignment result",
        "lateral face alignment results",
        "Face feature point location",
        "iteration step size",
        "objective function Eq",
        "principal component analysis",
        "world coordinate system",
        "rigid body invariance",
        "LFW face databases",
        "Surrey Face Model",
        "current spatial position",
        "new spatial position",
        "general 3D face",
        "3.4.1 Feature point labelling",
        "3D face model",
        "linear regression problem",
        "Local feature extraction",
        "3.4 Face pose estimation",
        "local feature points",
        "late 3D coordinates",
        "71 face feature points",
        "68 face feature points",
        "camera parame- ters",
        "3D geometric position",
        "3D rigid object",
        "geometric model parameters",
        "Sample sequence set",
        "3D position",
        "3D pose",
        "alignment accuracy",
        "132 face images",
        "206 face images",
        "803 face images",
        "input feature",
        "Matching results",
        "corresponding point",
        "The pose",
        "2D coordinates",
        "N points",
        "six points",
        "3D object",
        "test sample",
        "argminR0b0 X",
        "two parameters",
        "critical dimensions",
        "pts file",
        "head surface",
        "most cases",
        "optical properties",
        "projection relationship",
        "two movements",
        "② Rotary movement",
        "XYZ axis",
        "six degrees",
        "six numbers",
        "nose tip",
        "extraction process",
        "tion coordinates",
        "test phase",
        "test images",
        "rota- tion",
        "training set",
        "dimension reduction",
        "camera imaging",
        "Camera labeling",
        "camera relative",
        "① Translation movement",
        "hat Zhang",
        "Translation vector",
        "x0",
        "Δxi",
        "way",
        "Rk",
        "impact",
        "amount",
        "Section",
        "detail",
        "IBUG",
        "former",
        "sets",
        "orientation",
        "periments",
        "direction",
        "idea",
        "lows",
        "freedom",
        "corners",
        "SFM",
        "τ",
        "radial distortion parame- ters",
        "Match result analysis table",
        "key points Match ratio",
        "2D facial feature points",
        "False match rate",
        "y direc- tions",
        "least square method",
        "Tracking registration technology",
        "computer-generated virtual objects",
        "4.3 Feature point mapping",
        "arbitrary reference/coordinate system",
        "3.5 Tracking registration system",
        "face alignment process",
        "unknown scaling factor",
        "3D facial point",
        "tersects image plane",
        "2D image plane",
        "rotation matrix R",
        "image coordinate system",
        "2D position",
        "Head rotation",
        "P point",
        "manual labeling",
        "world coordinates",
        "focal length",
        "Variate Number",
        "Total number",
        "Wearing glasses",
        "position O",
        "linear equations",
        "Equa- tion",
        "following form",
        "correct poses",
        "tration techniques",
        "3D point",
        "3D coordinates",
        "same image",
        "pose estimation",
        "optical center",
        "translation vector",
        "camera matrix",
        "S X",
        "projection error",
        "3D model",
        "Table 1",
        "5 ¼ R",
        "5 ¼ S",
        "Yang",
        "Patras",
        "systems",
        "parameters",
        "change",
        "plenty",
        "problem",
        "fx",
        "fy",
        "ray",
        "Eqs",
        "distance",
        "tures",
        "sum",
        "squares",
        "scenes",
        "4.2",
        "664",
        "geometric transformation relation track- ing method",
        "face feature point tracking method",
        "front face alignment",
        "corresponding perspective changes",
        "poor user experience",
        "overlarge head corner",
        "good tacking effect",
        "spective transformation",
        "user head",
        "Face geometry",
        "face superposition",
        "stereoscopic changes",
        "human head",
        "first technique",
        "tech- nique",
        "two methods",
        "movement",
        "3D",
        "estimation",
        "picture"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    },
    {
      "@search.score": 3.0775287,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "metadata_storage_content_type": "application/pdf",
      "metadata_storage_size": 1688565,
      "metadata_storage_name": "s40537-019-0184-5.pdf",
      "metadata_storage_path": "aHR0cHM6Ly9hdGMyMDI0LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wYXBlcnMvTGlicmFyeS9MaWJyYXJ5L3M0MDUzNy0wMTktMDE4NC01LnBkZg2",
      "metadata_storage_file_extension": ".pdf",
      "metadata_content_type": "application/pdf",
      "metadata_language": "en",
      "metadata_author": "Tu Nguyen Thi Ngoc ",
      "metadata_title": "Mining aspects of customer’s review on the social network",
      "metadata_creation_date": "2019-02-26T14:27:28Z",
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Naive Bayes Open Access",
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Vietnam Electric Power University",
        "Creative Commons license",
        "Nguyen Thi Ngoc",
        "Viet Anh Nguyen2",
        "commerce web sites",
        "useful informa- tion",
        "real world datasets",
        "creat iveco mmons",
        "235 Hoang Quoc Viet",
        "Bayes classification method",
        "original author(s",
        "5-star overall rating",
        "cus- tomer reviews",
        "social network",
        "recent years",
        "significant role",
        "huge amount",
        "efficient methods",
        "ferent level",
        "important role",
        "conditional probability",
        "bootstrap technique",
        "sentiment words",
        "Experimental results",
        "good performance",
        "other state",
        "art methods",
        "Core term",
        "unrestricted use",
        "appropriate credit",
        "Full list",
        "author information",
        "aspect ratings",
        "aspect weights",
        "aspect extraction",
        "aspect words",
        "aspect consistency",
        "three tasks",
        "positive opinions",
        "customers’ opinion",
        "doi.org",
        "Mining aspects",
        "different aspects",
        "acidity aspects",
        "user sentiments",
        "users’ opinions",
        "product aspects",
        "user review",
        "Introduction",
        "lot",
        "people",
        "things",
        "products",
        "services",
        "quality",
        "challenge",
        "problem",
        "paper",
        "study",
        "attributes",
        "components",
        "concept",
        "positivity",
        "negativity",
        "example",
        "Fig.",
        "coffee",
        "Abstract",
        "solutions",
        "satisfaction",
        "degree",
        "importance",
        "manufacturers",
        "approach",
        "features",
        "frequencies",
        "comparison",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "METHODOLOGY",
        "Correspondence",
        "tunn",
        "dhdl",
        "1 Department",
        "E-Commerce",
        "Hanoi",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "body",
        "taste",
        "aroma",
        "instance",
        "Several complex filter-based approaches",
        "Turkish -style cardamon coffee",
        "Turkish-style cardamon coffee",
        "copper stove-top pot",
        "sweetened condensed milk",
        "21Nguyen Thi Ngoc",
        "Three tasks Extracting",
        "Trung Nguyen coffee",
        "overall rat- ing",
        "Hidden Markov Model",
        "aspect extraction task",
        "conditional probability technique",
        "enough core terms",
        "quency-based approaches",
        "aspect candidates",
        "Aspect Rate",
        "Estimating Aspect",
        "overall rating",
        "previous work",
        "non-aspect concepts",
        "earliest work",
        "relevant words",
        "low cost",
        "frequent nouns",
        "noun phrases",
        "dom Field",
        "manu- ally",
        "main challenge",
        "many reviews",
        "big fan",
        "thorough understanding",
        "general impression",
        "specific rating",
        "implicit aspects",
        "important aspects",
        "low-frequent aspects",
        "possible aspects",
        "wrong aspects",
        "multiple aspects",
        "training data",
        "learning techniques",
        "chocolate-like note",
        "frequency-based methods",
        "ter results",
        "universal set",
        "explicit aspects",
        "acidity",
        "difficulty",
        "noise",
        "rare",
        "concerns",
        "statistics",
        "high",
        "HMM",
        "CRF",
        "product",
        "assumption",
        "number",
        "domain",
        "experts",
        "existing",
        "sentences",
        "new",
        "MYOB",
        "January",
        "flared",
        "Istanbul",
        "stuff",
        "cream",
        "sugar",
        "1 Comment",
        "discussion",
        "section",
        "user",
        "Weight",
        "Dark",
        "flared copper stove",
        "J Big Data",
        "future research directions",
        "based, machine learning",
        "different real-life datasets",
        "data mining algorithm",
        "two important tasks",
        "aspect-based opinion mining",
        "associated overall rating",
        "aspect-based rating inference",
        "many researches work",
        "aspect weighting tasks",
        "Different approach",
        "earliest researches",
        "Related work",
        "top pot",
        "condensed milk",
        "supervised approach",
        "sophisticated state",
        "Problem definition",
        "last decade",
        "increasing attention",
        "sentiment analysis",
        "topic modeling",
        "label assignment",
        "commercial companies",
        "improve- ments",
        "filtering approach",
        "Method” sections",
        "frequency threshold",
        "frequency-based approach",
        "aspect extracting",
        "aspect identification",
        "aspect terms",
        "review content",
        "art approaches",
        "The Fig. 2",
        "similar solution",
        "information distance",
        "frequency-based method",
        "regression methods",
        "interesting methods",
        "words frequency",
        "chocolate",
        "note",
        "Body",
        "Aroma",
        "Acidity",
        "bitter",
        "weights",
        "aspects",
        "reviews",
        "fact",
        "accuracy",
        "Results",
        "details",
        "methodology",
        "experimental",
        "evaluation",
        "Conclusion",
        "area",
        "Researchers",
        "survey",
        "nouns",
        "Hu",
        "Liu",
        "part",
        "speech/POS",
        "occurrence",
        "quencies",
        "frequent",
        "spite",
        "simplicity",
        "business",
        "limita",
        "high-frequency",
        "problems",
        "filters",
        "seed",
        "0.",
        "many practical sentiment analysis applications",
        "abilistic Latent Semantic Analysis",
        "real-life sentiment analysis applications",
        "two main basic models",
        "current topic modeling methods",
        "two parameter vectors",
        "Latent Dirichlet allocation",
        "topic mod- eling",
        "large document collec",
        "topic- modeling approaches",
        "mining textual reviews",
        "Learning aspect labels",
        "traditional topic models",
        "The FLDA method",
        "aspect-specific sentiment words",
        "TripAdvisor data set",
        "latent aspect ratings",
        "many types",
        "lexicon-based methods",
        "rule-based approaches",
        "large collection",
        "latent topics",
        "negative topic",
        "principled method",
        "multi-domain reviews",
        "new method",
        "short reviews",
        "manual effort",
        "Joint Sentiment-Topic",
        "Lee [7] dataset",
        "bipartite graph",
        "small number",
        "Rating model",
        "overall ratings",
        "word distribution",
        "probabilistic inference",
        "significant amount",
        "specific entities",
        "mining aspects",
        "unlabeled data",
        "aspect price",
        "information models",
        "weak supervision",
        "full supervision",
        "AIR model",
        "various parameters",
        "reasonable results",
        "frequent topics",
        "unsupervised approach",
        "review rating",
        "LDA model",
        "frequent aspects",
        "labeled sentences",
        "distance",
        "other",
        "dollars",
        "generalization",
        "practice",
        "limitations",
        "texts",
        "researches",
        "pLSA",
        "authors",
        "adjectives",
        "JST",
        "Both",
        "positive",
        "Pang",
        "distributions",
        "addition",
        "Moghaddam",
        "Ester",
        "fac",
        "item",
        "work",
        "fication",
        "sampling",
        "extraction",
        "reviewers",
        "unbalance",
        "tuning",
        "order",
        "Such",
        "Gini Index based feature selection method",
        "The Gini Index method",
        "large movie review data set",
        "Naïve Bayes",
        "Support Vector Machine",
        "machine learning approaches",
        "multiple layer architecture",
        "different sentiment levels",
        "adjectival modifying relations",
        "knowl- edge representation",
        "aspect-based sentiment analysis",
        "Machine learning methods",
        "product overall ratings",
        "possible K aspects",
        "associated sentiment words",
        "annotated data",
        "supervised learning",
        "associated orientations",
        "Movie Ontology",
        "multiple sentences",
        "possible worlds",
        "wk| k",
        "sentiment score",
        "sentiment lexicon",
        "Sentiment classification",
        "dependency relations",
        "predicate relations",
        "verb-object relations",
        "multiple words",
        "dictionary-based approach",
        "Xiaowen Ding",
        "Minqing Hu",
        "eRank algorithm",
        "Synonym lexicon",
        "relative clause",
        "rela- tions",
        "Non-features nouns",
        "proper nouns",
        "brand names",
        "verbal nouns",
        "personal nouns",
        "Peñalver-Martinez",
        "domain ontology",
        "critical issue",
        "supervised methods",
        "two sets",
        "Decision Tree",
        "Neural Network",
        "Maximum Entropy",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "two parts",
        "core terms",
        "movie reviews",
        "topic words",
        "nouns/noun phrases",
        "higher accuracy",
        "SVM) classifier",
        "input text",
        "product features",
        "product coffee",
        "word dictionary",
        "taste aspect",
        "Opinions",
        "respect",
        "polarity",
        "strength",
        "intensification",
        "negation",
        "document",
        "Yan",
        "subject",
        "list",
        "synonyms",
        "basis",
        "cost",
        "money",
        "dictionaries",
        "training",
        "testing",
        "classifiers",
        "DT",
        "Asha",
        "research",
        "model",
        "prediction",
        "techniques",
        "attribute",
        "component",
        "aj",
        "A(.",
        "operator",
        "aftertaste",
        "mouth",
        "supervised learning method",
        "Naive Bayes method",
        "many other people",
        "Aspect core terms",
        "field experts",
        "probability distribution",
        "multiple labels",
        "Notation Description",
        "non-negative weights",
        "R K",
        "K-dimensional vector",
        "corresponding aspect",
        "Extracting aspect",
        "one aspect",
        "aspect rate",
        "aspect expressions",
        "j-th aspect",
        "same aspect",
        "aspect labels",
        "higher weight",
        "Major notations",
        "K aspect",
        "negative words",
        "aspect aj",
        "vector ri",
        "same review",
        "reviews’ text",
        "positive words",
        "review i",
        "∑K",
        "αi",
        "riK",
        "rij",
        "opinion",
        "assessment",
        "rmin",
        "range",
        "Definition",
        "emphasis",
        "associated",
        "Cj",
        "wjk",
        "Table",
        "goal",
        "task",
        "sentence",
        "1,Q",
        "yi",
        "wk",
        "corpus",
        "Sj",
        "Tj",
        "aij",
        "idea",
        "observations",
        "formula",
        "account",
        "occurrences",
        "frequency",
        "subset",
        "initial aspect core terms",
        "flavor taste aftertaste mouthfeel",
        "conditional probabilistic model",
        "initial aspect labeling",
        "initial core terms",
        "original core terms",
        "Aspect Extraction Algorithm",
        "universal label set",
        "new core terms",
        "corresponding aspect words",
        "new-found aspect words",
        "new aspect word",
        "body acidity acid",
        "bootstrapping algorithm",
        "aspect weight",
        "Aspect ratings",
        "new words",
        "existing methods",
        "Bootstrap technique",
        "bol O",
        "symbol X",
        "high probability",
        "sion-based methods",
        "two parameters",
        "following equation",
        "new set",
        "four aspects",
        "K aspects",
        "two aspects",
        "reviews’ texts",
        "new sentences",
        "four circles",
        "maximum number",
        "incorrect labels",
        "coffee product",
        "richer set",
        "Bayes",
        "Figure",
        "sets",
        "ishing",
        "smell",
        "threshold",
        "one",
        "adverbs",
        "process",
        "procedure",
        "step",
        "iterations",
        "sum",
        "θ",
        "most sentiment analysis work",
        "Naïve Bayes method",
        "Speech tech- nique",
        "two consecutive words",
        "linear regression methods",
        "good grassy note",
        "following two sentences",
        "two noun phrases",
        "aspect rating problem",
        "two patterns",
        "candidate sentiment",
        "other methods",
        "same time",
        "key point",
        "important point",
        "weighted sum",
        "multi-label classification",
        "different contexts",
        "big problem",
        "big room",
        "syntactic patterns",
        "fixed patterns",
        "JJ tags",
        "NN tags",
        "RB tags",
        "VB tags",
        "Laplace transformation",
        "class c",
        "one word",
        "first word",
        "k-th aspect",
        "opposite sentiments",
        "same adjective",
        "POS tags",
        "P(rij",
        "class label",
        "second word",
        "rating label",
        "rating rij",
        "rijαij",
        "third rules",
        "feature fk",
        "word features",
        "probability rij",
        "review",
        "content",
        "requirement",
        "Eq.",
        "labels",
        "known",
        "Part",
        "staff",
        "naj",
        "smoothing",
        "∑",
        "POS labeled rules",
        "RBS JJ",
        "JJ JJ",
        "RBS VB",
        "third word",
        "JJ NN",
        "NNS JJ",
        "fq",
        "∏q",
        "fk",
        "fj",
        "Table 2",
        "RBR",
        "VBD",
        "VBN",
        "VBG",
        "method"
      ],
      "publication_name": null,
      "publisher": null,
      "doi": null,
      "publication_date": null
    }
  ]
}